{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SHORYA SETHIA [ 22B2725 ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "Using data from : https://www.kaggle.com/netflix-inc/netflix-prize-data/data\n",
    "It contains:\n",
    "1. combined_data_1.txt\n",
    "2. combined_data_2.txt\n",
    "3. combined_data_3.txt\n",
    "4. combined_data_4.txt\n",
    "5. movie_titles.csv\n",
    "\n",
    "### Data Overview\n",
    "The first line of each file combined_data_{i}.txt contains the movie id followed by a colon. Each subsequent line in the file corresponds to a rating from a customer and its date in the format: CustomerID,Rating,Date\n",
    "\n",
    "- MovieIDs range from 1 to 17770 sequentially.\n",
    "- CustomerIDs range from 1 to 2649429, with gaps. There are 480189 users.\n",
    "- Ratings are on a five star (integral) scale from 1 to 5.\n",
    "- Dates have the format YYYY-MM-DD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is just to know how much time will it take to run this entire ipython notebook \n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('nbagg')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'figure.max_open_warning': 0})\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "import os\n",
    "from scipy import sparse\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import TimeSeriesSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "### Converting entire data to following format:\n",
    "u_i,m_j,r_ij"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken : 0:00:00\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "if not os.path.isfile('data.csv'):\n",
    "    # Create a file 'data.csv' before reading it\n",
    "    # Read all the files in Netflix Prize Data and store them in one big file('data.csv')\n",
    "    # I am Re-reading from each of the four files and appendig each rating to a global file 'train.csv'\n",
    "    data = open('data.csv', mode='w')\n",
    "    \n",
    "    row = list()\n",
    "    files=['data/combined_data_1.txt','data/combined_data_2.txt', \n",
    "           'data/combined_data_3.txt', 'data/combined_data_4.txt']\n",
    "    for file in files:\n",
    "        print(\"Reading ratings from {}...\".format(file))\n",
    "        with open(file) as f:\n",
    "            for line in f: \n",
    "                del row[:] # you don't have to do this.\n",
    "                line = line.strip()\n",
    "                if line.endswith(':'):\n",
    "                    # All below are ratings for this movie, until another movie appears.\n",
    "                    movie_id = line.replace(':', '')\n",
    "                else:\n",
    "                    row = [x for x in line.split(',')]\n",
    "                    row.insert(0, movie_id)\n",
    "                    data.write(','.join(row))\n",
    "                    data.write('\\n')\n",
    "        print(\"Done.\\n\")\n",
    "    data.close()\n",
    "print('Time taken :', datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating the dataframe from data.csv file..\n",
      "Done.\n",
      "\n",
      "Sorting the dataframe by date..\n",
      "Done..\n",
      "Time taken : 0:03:26.270092\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "if not os.path.isfile('sorted_data.csv'):\n",
    "  print(\"creating the dataframe from data.csv file..\")\n",
    "  df = pd.read_csv('data.csv', sep=',', names=['movie', 'user','rating','date'])\n",
    "  df.date = pd.to_datetime(df.date)\n",
    "  print('Done.\\n')\n",
    "\n",
    "  # we are arranging the ratings according to time.\n",
    "  print('Sorting the dataframe by date..')\n",
    "  df.sort_values(by='date', inplace=True)\n",
    "  print('Done..')\n",
    "\n",
    "  output_filename = 'sorted_data.csv'\n",
    "  df.to_csv(output_filename, index=False)\n",
    "\n",
    "else:\n",
    "  print(\"File already exists. Reading it...\")\n",
    "  df = pd.read_csv('sorted_data.csv')\n",
    "  \n",
    "print('Time taken :', datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie</th>\n",
       "      <th>user</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>56431994</th>\n",
       "      <td>10341</td>\n",
       "      <td>510180</td>\n",
       "      <td>4</td>\n",
       "      <td>1999-11-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9056171</th>\n",
       "      <td>1798</td>\n",
       "      <td>510180</td>\n",
       "      <td>5</td>\n",
       "      <td>1999-11-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58698779</th>\n",
       "      <td>10774</td>\n",
       "      <td>510180</td>\n",
       "      <td>3</td>\n",
       "      <td>1999-11-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48101611</th>\n",
       "      <td>8651</td>\n",
       "      <td>510180</td>\n",
       "      <td>2</td>\n",
       "      <td>1999-11-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81893208</th>\n",
       "      <td>14660</td>\n",
       "      <td>510180</td>\n",
       "      <td>2</td>\n",
       "      <td>1999-11-11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          movie    user  rating       date\n",
       "56431994  10341  510180       4 1999-11-11\n",
       "9056171    1798  510180       5 1999-11-11\n",
       "58698779  10774  510180       3 1999-11-11\n",
       "48101611   8651  510180       2 1999-11-11\n",
       "81893208  14660  510180       2 1999-11-11"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1.004805e+08\n",
       "mean     3.604290e+00\n",
       "min      1.000000e+00\n",
       "25%      3.000000e+00\n",
       "50%      4.000000e+00\n",
       "75%      4.000000e+00\n",
       "max      5.000000e+00\n",
       "std      1.085219e+00\n",
       "Name: rating, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()['rating']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Nan values in our dataframe :  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Nan values in our dataframe : \", sum(df.isnull().any()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deleting Duplicates either movie_id, user/customer_id, ratings, date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 duplicate rating entries in the data..\n"
     ]
    }
   ],
   "source": [
    "dup_bool = df.duplicated(['movie','user','rating'])\n",
    "dups = sum(dup_bool) \n",
    "print(\"There are {} duplicate rating entries in the data..\".format(dups))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Users, movies and ratings in data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total No of Users   : 480189\n",
      "Total No of movies  : 17770\n",
      "Total no of ratings : 100480507\n"
     ]
    }
   ],
   "source": [
    "print(\"Total No of Users   :\", len(np.unique(df.user)))\n",
    "print(\"Total No of movies  :\", len(np.unique(df.movie)))\n",
    "print(\"Total no of ratings :\",df.shape[0]) #total rows == no. of ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spliting data into Train and Test (0.80 : 0.20 respectively)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.csv exists\n",
      "test.csv exists\n",
      "read both csv\n",
      "Time taken : 0:00:33.163274\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isfile('train.csv'):\n",
    "    # create the dataframe and store it as csv for further purposes\n",
    "    df.iloc[:int(df.shape[0]*0.80)].to_csv(\"train.csv\", index=False)\n",
    "    print(\"train.csv formed.\")\n",
    "else :\n",
    "    print(\"train.csv exists\")\n",
    "\n",
    "if not os.path.isfile('test.csv'):\n",
    "    # create the dataframe and store it as csv for further purposes\n",
    "    df.iloc[int(df.shape[0]*0.80):].to_csv(\"test.csv\", index=False)\n",
    "    print(\"test.csv formed.\")\n",
    "else :\n",
    "    print(\"test.csv exists\")\n",
    "\n",
    "start = datetime.now()\n",
    "train_df = pd.read_csv(\"train.csv\", parse_dates=['date'])\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "print(\"read both csv\")\n",
    "print('Time taken :', datetime.now() - start)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Users, Movies and ratings in train.csv and test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numbers for train.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total No of Users   : 405041\n",
      "Total No of movies  : 17424\n",
      "Total no of ratings : 80384405\n",
      "\n",
      "Numbers for test.csv\n",
      "Total No of Users   : 349312\n",
      "Total No of movies  : 17757\n",
      "Total no of ratings : 20096102\n"
     ]
    }
   ],
   "source": [
    "print(\"Numbers for train.csv\")\n",
    "print(\"Total No of Users   :\", len(np.unique(train_df.user)))\n",
    "print(\"Total No of movies  :\", len(np.unique(train_df.movie)))\n",
    "print(\"Total no of ratings :\",train_df.shape[0])\n",
    "\n",
    "print(\"\\nNumbers for test.csv\")\n",
    "print(\"Total No of Users   :\", len(np.unique(test_df.user)))\n",
    "print(\"Total No of movies  :\", len(np.unique(test_df.movie)))\n",
    "print(\"Total no of ratings :\",test_df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA on trian_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to make y-axis more readable\n",
    "def human(num, units = 'M'):\n",
    "    units = units.lower()\n",
    "    num = float(num)\n",
    "    if units == 'k':\n",
    "        return str(num/10**3) + \" K\"\n",
    "    elif units == 'm':\n",
    "        return str(num/10**6) + \" M\"\n",
    "    elif units == 'b':\n",
    "        return str(num/10**9) +  \" B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ratind Distribution ploting was taking very long time\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# plt.title('Distribution of ratings over Training dataset', fontsize=15)\n",
    "# sns.countplot(train_df.rating)\n",
    "# ax.set_yticklabels([human(item, 'M') for item in ax.get_yticks()])\n",
    "# ax.set_ylabel('No. of Ratings(Millions)')\n",
    "\n",
    "# plt.savefig('img/rating-distribution-train_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of ratings over Training dataset:\n",
      "rating\n",
      "4    27161596\n",
      "3    23339084\n",
      "5    17772845\n",
      "2     8369795\n",
      "1     3741085\n",
      "Name: count, dtype: int64\n",
      "Time taken: 0:00:00.425591\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "rating_counts = train_df['rating'].value_counts()\n",
    "print(\"Distribution of ratings over Training dataset:\")\n",
    "print(rating_counts)\n",
    "print('Time taken:', datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie</th>\n",
       "      <th>user</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>day_of_week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10341</td>\n",
       "      <td>510180</td>\n",
       "      <td>4</td>\n",
       "      <td>1999-11-11</td>\n",
       "      <td>Thursday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1798</td>\n",
       "      <td>510180</td>\n",
       "      <td>5</td>\n",
       "      <td>1999-11-11</td>\n",
       "      <td>Thursday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10774</td>\n",
       "      <td>510180</td>\n",
       "      <td>3</td>\n",
       "      <td>1999-11-11</td>\n",
       "      <td>Thursday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8651</td>\n",
       "      <td>510180</td>\n",
       "      <td>2</td>\n",
       "      <td>1999-11-11</td>\n",
       "      <td>Thursday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14660</td>\n",
       "      <td>510180</td>\n",
       "      <td>2</td>\n",
       "      <td>1999-11-11</td>\n",
       "      <td>Thursday</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   movie    user  rating       date day_of_week\n",
       "0  10341  510180       4 1999-11-11    Thursday\n",
       "1   1798  510180       5 1999-11-11    Thursday\n",
       "2  10774  510180       3 1999-11-11    Thursday\n",
       "3   8651  510180       2 1999-11-11    Thursday\n",
       "4  14660  510180       2 1999-11-11    Thursday"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Add new column (week day) to the data\n",
    "# train_df['day_of_week'] = train_df.date.dt.weekday_name\n",
    "# train_df.head()\n",
    "\n",
    "# Add new column (week day) to the data\n",
    "train_df['day_of_week'] = train_df['date'].dt.day_name()\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ratings\n",
      "day_of_week\n",
      "Friday       3.585274\n",
      "Monday       3.577250\n",
      "Saturday     3.591791\n",
      "Sunday       3.594144\n",
      "Thursday     3.582463\n",
      "Tuesday      3.574438\n",
      "Wednesday    3.583751\n",
      "Name: rating, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "avg_week_df = train_df.groupby(by=['day_of_week'])['rating'].mean()\n",
    "print(\"Average ratings\")\n",
    "print(avg_week_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shory\\AppData\\Local\\Temp\\ipykernel_7920\\455898017.py:6: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_yticklabels([human(item, 'M') for item in ax.get_yticks()])\n"
     ]
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "sns.countplot(x='day_of_week', data=train_df, ax=ax)\n",
    "plt.title('No of ratings on each day.')\n",
    "plt.ylabel('Total no of ratings')\n",
    "plt.xlabel('')\n",
    "ax.set_yticklabels([human(item, 'M') for item in ax.get_yticks()])\n",
    "plt.savefig('img/no.-of-rating-on-each-day_of_week-train_df.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = train_df.resample('m', on='date')['rating'].count().plot()\n",
    "ax.set_title('No of ratings per month (Training data)')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('No of ratings(per month)')\n",
    "ax.set_yticklabels([human(item, 'M') for item in ax.get_yticks()])\n",
    "plt.savefig('img/no.-of-ratings-per-month-train_df.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis on ratings given by a user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user\n",
       "305344     17112\n",
       "2439493    15896\n",
       "387418     15402\n",
       "1639792     9767\n",
       "1461435     9447\n",
       "Name: rating, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_of_rated_movies_per_user = train_df.groupby(by='user')['rating'].count().sort_values(ascending=False)\n",
    "no_of_rated_movies_per_user.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shory\\AppData\\Local\\Temp\\ipykernel_33132\\4202440675.py:4: FutureWarning: \n",
      "\n",
      "`shade` is now deprecated in favor of `fill`; setting `fill=True`.\n",
      "This will become an error in seaborn v0.14.0; please update your code.\n",
      "\n",
      "  sns.kdeplot(no_of_rated_movies_per_user, shade=True, ax=ax1)\n",
      "C:\\Users\\shory\\AppData\\Local\\Temp\\ipykernel_33132\\4202440675.py:9: FutureWarning: \n",
      "\n",
      "`shade` is now deprecated in favor of `fill`; setting `fill=True`.\n",
      "This will become an error in seaborn v0.14.0; please update your code.\n",
      "\n",
      "  sns.kdeplot(no_of_rated_movies_per_user, shade=True, cumulative=True,ax=ax2)\n"
     ]
    }
   ],
   "source": [
    "fig = plt.figure(figsize=plt.figaspect(.5))\n",
    "\n",
    "ax1 = plt.subplot(121)\n",
    "sns.kdeplot(no_of_rated_movies_per_user, shade=True, ax=ax1)\n",
    "plt.xlabel('No of ratings by user')\n",
    "plt.title(\"PDF\")\n",
    "\n",
    "ax2 = plt.subplot(122)\n",
    "sns.kdeplot(no_of_rated_movies_per_user, shade=True, cumulative=True,ax=ax2)\n",
    "plt.xlabel('No of ratings by user')\n",
    "plt.title('CDF')\n",
    "\n",
    "plt.savefig('img/pdf-cdf-rating-by-user-train_df.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above warning is just about to use \"fill\" in place of \"shade\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    405041.000000\n",
       "mean        198.459921\n",
       "std         290.793238\n",
       "min           1.000000\n",
       "25%          34.000000\n",
       "50%          89.000000\n",
       "75%         245.000000\n",
       "max       17112.000000\n",
       "Name: rating, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_of_rated_movies_per_user.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00        1\n",
       "0.01        1\n",
       "0.02        2\n",
       "0.03        4\n",
       "0.04        5\n",
       "        ...  \n",
       "0.96      829\n",
       "0.97      934\n",
       "0.98     1079\n",
       "0.99     1341\n",
       "1.00    17112\n",
       "Name: rating, Length: 101, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantiles = no_of_rated_movies_per_user.quantile(np.arange(0,1.01,0.01), interpolation='higher')\n",
    "quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Quantiles and their Values\")\n",
    "quantiles.plot()\n",
    "# quantiles with 0.05 difference\n",
    "plt.scatter(x=quantiles.index[::5], y=quantiles.values[::5], c='orange', label=\"quantiles with 0.05 intervals\")\n",
    "# quantiles with 0.25 difference\n",
    "plt.scatter(x=quantiles.index[::25], y=quantiles.values[::25], c='m', label = \"quantiles with 0.25 intervals\")\n",
    "plt.ylabel('No of ratings by user')\n",
    "plt.xlabel('Value at the quantile')\n",
    "plt.legend(loc='best')\n",
    "\n",
    "# annotate the 25th, 50th, 75th and 100th percentile values....\n",
    "for x,y in zip(quantiles.index[::25], quantiles[::25]):\n",
    "    s= s=\"({} , {})\".format(x,y)\n",
    "    plt.annotate(s, xy=(x,y), xytext=(x-0.05, y+500)\n",
    "                ,fontweight='bold')\n",
    "\n",
    "plt.savefig('img/quantiles.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00        1\n",
       "0.05        7\n",
       "0.10       15\n",
       "0.15       21\n",
       "0.20       27\n",
       "0.25       34\n",
       "0.30       41\n",
       "0.35       50\n",
       "0.40       60\n",
       "0.45       73\n",
       "0.50       89\n",
       "0.55      109\n",
       "0.60      133\n",
       "0.65      163\n",
       "0.70      199\n",
       "0.75      245\n",
       "0.80      307\n",
       "0.85      392\n",
       "0.90      520\n",
       "0.95      749\n",
       "1.00    17112\n",
       "Name: rating, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantiles[::5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_ratings_per_movie = train_df.groupby(by='movie')['rating'].count().sort_values(ascending=False)\n",
    "\n",
    "fig = plt.figure(figsize=plt.figaspect(.5))\n",
    "ax = plt.gca()\n",
    "plt.plot(no_of_ratings_per_movie.values)\n",
    "plt.title('# RATINGS per Movie')\n",
    "plt.xlabel('Movie')\n",
    "plt.ylabel('No of Users who rated a movie')\n",
    "ax.set_xticklabels([])\n",
    "\n",
    "plt.savefig('img/per-movie-ratings-train_df.png')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There are some (<10%) movies which are rated by huge number of users.\n",
    "- But majority movies exists which are rated by some hundereds of users. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building sparse matrices from data\n",
    "\n",
    "- Present data has 3 columns, user, movie, ratings; for each movie there are many users and each user gives rating.\n",
    "- This takes lot of memory.\n",
    "- To minimize usage of memory, I am creating two arrays, one for movies(m_i's) and one for users(u_j's), by some matrix operation (generally dot product) would give me rating (r_ij's)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is present in pwd, loading it\n",
      "Done. It's shape is : (user, movie) :  (2649430, 17771)\n",
      "0:00:02.215208\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "if os.path.isfile('train_sparse_matrix.npz'):\n",
    "    print(\"It is present in pwd, loading it\")\n",
    "    train_sparse_matrix = sparse.load_npz('train_sparse_matrix.npz')\n",
    "    print('Done. It\\'s shape is : (user, movie) : ',train_sparse_matrix.shape)\n",
    "else: \n",
    "    print(\"Building sparse_matrix from the dataframe...\")\n",
    "    # create sparse_matrix and store it for after usage.\n",
    "    # csr_matrix(data_values, (row_index, col_index), shape_of_matrix)\n",
    "    # It should be in such a way that, MATRIX[row, col] = data\n",
    "    train_sparse_matrix = sparse.csr_matrix((train_df.rating.values, (train_df.user.values,\n",
    "                                               train_df.movie.values)),)\n",
    "    \n",
    "    print('Done. It\\'s shape is : (user, movie) : ',train_sparse_matrix.shape)\n",
    "    print('Saving it into pwd for further usages...')\n",
    "\n",
    "    sparse.save_npz(\"train_sparse_matrix.npz\", train_sparse_matrix)\n",
    "    print('Done.\\n')\n",
    "\n",
    "print(datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is present in pwd, loading it.\n",
      "Done. It's shape is : (user, movie) :  (2649430, 17771)\n",
      "0:00:00.585268\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "if os.path.isfile('test_sparse_matrix.npz'):\n",
    "    print(\"It is present in pwd, loading it.\")\n",
    "    # just get it from the disk instead of computing it\n",
    "    test_sparse_matrix = sparse.load_npz('test_sparse_matrix.npz')\n",
    "    print('Done. It\\'s shape is : (user, movie) : ',test_sparse_matrix.shape)\n",
    "else: \n",
    "    print(\"Building sparse_matrix from the dataframe...\")\n",
    "    # create sparse_matrix and store it for after usage.\n",
    "    # csr_matrix(data_values, (row_index, col_index), shape_of_matrix)\n",
    "    # It should be in such a way that, MATRIX[row, col] = data\n",
    "    test_sparse_matrix = sparse.csr_matrix((test_df.rating.values, (test_df.user.values,\n",
    "                                               test_df.movie.values)))\n",
    "    \n",
    "    print('Done. It\\'s shape is : (user, movie) : ',test_sparse_matrix.shape)\n",
    "    print('Saving it into pwd for further usages...')\n",
    "\n",
    "    sparse.save_npz(\"test_sparse_matrix.npz\", test_sparse_matrix)\n",
    "    print('Done.')\n",
    "    \n",
    "print(datetime.now() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparsity = (Number of Zero enteries/Number of total enteries)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity Of Train matrix : 99.8292709259195 % \n"
     ]
    }
   ],
   "source": [
    "us,mv = train_sparse_matrix.shape\n",
    "elem = train_sparse_matrix.count_nonzero()\n",
    "print(\"Sparsity Of Train matrix : {} % \".format((1-(elem/(us*mv)))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity Of Test matrix : 99.95731772988694 % \n"
     ]
    }
   ],
   "source": [
    "us,mv = test_sparse_matrix.shape\n",
    "elem = test_sparse_matrix.count_nonzero()\n",
    "print(\"Sparsity Of Test matrix : {} % \".format(  (1-(elem/(us*mv))) * 100) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Average rating globally, per movie and per user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_ratings(sparse_matrix, of_users):  # of_users is boolean flag (1: users, 0:movies)\n",
    "    \n",
    "    # selecting axes of sparse matrix\n",
    "    ax = 1 if of_users else 0\n",
    "    \n",
    "    sum_of_ratings = sparse_matrix.sum(axis=ax).A1     # \".A1\" for converting Column_Matrix to 1-D numpy array \n",
    "    \n",
    "    # Boolean matrix of ratings (whether a user rated that movie or not)\n",
    "    is_rated = sparse_matrix!=0\n",
    "    \n",
    "    # no of ratings that each user OR movie..\n",
    "    no_of_ratings = is_rated.sum(axis=ax).A1\n",
    "    \n",
    "    u,m = sparse_matrix.shape     # max_user(u)  and max_movie(m) id's in sparse matrix \n",
    "\n",
    "    # average_rating = sum of ratings/sum of non-zero entries\n",
    "    average_ratings = { i : sum_of_ratings[i]/no_of_ratings[i]            \n",
    "                                 for i in range(u if of_users else m) \n",
    "                                    if no_of_ratings[i] !=0}  \n",
    "    \n",
    "    return average_ratings # returns dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Average of Ratings in training data is {'global': 3.582890686321557}\n"
     ]
    }
   ],
   "source": [
    "train_averages = dict()\n",
    "\n",
    "train_global_average = train_sparse_matrix.sum()/train_sparse_matrix.count_nonzero()\n",
    "train_averages['global'] = train_global_average\n",
    "print(f\"Global Average of Ratings in training data is {train_averages}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average rating of user 573242 : 4.138339920948616\n"
     ]
    }
   ],
   "source": [
    "train_averages['user'] = get_average_ratings(train_sparse_matrix, of_users=True)\n",
    "# user = random.randint(1,train_sparse_matrix.shape [0])\n",
    "# print(user)\n",
    "\n",
    "# Generate a random user ID within the valid range\n",
    "valid_users = list(train_averages['user'].keys())  # Get the list of valid user IDs\n",
    "user = random.choice(valid_users) \n",
    "print(f'Average rating of user {user} :',train_averages['user'][user])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average rating of movie 12910 : 2.7708333333333335\n"
     ]
    }
   ],
   "source": [
    "train_averages['movie'] =  get_average_ratings(train_sparse_matrix, of_users=False)\n",
    "\n",
    "valid_movies = list(train_averages['movie'].keys())\n",
    "movie = random.choice(valid_movies)\n",
    "print(f'Average rating of movie {movie} :',train_averages['movie'][movie])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PDF and CDF of avg rating of user and movie in train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:05.615467\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "# Draw PDFs for average rating per user and per movie\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=plt.figaspect(.5))\n",
    "fig.suptitle('Avg Ratings per User and per Movie', fontsize=15)\n",
    "\n",
    "ax1.set_title('Users-Avg-Ratings')\n",
    "# Get the list of average user ratings from the averages dictionary\n",
    "user_averages = [rat for rat in train_averages['user'].values()]\n",
    "sns.kdeplot(user_averages, cumulative=True, ax=ax1, label='Cdf')\n",
    "sns.kdeplot(user_averages, ax=ax1, label='Pdf')\n",
    "\n",
    "ax2.set_title('Movies-Avg-Rating')\n",
    "# Get the list of movie average ratings from the dictionary\n",
    "movie_averages = [rat for rat in train_averages['movie'].values()]\n",
    "sns.kdeplot(movie_averages, cumulative=True, ax=ax2, label='Cdf')\n",
    "sns.kdeplot(movie_averages, ax=ax2, label='Pdf')\n",
    "\n",
    "plt.savefig('img/pdf-cdf-avg-rating-user&movie.png')\n",
    "print(datetime.now() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many new users and movies would I encounter in test_csv ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of Users  : 480189\n",
      "Number of Users in Train data : 405041\n",
      "No of Users that didn't appear in train data: 75148 (15.649671275268695 %) \n",
      " \n"
     ]
    }
   ],
   "source": [
    "total_users = len(np.unique(df.user))\n",
    "users_train = len(train_averages['user'])\n",
    "new_users = total_users - users_train\n",
    "\n",
    "print('Total number of Users  :', total_users)\n",
    "print('Number of Users in Train data :', users_train)\n",
    "print(\"No of Users that didn't appear in train data: {} ({} %) \\n \".format(new_users,(new_users/total_users)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of Movies  : 17770\n",
      "Number of Users in Train data : 17424\n",
      "No of Movies that didn't appear in train data: 346 (1.9471018570624647 %) \n",
      " \n"
     ]
    }
   ],
   "source": [
    "total_movies = len(np.unique(df.movie))\n",
    "movies_train = len(train_averages['movie'])\n",
    "new_movies = total_movies - movies_train\n",
    "\n",
    "print('Total number of Movies  :', total_movies)\n",
    "print('Number of Users in Train data :', movies_train)\n",
    "print(\"No of Movies that didn't appear in train data: {} ({} %) \\n \".format(new_movies,(new_movies/total_movies)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing similarity matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### user - user collaborative filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "def compute_user_similarity(sparse_matrix, compute_for_few=False, top = 100, verbose=False, verb_for_n_rows = 20,\n",
    "                            draw_time_taken=True):\n",
    "    no_of_users = sparse_matrix.shape[0]\n",
    "    # get the indices of  non zero rows (users) from our sparse matrix\n",
    "    row_ind, col_ind = sparse_matrix.nonzero()\n",
    "    row_ind = sorted(set(row_ind)) # we don't have to\n",
    "    time_taken = list() #  time taken for finding similar users for an user\n",
    "    \n",
    "    # Create rows, cols, and data lists.., which can be used to create sparse matrices\n",
    "    rows, cols, data = list(), list(), list()\n",
    "    if verbose: print(\"Computing strted for top\",top,\"similarities for each user...\")\n",
    "    \n",
    "    start = datetime.now()\n",
    "    temp = 0\n",
    "    \n",
    "    for row in row_ind[:top] if compute_for_few else row_ind:\n",
    "        temp = temp+1\n",
    "        prev = datetime.now()\n",
    "        \n",
    "        # get the similarity row for this user with all other users\n",
    "        sim = cosine_similarity(sparse_matrix.getrow(row), sparse_matrix).ravel()\n",
    "        # I will consider only the top 10/20/40/100 etc  most similar users and ignore rest of them..\n",
    "        top_sim_ind = sim.argsort()[-top:]\n",
    "        top_sim_val = sim[top_sim_ind]\n",
    "        \n",
    "        # add them to our rows, cols and data\n",
    "        rows.extend([row]*top)\n",
    "        cols.extend(top_sim_ind)\n",
    "        data.extend(top_sim_val)\n",
    "        time_taken.append(datetime.now().timestamp() - prev.timestamp())\n",
    "        if verbose:\n",
    "            if temp%verb_for_n_rows == 0:\n",
    "                print(\"Computing done for {} users [  time elapsed : {}  ]\"\n",
    "                      .format(temp, datetime.now()-start))\n",
    "            \n",
    "        \n",
    "    # lets create sparse matrix out of these and return it\n",
    "    if verbose: print('Creating Sparse matrix from the computed similarities')\n",
    "    #return rows, cols, data\n",
    "    \n",
    "    if draw_time_taken:\n",
    "        plt.plot(time_taken, label = 'time taken for each user')\n",
    "        plt.plot(np.cumsum(time_taken), label='Total time')\n",
    "        plt.legend(loc='best')\n",
    "        plt.xlabel('User')\n",
    "        plt.ylabel('Time (seconds)')\n",
    "        plt.savefig('img/u-u-cf-17k-dim-per-user.png')\n",
    "        \n",
    "    return sparse.csr_matrix((data, (rows, cols)), shape=(no_of_users, no_of_users)), time_taken "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing strted for top 200 similarities for each user...\n",
      "Computing done for 20 users [  time elapsed : 0:01:04.300235  ]\n",
      "Computing done for 40 users [  time elapsed : 0:01:57.593116  ]\n",
      "Computing done for 60 users [  time elapsed : 0:02:48.430230  ]\n",
      "Computing done for 80 users [  time elapsed : 0:03:38.995090  ]\n",
      "Computing done for 100 users [  time elapsed : 0:04:30.595433  ]\n",
      "Computing done for 120 users [  time elapsed : 0:05:25.550533  ]\n",
      "Computing done for 140 users [  time elapsed : 0:06:17.773366  ]\n",
      "Computing done for 160 users [  time elapsed : 0:07:08.075599  ]\n",
      "Computing done for 180 users [  time elapsed : 0:08:07.425267  ]\n",
      "Computing done for 200 users [  time elapsed : 0:09:08.188622  ]\n",
      "Creating Sparse matrix from the computed similarities\n",
      "Time taken for user-user cf with 17k dimensions per user : 0:09:13.713449\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "u_u_sim_sparse, _ = compute_user_similarity(train_sparse_matrix, compute_for_few=True, top = 200,\n",
    "                                                     verbose=True)\n",
    "print(\"Time taken for user-user cf with 17k dimensions per user :\",datetime.now()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Calculating user-user Similarity_Matrix (user-user collaborative filtering) is not an easy task\n",
    "- For top 200 users it took **0:09:13.713449** time, and as users count increases, complexity increases as one could find more and more similarities. \n",
    "\n",
    "* On avg per time consumed for searching similarity for one user = (9*60 + 13.71)/200 = **2.76 seconds**\n",
    "* training data have 405041 users, so approximately it would take **405041*2.76 = 1117913 seconds = 12.93 days**\n",
    "* It will take almost **13** days to just find similarities !\n",
    "\n",
    "- Hence, i would try to find user-user similarity via reduced dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Truncated SVD for reducing the dimesnion of user vector\n",
    "- SVD basically is a factorization of that matrix into three smaller matrices.\n",
    "- The SVD of mxn matrix A is given by the formula A = U Σ V^T \n",
    "- Where\n",
    "   - U is m*m matrix of orthonormal eigen vectors of AA^T\n",
    "   - V^T is n*n matrix of orthonormal eigen vectors of (A^T)A\n",
    "   - Σ is diagonal matrix with r elements, r = square root of positive eigen values of AA^T (or (A^T)A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting started...\n",
      "0:02:09.566720\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "start = datetime.now()\n",
    "\n",
    "# All parameters are default except n_components. n_itr is for Randomized SVD solver.\n",
    "netflix_svd = TruncatedSVD(n_components=100, algorithm='randomized', random_state=42)\n",
    "print(\"Fitting started...\")\n",
    "trunc_svd = netflix_svd.fit_transform(train_sparse_matrix)\n",
    "\n",
    "# num_iterations = 10\n",
    "# for i in range(num_iterations):\n",
    "#     # Fit the TruncatedSVD model for each iteration\n",
    "#     trunc_svd = netflix_svd.fit_transform(train_sparse_matrix)\n",
    "    \n",
    "#     # Print progress update\n",
    "#     print(f\"Iteration {i+1}/{num_iterations} completed\")\n",
    "\n",
    "print(datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.23362135, 0.26270872, 0.28323418, 0.29936103, 0.31129667,\n",
       "       0.32272449, 0.33168545, 0.33816688, 0.34421001, 0.34939129,\n",
       "       0.35412811, 0.35790579, 0.36145969, 0.36481079, 0.36796535,\n",
       "       0.3709693 , 0.37381048, 0.37654066, 0.37892266, 0.38128434,\n",
       "       0.38355732, 0.38573246, 0.38787214, 0.38996681, 0.39201513,\n",
       "       0.39393495, 0.39577018, 0.39753914, 0.39924786, 0.40091947,\n",
       "       0.40251418, 0.40408101, 0.40563205, 0.40715363, 0.40864418,\n",
       "       0.41009275, 0.41151715, 0.41291575, 0.41428276, 0.4156209 ,\n",
       "       0.41692581, 0.41818944, 0.41941626, 0.4206308 , 0.42183602,\n",
       "       0.42301205, 0.42417439, 0.42530795, 0.42642577, 0.42753769,\n",
       "       0.42862981, 0.4297012 , 0.43074982, 0.43178112, 0.43281107,\n",
       "       0.43382522, 0.434825  , 0.43580279, 0.43677693, 0.43773492,\n",
       "       0.43868205, 0.43962079, 0.44054526, 0.44145286, 0.44236078,\n",
       "       0.44325366, 0.44413545, 0.4450134 , 0.44587698, 0.44672994,\n",
       "       0.44757815, 0.44841497, 0.44924205, 0.45006438, 0.45087095,\n",
       "       0.45167476, 0.45246745, 0.45326091, 0.45404195, 0.45482063,\n",
       "       0.45558687, 0.45634511, 0.45709084, 0.45783477, 0.45856918,\n",
       "       0.45929436, 0.4600164 , 0.46073887, 0.46145237, 0.46215905,\n",
       "       0.46286149, 0.46355766, 0.46424323, 0.46492155, 0.46559893,\n",
       "       0.4662713 , 0.46693426, 0.46759437, 0.46824951, 0.4688893 ])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expl_var = np.cumsum(netflix_svd.explained_variance_ratio_)\n",
    "expl_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It basically is the gain of variance explained, if we add one additional latent factor to it via np.cumsum()\n",
    "- By adding one by one latent factore to it,___gain in explained variance__ is decreasing.\n",
    "- To take it to greter than 0.60, we have to take almost 400-500+ latent factors. It's totally us-less (more compute power and memory loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, figsize=(10, 12))\n",
    "\n",
    "ax1.set_ylabel(\"Cummulative Variance Explained\")\n",
    "ax1.set_xlabel(\"Number of Latent Facors\")\n",
    "ax1.plot(expl_var)\n",
    "# annote some (latentfactors, expl_var) to make it clear\n",
    "ind = [1, 2, 4, 8, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "ax1.scatter(x = [i-1 for i in ind], y = expl_var[[i-1 for i in ind]], c='#ff3300')\n",
    "for i in ind:\n",
    "    ax1.annotate(\"({}, {})\".format(i, np.round(expl_var[i-1], 2)), xy=(i-1, expl_var[i-1]),\n",
    "                xytext = ( i+20, expl_var[i-1] - 0.01),fontweight='bold')\n",
    "\n",
    "change_in_expl_var = [expl_var[i+1] - expl_var[i] for i in range(len(expl_var)-1)]\n",
    "ax2.plot(change_in_expl_var)\n",
    "\n",
    "ax2.set_ylabel(\"Increment in Cummulative Variance with One Additional Latent Factor\", fontsize=10)\n",
    "ax2.yaxis.set_label_position(\"right\")\n",
    "ax2.set_xlabel(\"Number of Latent Factor\")\n",
    "\n",
    "plt.savefig('img/netflix_svd-expl-var.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are not getting benifitted from adding one latent factor each time. This is what is shown in the plots (specially the bottom plot, it gets almost flatten after that knee)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 0.23)\n",
      "(2, 0.26)\n",
      "(4, 0.3)\n",
      "(8, 0.34)\n",
      "(10, 0.35)\n",
      "(20, 0.38)\n",
      "(30, 0.4)\n",
      "(40, 0.42)\n",
      "(50, 0.43)\n",
      "(60, 0.44)\n",
      "(70, 0.45)\n",
      "(80, 0.45)\n",
      "(90, 0.46)\n",
      "(100, 0.47)\n"
     ]
    }
   ],
   "source": [
    "for i in ind:\n",
    "    print(\"({}, {})\".format(i, np.round(expl_var[i-1], 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:03.645870\n"
     ]
    }
   ],
   "source": [
    "# Project Original U_M matrix into into 100 Dimensional space...\n",
    "start = datetime.now()\n",
    "trunc_matrix = train_sparse_matrix.dot(netflix_svd.components_.T)\n",
    "print(datetime.now()- start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, (2649430, 100))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(trunc_matrix), trunc_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trunc_sparse_matrix.npz already exists. Loading it...\n",
      "0:00:01.438214\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isfile('trunc_sparse_matrix.npz'):\n",
    "    trunc_sparse_matrix = sparse.csr_matrix(trunc_matrix)\n",
    "    sparse.save_npz('trunc_sparse_matrix', trunc_sparse_matrix)\n",
    "else:\n",
    "    print(\"trunc_sparse_matrix.npz already exists. Loading it...\")\n",
    "    start = datetime.now()\n",
    "    trunc_sparse_matrix = sparse.load_npz('trunc_sparse_matrix.npz')\n",
    "    print(datetime.now()- start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2649430, 100)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trunc_sparse_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing strted for top 50 similarities for each user...\n",
      "Computing done for 10 users [  time elapsed : 0:00:07.850645  ]\n",
      "Computing done for 20 users [  time elapsed : 0:00:18.650507  ]\n",
      "Computing done for 30 users [  time elapsed : 0:00:26.201450  ]\n",
      "Computing done for 40 users [  time elapsed : 0:00:34.316376  ]\n",
      "Computing done for 50 users [  time elapsed : 0:00:41.799720  ]\n",
      "Creating Sparse matrix from the computed similarities\n",
      "time: 0:00:44.379248\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "trunc_u_u_sim_matrix, _ = compute_user_similarity(trunc_sparse_matrix, compute_for_few=True, top=50, verbose=True, \n",
    "                                                 verb_for_n_rows=10)\n",
    "\n",
    "print(\"time:\",datetime.now()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Time taken per user = 0:00:44.379248 / 50 = **0.88 seconds**\n",
    "-  We have total users = 405041, which means u-u similarity presize computation would take 405041*0.88 = 4.125 days\n",
    "- No doubt, svd has decreased the time of computation, but 4+ days time is also a very long time. It would take lot of memory and computation power, which is very very hard to execute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative/Modification to traditional SVD\n",
    "But one drawback i noticed in my above method is, it re-calculate the similarities of a user with another user in some iterations.\n",
    "To minimize/optimize it:\n",
    "- I will maintain a binary Vector for users, which tells us whether program has already computed top(say, 100) similarities for a user or not.\n",
    "-  **If not** : Compute top (say, 100) most similar users for this user, and add this to our datastructure, so that we can just access it(similar users) without recomputing it again. The way which i did above\n",
    "- But **If It is already Computed** : Just get it directly from our datastructure. In due time,i might have to recompute similarities, if it is computed a long time ago. Because user preferences changes over time. \n",
    "- So, program could maintain some kind of **Timer**, which when expires, we have to update it ( recompute it ).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Movie - Movie collaborative filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "m_m_sim_sparse.npz is a  (17771, 17771)  dimensional matrix\n",
      "0:06:13.093896\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "if not os.path.isfile('m_m_sim_sparse.npz'):\n",
    "    start = datetime.now()\n",
    "    m_m_sim_sparse = cosine_similarity(X=train_sparse_matrix.T, dense_output=False)\n",
    "    # store this sparse matrix in disk before using it. For future purposes.\n",
    "    sparse.save_npz(\"m_m_sim_sparse.npz\", m_m_sim_sparse)\n",
    "    print(\"Done.\")\n",
    "else:\n",
    "    print(\"m_m_sim_saprse.npz is there already, Loading it...\")\n",
    "    m_m_sim_sparse = sparse.load_npz(\"m_m_sim_sparse.npz\")\n",
    "    print(\"Done.\")\n",
    "\n",
    "# print(\"m_m_sim_sparse.npz is a \",m_m_sim_sparse.shape,\" dimensional matrix\")\n",
    "\n",
    "print(datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17771, 17771)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_m_sim_sparse.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Even though we have similarity measure of each movie, with all other movies. But generally one don't care much about least similar movies.\n",
    "- Most of the times platforms recommends only top_xx similar items (here, item = movie). It may be top 10 or 100.\n",
    "- So, its better to take only top similar movie ratings and store them in a saperate dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_ids = np.unique(m_m_sim_sparse.nonzero()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17424"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(movie_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "m_m_sim_sparse is based on training dataset, so 0.8*17771 = 17424"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:20.529888\n",
      "Similar movies for movie id 12711 are :\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 9958, 10439, 11825,  8666,  5963, 15896,   471, 16157,  1708,\n",
       "        9001, 14687,  9810, 13147,  5231, 13374, 15193,  9103,  7660,\n",
       "       11111, 12871, 15359, 12061,   370,  8708, 16670, 11586,  9734,\n",
       "       16500,  3387, 14828, 10476,  4802,  6604, 13096, 14372,   925,\n",
       "        4804,  9334,  3987, 16238, 12585,  3855,  5742,  4116,   840,\n",
       "        3554, 14183,  6669, 13742,  2525,  8626, 14704,  5948, 13647,\n",
       "       13149, 17184,  4249, 14057, 17257,  5986,   990, 10360, 17001,\n",
       "        3868,  6086, 10273,  7220,  4253, 12139, 14262, 12702, 12182,\n",
       "        2644,  4004,  7006, 14063,  1327,   974, 17389,  2408,  8890,\n",
       "        8864,  1074, 12250,  5605, 11020, 12645,  6438,  4419,  7960,\n",
       "        8539, 11854,  6710, 14719,   261,  3164,  9373,  8522,  9106,\n",
       "       17275], dtype=int64)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "similar_movies = dict() \n",
    "for movie in movie_ids:\n",
    "    # get the top similar movies and store them in the dictionary\n",
    "    sim_movies = m_m_sim_sparse[movie].toarray().ravel().argsort()[::-1][1:]\n",
    "    similar_movies[movie] = sim_movies[:100]\n",
    "print(datetime.now() - start)\n",
    "\n",
    "# just testing similar movies for movie_id=\n",
    "movie=random.choice(movie_ids)\n",
    "print(f\"Similar movies for movie id {movie} are :\\n\")\n",
    "similar_movies[movie]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To verify whether these movies are actually similar? \n",
    " - #### I am using netflix's movie_titles.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization took: 4.00 ms\n",
      "Type conversion took: 5.57 ms\n",
      "Parser memory cleanup took: 0.00 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shory\\AppData\\Local\\Temp\\ipykernel_32476\\3168960363.py:1: FutureWarning: The 'verbose' keyword in pd.read_csv is deprecated and will be removed in a future version.\n",
      "  movie_titles = pd.read_csv(\"data/movie_titles.csv\", sep=',', header = None,\n"
     ]
    }
   ],
   "source": [
    "movie_titles = pd.read_csv(\"data/movie_titles.csv\", sep=',', header = None,\n",
    "                           names=['movie_id', 'year_of_release', 'title'],\n",
    "                           usecols=[0, 1, 2], verbose=True,\n",
    "                      index_col = 'movie_id', encoding = \"ISO-8859-1\")  \n",
    "#encoding necessary as movie_titles.csv has characters outside ASCII range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year_of_release</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movie_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2003.0</td>\n",
       "      <td>Dinosaur Planet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2004.0</td>\n",
       "      <td>Isle of Man TT 2004 Review</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1997.0</td>\n",
       "      <td>Character</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>Paula Abdul's Get Up &amp; Dance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2004.0</td>\n",
       "      <td>The Rise and Fall of ECW</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          year_of_release                         title\n",
       "movie_id                                               \n",
       "1                  2003.0               Dinosaur Planet\n",
       "2                  2004.0    Isle of Man TT 2004 Review\n",
       "3                  1997.0                     Character\n",
       "4                  1994.0  Paula Abdul's Get Up & Dance\n",
       "5                  2004.0      The Rise and Fall of ECW"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_titles.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check : I am searching similar movies for movie id = 67 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movie id 67 corresponds to  Vampire Journals\n",
      "It has 270 Ratings from users.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movide id = 67 have 17284 movies which are similar to this and but only top 100 most similar ones are of interest.\n"
     ]
    }
   ],
   "source": [
    "mv_id = 67\n",
    "\n",
    "print(f\"Movie id {mv_id} corresponds to \",movie_titles.loc[mv_id].values[1])\n",
    "\n",
    "print(\"It has {} Ratings from users.\".format(train_sparse_matrix[:,mv_id].getnnz()))\n",
    "\n",
    "print(f\"Movide id = {mv_id}\" + \" have {} movies which are similar to this and but only top 100 most similar ones are of interest.\".format(m_m_sim_sparse[:,mv_id].getnnz()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities = m_m_sim_sparse[mv_id].toarray().ravel()\n",
    "similar_indices = similarities.argsort()[::-1][1:]\n",
    "similarities[similar_indices]\n",
    "\n",
    "sim_indices = similarities.argsort()[::-1][1:] \n",
    "# It will sort and reverse the array and ignore its similarity (i.e. 1)\n",
    "# and return its indices (movie_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(similarities[sim_indices], label='All the ratings')\n",
    "plt.plot(similarities[sim_indices[:100]], label='top 100 similar movies')\n",
    "plt.title(\"Similar Movies of {}(movie_id)\".format(mv_id), fontsize=20)\n",
    "plt.xlabel(\"Movies (Not Movie_Ids)\", fontsize=15)\n",
    "plt.ylabel(\"Cosine Similarity\",fontsize=15)\n",
    "plt.legend()\n",
    "plt.savefig(\"img/similar-movies-for-movieId-67.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year_of_release</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movie_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>1999.0</td>\n",
       "      <td>Modern Vampires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4044</th>\n",
       "      <td>1998.0</td>\n",
       "      <td>Subspecies 4: Bloodstorm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1688</th>\n",
       "      <td>1993.0</td>\n",
       "      <td>To Sleep With a Vampire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13962</th>\n",
       "      <td>2001.0</td>\n",
       "      <td>Dracula: The Dark Prince</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12053</th>\n",
       "      <td>1993.0</td>\n",
       "      <td>Dracula Rising</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16279</th>\n",
       "      <td>2002.0</td>\n",
       "      <td>Vampires: Los Muertos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4667</th>\n",
       "      <td>1996.0</td>\n",
       "      <td>Vampirella</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1900</th>\n",
       "      <td>1997.0</td>\n",
       "      <td>Club Vampire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13873</th>\n",
       "      <td>2001.0</td>\n",
       "      <td>The Breed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15867</th>\n",
       "      <td>2003.0</td>\n",
       "      <td>Dracula II: Ascension</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          year_of_release                     title\n",
       "movie_id                                           \n",
       "323                1999.0           Modern Vampires\n",
       "4044               1998.0  Subspecies 4: Bloodstorm\n",
       "1688               1993.0   To Sleep With a Vampire\n",
       "13962              2001.0  Dracula: The Dark Prince\n",
       "12053              1993.0            Dracula Rising\n",
       "16279              2002.0     Vampires: Los Muertos\n",
       "4667               1996.0                Vampirella\n",
       "1900               1997.0              Club Vampire\n",
       "13873              2001.0                 The Breed\n",
       "15867              2003.0     Dracula II: Ascension"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top 10 similar movies for moive_id = 67\n",
    "movie_titles.loc[sim_indices[:10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This same approach could be applied for **user-user** similarity too, then one could get top 10/100 etc similar users (except itself)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
