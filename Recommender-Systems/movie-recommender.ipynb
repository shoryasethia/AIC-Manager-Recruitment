{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SHORYA SETHIA [ 22B2725 ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "Using data from : https://www.kaggle.com/netflix-inc/netflix-prize-data/data\n",
    "It contains:\n",
    "1. combined_data_1.txt\n",
    "2. combined_data_2.txt\n",
    "3. combined_data_3.txt\n",
    "4. combined_data_4.txt\n",
    "5. movie_titles.csv\n",
    "\n",
    "### Data Overview\n",
    "The first line of each file combined_data_{i}.txt contains the movie id followed by a colon. Each subsequent line in the file corresponds to a rating from a customer and its date in the format: CustomerID,Rating,Date\n",
    "\n",
    "- MovieIDs range from 1 to 17770 sequentially.\n",
    "- CustomerIDs range from 1 to 2649429, with gaps. There are 480189 users.\n",
    "- Ratings are on a five star (integral) scale from 1 to 5.\n",
    "- Dates have the format YYYY-MM-DD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is just to know how much time will it take to run this entire ipython notebook \n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('nbagg')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'figure.max_open_warning': 0})\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "import os\n",
    "from scipy import sparse\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import TimeSeriesSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "### Converting entire data to following format:\n",
    "u_i,m_j,r_ij"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken : 0:00:00\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "if not os.path.isfile('data.csv'):\n",
    "    # Create a file 'data.csv' before reading it\n",
    "    # Read all the files in Netflix Prize Data and store them in one big file('data.csv')\n",
    "    # I am Re-reading from each of the four files and appendig each rating to a global file 'train.csv'\n",
    "    data = open('data.csv', mode='w')\n",
    "    \n",
    "    row = list()\n",
    "    files=['data/combined_data_1.txt','data/combined_data_2.txt', \n",
    "           'data/combined_data_3.txt', 'data/combined_data_4.txt']\n",
    "    for file in files:\n",
    "        print(\"Reading ratings from {}...\".format(file))\n",
    "        with open(file) as f:\n",
    "            for line in f: \n",
    "                del row[:] # you don't have to do this.\n",
    "                line = line.strip()\n",
    "                if line.endswith(':'):\n",
    "                    # All below are ratings for this movie, until another movie appears.\n",
    "                    movie_id = line.replace(':', '')\n",
    "                else:\n",
    "                    row = [x for x in line.split(',')]\n",
    "                    row.insert(0, movie_id)\n",
    "                    data.write(','.join(row))\n",
    "                    data.write('\\n')\n",
    "        print(\"Done.\\n\")\n",
    "    data.close()\n",
    "print('Time taken :', datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>1488844</th>\n",
       "      <th>3</th>\n",
       "      <th>2005-09-06</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>822109</td>\n",
       "      <td>5</td>\n",
       "      <td>2005-05-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>885013</td>\n",
       "      <td>4</td>\n",
       "      <td>2005-10-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>30878</td>\n",
       "      <td>4</td>\n",
       "      <td>2005-12-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>823519</td>\n",
       "      <td>3</td>\n",
       "      <td>2004-05-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>893988</td>\n",
       "      <td>3</td>\n",
       "      <td>2005-11-17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   1  1488844  3  2005-09-06\n",
       "0  1   822109  5  2005-05-13\n",
       "1  1   885013  4  2005-10-19\n",
       "2  1    30878  4  2005-12-26\n",
       "3  1   823519  3  2004-05-03\n",
       "4  1   893988  3  2005-11-17"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_data=pd.read_csv('data.csv')\n",
    "loaded_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists. Reading it...\n",
      "Time taken : 0:00:28.001155\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "if not os.path.isfile('sorted_data.csv'):\n",
    "  print(\"creating the dataframe from data.csv file..\")\n",
    "  df = pd.read_csv('data.csv', sep=',', names=['movie', 'user','rating','date'])\n",
    "  df.date = pd.to_datetime(df.date)\n",
    "  print('Done.\\n')\n",
    "\n",
    "  # I am arranging the ratings according to time\n",
    "  print('Sorting the dataframe by date..')\n",
    "  df.sort_values(by='date', inplace=True)\n",
    "  print('Done..')\n",
    "\n",
    "  output_filename = 'sorted_data.csv'\n",
    "  df.to_csv(output_filename, index=False)\n",
    "\n",
    "else:\n",
    "  print(\"File already exists. Reading it...\")\n",
    "  df = pd.read_csv('sorted_data.csv')\n",
    "  \n",
    "print('Time taken :', datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100480507, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100480507 entries, 0 to 100480506\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Dtype \n",
      "---  ------  ----- \n",
      " 0   movie   int64 \n",
      " 1   user    int64 \n",
      " 2   rating  int64 \n",
      " 3   date    object\n",
      "dtypes: int64(3), object(1)\n",
      "memory usage: 3.0+ GB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie</th>\n",
       "      <th>user</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10341</td>\n",
       "      <td>510180</td>\n",
       "      <td>4</td>\n",
       "      <td>1999-11-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1798</td>\n",
       "      <td>510180</td>\n",
       "      <td>5</td>\n",
       "      <td>1999-11-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10774</td>\n",
       "      <td>510180</td>\n",
       "      <td>3</td>\n",
       "      <td>1999-11-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8651</td>\n",
       "      <td>510180</td>\n",
       "      <td>2</td>\n",
       "      <td>1999-11-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14660</td>\n",
       "      <td>510180</td>\n",
       "      <td>2</td>\n",
       "      <td>1999-11-11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   movie    user  rating        date\n",
       "0  10341  510180       4  1999-11-11\n",
       "1   1798  510180       5  1999-11-11\n",
       "2  10774  510180       3  1999-11-11\n",
       "3   8651  510180       2  1999-11-11\n",
       "4  14660  510180       2  1999-11-11"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1.004805e+08\n",
       "mean     3.604290e+00\n",
       "min      1.000000e+00\n",
       "25%      3.000000e+00\n",
       "50%      4.000000e+00\n",
       "75%      4.000000e+00\n",
       "max      5.000000e+00\n",
       "std      1.085219e+00\n",
       "Name: rating, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()['rating']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Nan values in our dataframe :  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Nan values in our dataframe : \", sum(df.isnull().any()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deleting Duplicates either movie_id, user/customer_id, ratings, date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 duplicate rating entries in the data..\n"
     ]
    }
   ],
   "source": [
    "dup_bool = df.duplicated(['movie','user','rating'])\n",
    "dups = sum(dup_bool) \n",
    "print(\"There are {} duplicate rating entries in the data..\".format(dups))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Users, movies and ratings in sorted_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total No of Users   : 480189\n",
      "Total No of movies  : 17770\n",
      "Total no of ratings : 100480507\n"
     ]
    }
   ],
   "source": [
    "print(\"Total No of Users   :\", len(np.unique(df.user)))\n",
    "print(\"Total No of movies  :\", len(np.unique(df.movie)))\n",
    "print(\"Total no of ratings :\",df.shape[0]) #total rows == no. of ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spliting data into Train and Test (0.80 : 0.20 respectively)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.csv exists\n",
      "test.csv exists\n",
      "read both csv\n",
      "Time taken : 0:00:55.331386\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isfile('train.csv'):\n",
    "    # create the dataframe and store it as csv for further purposes\n",
    "    df.iloc[:int(df.shape[0]*0.80)].to_csv(\"train.csv\", index=False)\n",
    "    print(\"train.csv formed.\")\n",
    "else :\n",
    "    print(\"train.csv exists\")\n",
    "\n",
    "if not os.path.isfile('test.csv'):\n",
    "    # create the dataframe and store it as csv for further purposes\n",
    "    df.iloc[int(df.shape[0]*0.80):].to_csv(\"test.csv\", index=False)\n",
    "    print(\"test.csv formed.\")\n",
    "else :\n",
    "    print(\"test.csv exists\")\n",
    "\n",
    "start = datetime.now()\n",
    "train_df = pd.read_csv(\"train.csv\", parse_dates=['date'])\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "print(\"read both csv\")\n",
    "print('Time taken :', datetime.now() - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie</th>\n",
       "      <th>user</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10341</td>\n",
       "      <td>510180</td>\n",
       "      <td>4</td>\n",
       "      <td>1999-11-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1798</td>\n",
       "      <td>510180</td>\n",
       "      <td>5</td>\n",
       "      <td>1999-11-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10774</td>\n",
       "      <td>510180</td>\n",
       "      <td>3</td>\n",
       "      <td>1999-11-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8651</td>\n",
       "      <td>510180</td>\n",
       "      <td>2</td>\n",
       "      <td>1999-11-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14660</td>\n",
       "      <td>510180</td>\n",
       "      <td>2</td>\n",
       "      <td>1999-11-11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   movie    user  rating       date\n",
       "0  10341  510180       4 1999-11-11\n",
       "1   1798  510180       5 1999-11-11\n",
       "2  10774  510180       3 1999-11-11\n",
       "3   8651  510180       2 1999-11-11\n",
       "4  14660  510180       2 1999-11-11"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Users, Movies and ratings in train.csv and test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numbers for train.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total No of Users   : 405041\n",
      "Total No of movies  : 17424\n",
      "Total no of ratings : 80384405\n",
      "\n",
      "Numbers for test.csv\n",
      "Total No of Users   : 349312\n",
      "Total No of movies  : 17757\n",
      "Total no of ratings : 20096102\n"
     ]
    }
   ],
   "source": [
    "print(\"Numbers for train.csv\")\n",
    "print(\"Total No of Users   :\", len(np.unique(train_df.user)))\n",
    "print(\"Total No of movies  :\", len(np.unique(train_df.movie)))\n",
    "print(\"Total no of ratings :\",train_df.shape[0])\n",
    "\n",
    "print(\"\\nNumbers for test.csv\")\n",
    "print(\"Total No of Users   :\", len(np.unique(test_df.user)))\n",
    "print(\"Total No of movies  :\", len(np.unique(test_df.movie)))\n",
    "print(\"Total no of ratings :\",test_df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA on trian_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to make y-axis more readable\n",
    "def human(num, units = 'M'):\n",
    "    units = units.lower()\n",
    "    num = float(num)\n",
    "    if units == 'k':\n",
    "        return str(num/10**3) + \" K\"\n",
    "    elif units == 'm':\n",
    "        return str(num/10**6) + \" M\"\n",
    "    elif units == 'b':\n",
    "        return str(num/10**9) +  \" B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rating Distribution ploting was taking very long time\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# plt.title('Distribution of ratings over Training dataset', fontsize=15)\n",
    "# sns.countplot(train_df.rating)\n",
    "# ax.set_yticklabels([human(item, 'M') for item in ax.get_yticks()])\n",
    "# ax.set_ylabel('No. of Ratings(Millions)')\n",
    "\n",
    "# plt.savefig('img/rating-distribution-train_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of ratings over Training dataset:\n",
      "rating\n",
      "4    27161596\n",
      "3    23339084\n",
      "5    17772845\n",
      "2     8369795\n",
      "1     3741085\n",
      "Name: count, dtype: int64\n",
      "Time taken: 0:00:00.308258\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "rating_counts = train_df['rating'].value_counts()\n",
    "print(\"Distribution of ratings over Training dataset:\")\n",
    "print(rating_counts)\n",
    "print('Time taken:', datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie</th>\n",
       "      <th>user</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>day_of_week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10341</td>\n",
       "      <td>510180</td>\n",
       "      <td>4</td>\n",
       "      <td>1999-11-11</td>\n",
       "      <td>Thursday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1798</td>\n",
       "      <td>510180</td>\n",
       "      <td>5</td>\n",
       "      <td>1999-11-11</td>\n",
       "      <td>Thursday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10774</td>\n",
       "      <td>510180</td>\n",
       "      <td>3</td>\n",
       "      <td>1999-11-11</td>\n",
       "      <td>Thursday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8651</td>\n",
       "      <td>510180</td>\n",
       "      <td>2</td>\n",
       "      <td>1999-11-11</td>\n",
       "      <td>Thursday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14660</td>\n",
       "      <td>510180</td>\n",
       "      <td>2</td>\n",
       "      <td>1999-11-11</td>\n",
       "      <td>Thursday</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   movie    user  rating       date day_of_week\n",
       "0  10341  510180       4 1999-11-11    Thursday\n",
       "1   1798  510180       5 1999-11-11    Thursday\n",
       "2  10774  510180       3 1999-11-11    Thursday\n",
       "3   8651  510180       2 1999-11-11    Thursday\n",
       "4  14660  510180       2 1999-11-11    Thursday"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Add new column (week day) to the data\n",
    "# train_df['day_of_week'] = train_df.date.dt.weekday_name\n",
    "# train_df.head()\n",
    "\n",
    "# Add new column (week day) to the data\n",
    "train_df['day_of_week'] = train_df['date'].dt.day_name()\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ratings\n",
      "day_of_week\n",
      "Friday       3.585274\n",
      "Monday       3.577250\n",
      "Saturday     3.591791\n",
      "Sunday       3.594144\n",
      "Thursday     3.582463\n",
      "Tuesday      3.574438\n",
      "Wednesday    3.583751\n",
      "Name: rating, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "avg_week_df = train_df.groupby(by=['day_of_week'])['rating'].mean()\n",
    "print(\"Average ratings\")\n",
    "print(avg_week_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shory\\AppData\\Local\\Temp\\ipykernel_7920\\455898017.py:6: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_yticklabels([human(item, 'M') for item in ax.get_yticks()])\n"
     ]
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "sns.countplot(x='day_of_week', data=train_df, ax=ax)\n",
    "plt.title('No of ratings on each day.')\n",
    "plt.ylabel('Total no of ratings')\n",
    "plt.xlabel('')\n",
    "ax.set_yticklabels([human(item, 'M') for item in ax.get_yticks()])\n",
    "plt.savefig('img/no.-of-rating-on-each-day_of_week-train_df.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = train_df.resample('m', on='date')['rating'].count().plot()\n",
    "ax.set_title('No of ratings per month (Training data)')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('No of ratings(per month)')\n",
    "ax.set_yticklabels([human(item, 'M') for item in ax.get_yticks()])\n",
    "plt.savefig('img/no.-of-ratings-per-month-train_df.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis on ratings given by a user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user\n",
       "305344     17112\n",
       "2439493    15896\n",
       "387418     15402\n",
       "1639792     9767\n",
       "1461435     9447\n",
       "Name: rating, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_of_rated_movies_per_user = train_df.groupby(by='user')['rating'].count().sort_values(ascending=False)\n",
    "no_of_rated_movies_per_user.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shory\\AppData\\Local\\Temp\\ipykernel_33132\\4202440675.py:4: FutureWarning: \n",
      "\n",
      "`shade` is now deprecated in favor of `fill`; setting `fill=True`.\n",
      "This will become an error in seaborn v0.14.0; please update your code.\n",
      "\n",
      "  sns.kdeplot(no_of_rated_movies_per_user, shade=True, ax=ax1)\n",
      "C:\\Users\\shory\\AppData\\Local\\Temp\\ipykernel_33132\\4202440675.py:9: FutureWarning: \n",
      "\n",
      "`shade` is now deprecated in favor of `fill`; setting `fill=True`.\n",
      "This will become an error in seaborn v0.14.0; please update your code.\n",
      "\n",
      "  sns.kdeplot(no_of_rated_movies_per_user, shade=True, cumulative=True,ax=ax2)\n"
     ]
    }
   ],
   "source": [
    "fig = plt.figure(figsize=plt.figaspect(.5))\n",
    "\n",
    "ax1 = plt.subplot(121)\n",
    "sns.kdeplot(no_of_rated_movies_per_user, shade=True, ax=ax1)\n",
    "plt.xlabel('No of ratings by user')\n",
    "plt.title(\"PDF\")\n",
    "\n",
    "ax2 = plt.subplot(122)\n",
    "sns.kdeplot(no_of_rated_movies_per_user, shade=True, cumulative=True,ax=ax2)\n",
    "plt.xlabel('No of ratings by user')\n",
    "plt.title('CDF')\n",
    "\n",
    "plt.savefig('img/pdf-cdf-rating-by-user-train_df.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above warning is just about to use \"fill\" in place of \"shade\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    405041.000000\n",
       "mean        198.459921\n",
       "std         290.793238\n",
       "min           1.000000\n",
       "25%          34.000000\n",
       "50%          89.000000\n",
       "75%         245.000000\n",
       "max       17112.000000\n",
       "Name: rating, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_of_rated_movies_per_user.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00        1\n",
       "0.01        1\n",
       "0.02        2\n",
       "0.03        4\n",
       "0.04        5\n",
       "        ...  \n",
       "0.96      829\n",
       "0.97      934\n",
       "0.98     1079\n",
       "0.99     1341\n",
       "1.00    17112\n",
       "Name: rating, Length: 101, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantiles = no_of_rated_movies_per_user.quantile(np.arange(0,1.01,0.01), interpolation='higher')\n",
    "quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Quantiles and their Values\")\n",
    "quantiles.plot()\n",
    "# quantiles with 0.05 difference\n",
    "plt.scatter(x=quantiles.index[::5], y=quantiles.values[::5], c='orange', label=\"quantiles with 0.05 intervals\")\n",
    "# quantiles with 0.25 difference\n",
    "plt.scatter(x=quantiles.index[::25], y=quantiles.values[::25], c='m', label = \"quantiles with 0.25 intervals\")\n",
    "plt.ylabel('No of ratings by user')\n",
    "plt.xlabel('Value at the quantile')\n",
    "plt.legend(loc='best')\n",
    "\n",
    "# annotate the 25th, 50th, 75th and 100th percentile values....\n",
    "for x,y in zip(quantiles.index[::25], quantiles[::25]):\n",
    "    s= s=\"({} , {})\".format(x,y)\n",
    "    plt.annotate(s, xy=(x,y), xytext=(x-0.05, y+500)\n",
    "                ,fontweight='bold')\n",
    "\n",
    "plt.savefig('img/quantiles.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00        1\n",
       "0.05        7\n",
       "0.10       15\n",
       "0.15       21\n",
       "0.20       27\n",
       "0.25       34\n",
       "0.30       41\n",
       "0.35       50\n",
       "0.40       60\n",
       "0.45       73\n",
       "0.50       89\n",
       "0.55      109\n",
       "0.60      133\n",
       "0.65      163\n",
       "0.70      199\n",
       "0.75      245\n",
       "0.80      307\n",
       "0.85      392\n",
       "0.90      520\n",
       "0.95      749\n",
       "1.00    17112\n",
       "Name: rating, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantiles[::5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_ratings_per_movie = train_df.groupby(by='movie')['rating'].count().sort_values(ascending=False)\n",
    "\n",
    "fig = plt.figure(figsize=plt.figaspect(.5))\n",
    "ax = plt.gca()\n",
    "plt.plot(no_of_ratings_per_movie.values)\n",
    "plt.title('# RATINGS per Movie')\n",
    "plt.xlabel('Movie')\n",
    "plt.ylabel('No of Users who rated a movie')\n",
    "ax.set_xticklabels([])\n",
    "\n",
    "plt.savefig('img/per-movie-ratings-train_df.png')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There are some (<10%) movies which are rated by huge number of users.\n",
    "- But majority movies exists which are rated by some hundereds of users. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building sparse matrices from data\n",
    "\n",
    "- Present data has 4 columns, user, movie, ratings and date; for each movie there are many users and each user gives rating.\n",
    "- This takes lot of memory.\n",
    "- To minimize usage of memory, I am creating two arrays, one for movies(m_i's) and one for users(u_j's), by some matrix operation (generally dot product) would give me rating (r_ij's)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is present in pwd, loading it\n",
      "Done. It's shape is : (user, movie) :  (2649430, 17771)\n",
      "0:00:02.099595\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "if os.path.isfile('train_sparse_matrix.npz'):\n",
    "    print(\"It is present in pwd, loading it\")\n",
    "    train_sparse_matrix = sparse.load_npz('train_sparse_matrix.npz')\n",
    "    print('Done. It\\'s shape is : (user, movie) : ',train_sparse_matrix.shape)\n",
    "else: \n",
    "    print(\"Building sparse_matrix from the dataframe...\")\n",
    "    # create sparse_matrix and store it for after usage.\n",
    "    # csr_matrix(data_values, (row_index, col_index), shape_of_matrix)\n",
    "    # It should be in such a way that, MATRIX[row, col] = data\n",
    "    train_sparse_matrix = sparse.csr_matrix((train_df.rating.values, (train_df.user.values,\n",
    "                                               train_df.movie.values)),)\n",
    "    \n",
    "    print('Done. It\\'s shape is : (user, movie) : ',train_sparse_matrix.shape)\n",
    "    print('Saving it into pwd for further usages...')\n",
    "\n",
    "    sparse.save_npz(\"train_sparse_matrix.npz\", train_sparse_matrix)\n",
    "    print('Done.\\n')\n",
    "\n",
    "print(datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is present in pwd, loading it.\n",
      "Done. It's shape is : (user, movie) :  (2649430, 17771)\n",
      "0:00:00.521067\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "if os.path.isfile('test_sparse_matrix.npz'):\n",
    "    print(\"It is present in pwd, loading it.\")\n",
    "    test_sparse_matrix = sparse.load_npz('test_sparse_matrix.npz')\n",
    "    print('Done. It\\'s shape is : (user, movie) : ',test_sparse_matrix.shape)\n",
    "else: \n",
    "    print(\"Building sparse_matrix from the dataframe...\")\n",
    "    # create sparse_matrix and store it for after usage.\n",
    "    # csr_matrix(data_values, (row_index, col_index), shape_of_matrix)\n",
    "    # It should be in such a way that, MATRIX[row, col] = data\n",
    "    test_sparse_matrix = sparse.csr_matrix((test_df.rating.values, (test_df.user.values,\n",
    "                                               test_df.movie.values)))\n",
    "    \n",
    "    print('Done. It\\'s shape is : (user, movie) : ',test_sparse_matrix.shape)\n",
    "    print('Saving it into pwd for further usages...')\n",
    "\n",
    "    sparse.save_npz(\"test_sparse_matrix.npz\", test_sparse_matrix)\n",
    "    print('Done.')\n",
    "    \n",
    "print(datetime.now() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparsity = (Number of Zero enteries/Number of total enteries)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity Of Train matrix : 99.8292709259195 % \n"
     ]
    }
   ],
   "source": [
    "us,mv = train_sparse_matrix.shape\n",
    "elem = train_sparse_matrix.count_nonzero()\n",
    "print(\"Sparsity Of Train matrix : {} % \".format((1-(elem/(us*mv)))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity Of Test matrix : 99.95731772988694 % \n"
     ]
    }
   ],
   "source": [
    "us,mv = test_sparse_matrix.shape\n",
    "elem = test_sparse_matrix.count_nonzero()\n",
    "print(\"Sparsity Of Test matrix : {} % \".format(  (1-(elem/(us*mv))) * 100) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Average rating globally, per movie and per user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_ratings(sparse_matrix, of_users):  # of_users is boolean flag (1: users, 0:movies)\n",
    "    \n",
    "    # selecting axes of sparse matrix\n",
    "    ax = 1 if of_users else 0\n",
    "    \n",
    "    sum_of_ratings = sparse_matrix.sum(axis=ax).A1     # \".A1\" for converting Column_Matrix to 1-D numpy array \n",
    "    \n",
    "    # Boolean matrix of ratings (whether a user rated that movie or not)\n",
    "    is_rated = sparse_matrix!=0\n",
    "    \n",
    "    # no of ratings that each user OR movie..\n",
    "    no_of_ratings = is_rated.sum(axis=ax).A1\n",
    "    \n",
    "    u,m = sparse_matrix.shape     # max_user(u)  and max_movie(m) id's in sparse matrix \n",
    "\n",
    "    # average_rating = sum of ratings/sum of non-zero entries\n",
    "    average_ratings = { i : sum_of_ratings[i]/no_of_ratings[i]            \n",
    "                                 for i in range(u if of_users else m) \n",
    "                                    if no_of_ratings[i] !=0}  \n",
    "    \n",
    "    return average_ratings # returns dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Average of Ratings in training data is {'global': 3.582890686321557}\n"
     ]
    }
   ],
   "source": [
    "train_averages = dict()\n",
    "\n",
    "train_global_average = train_sparse_matrix.sum()/train_sparse_matrix.count_nonzero()\n",
    "train_averages['global'] = train_global_average\n",
    "print(f\"Global Average of Ratings in training data is {train_averages}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average rating of user 573242 : 4.138339920948616\n"
     ]
    }
   ],
   "source": [
    "train_averages['user'] = get_average_ratings(train_sparse_matrix, of_users=True)\n",
    "# user = random.randint(1,train_sparse_matrix.shape [0])\n",
    "# print(user)\n",
    "\n",
    "# Generate a random user ID within the valid range\n",
    "valid_users = list(train_averages['user'].keys())  # Get the list of valid user IDs\n",
    "user = random.choice(valid_users) \n",
    "print(f'Average rating of user {user} :',train_averages['user'][user])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average rating of movie 12910 : 2.7708333333333335\n"
     ]
    }
   ],
   "source": [
    "train_averages['movie'] =  get_average_ratings(train_sparse_matrix, of_users=False)\n",
    "\n",
    "valid_movies = list(train_averages['movie'].keys())\n",
    "movie = random.choice(valid_movies)\n",
    "print(f'Average rating of movie {movie} :',train_averages['movie'][movie])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PDF and CDF of avg rating of user and movie in train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:05.615467\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "# Draw PDFs for average rating per user and per movie\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=plt.figaspect(.5))\n",
    "fig.suptitle('Avg Ratings per User and per Movie', fontsize=15)\n",
    "\n",
    "ax1.set_title('Users-Avg-Ratings')\n",
    "# Get the list of average user ratings from the averages dictionary\n",
    "user_averages = [rat for rat in train_averages['user'].values()]\n",
    "sns.kdeplot(user_averages, cumulative=True, ax=ax1, label='Cdf')\n",
    "sns.kdeplot(user_averages, ax=ax1, label='Pdf')\n",
    "\n",
    "ax2.set_title('Movies-Avg-Rating')\n",
    "# Get the list of movie average ratings from the dictionary\n",
    "movie_averages = [rat for rat in train_averages['movie'].values()]\n",
    "sns.kdeplot(movie_averages, cumulative=True, ax=ax2, label='Cdf')\n",
    "sns.kdeplot(movie_averages, ax=ax2, label='Pdf')\n",
    "\n",
    "plt.savefig('img/pdf-cdf-avg-rating-user&movie.png')\n",
    "print(datetime.now() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many new users and movies would I encounter in test_csv ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of Users  : 480189\n",
      "Number of Users in Train data : 405041\n",
      "No of Users that didn't appear in train data: 75148 (15.649671275268695 %) \n",
      " \n"
     ]
    }
   ],
   "source": [
    "total_users = len(np.unique(df.user))\n",
    "users_train = len(train_averages['user'])\n",
    "new_users = total_users - users_train\n",
    "\n",
    "print('Total number of Users  :', total_users)\n",
    "print('Number of Users in Train data :', users_train)\n",
    "print(\"No of Users that didn't appear in train data: {} ({} %) \\n \".format(new_users,(new_users/total_users)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of Movies  : 17770\n",
      "Number of Users in Train data : 17424\n",
      "No of Movies that didn't appear in train data: 346 (1.9471018570624647 %) \n",
      " \n"
     ]
    }
   ],
   "source": [
    "total_movies = len(np.unique(df.movie))\n",
    "movies_train = len(train_averages['movie'])\n",
    "new_movies = total_movies - movies_train\n",
    "\n",
    "print('Total number of Movies  :', total_movies)\n",
    "print('Number of Users in Train data :', movies_train)\n",
    "print(\"No of Movies that didn't appear in train data: {} ({} %) \\n \".format(new_movies,(new_movies/total_movies)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing similarity matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### user - user collaborative filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "def compute_user_similarity(sparse_matrix, compute_for_few=False, top = 100, verbose=False, verb_for_n_rows = 20,\n",
    "                            draw_time_taken=True):\n",
    "    no_of_users = sparse_matrix.shape[0]\n",
    "    # get the indices of  non zero rows (users) from our sparse matrix\n",
    "    row_ind, col_ind = sparse_matrix.nonzero()\n",
    "    row_ind = sorted(set(row_ind)) \n",
    "    time_taken = list() #  time taken for finding similar users for an user\n",
    "    \n",
    "    # Create rows, cols, and data lists.., which can be used to create sparse matrices\n",
    "    rows, cols, data = list(), list(), list()\n",
    "    if verbose: print(\"Computing strted for top\",top,\"similarities for each user...\")\n",
    "    \n",
    "    start = datetime.now()\n",
    "    temp = 0\n",
    "    \n",
    "    for row in row_ind[:top] if compute_for_few else row_ind:\n",
    "        temp = temp+1\n",
    "        prev = datetime.now()\n",
    "        \n",
    "        # get the similarity row for this user with all other users\n",
    "        sim = cosine_similarity(sparse_matrix.getrow(row), sparse_matrix).ravel()\n",
    "        # I will consider only the top 10/20/40/100 etc  most similar users and ignore rest of them..\n",
    "        top_sim_ind = sim.argsort()[-top:]\n",
    "        top_sim_val = sim[top_sim_ind]\n",
    "        \n",
    "        # add them to our rows, cols and data\n",
    "        rows.extend([row]*top)\n",
    "        cols.extend(top_sim_ind)\n",
    "        data.extend(top_sim_val)\n",
    "        time_taken.append(datetime.now().timestamp() - prev.timestamp())\n",
    "        if verbose:\n",
    "            if temp%verb_for_n_rows == 0:\n",
    "                print(\"Computing done for {} users [  time elapsed : {}  ]\"\n",
    "                      .format(temp, datetime.now()-start))\n",
    "            \n",
    "        \n",
    "    # lets create sparse matrix out of these and return it\n",
    "    if verbose: print('Creating Sparse matrix from the computed similarities')\n",
    "    #return rows, cols, data\n",
    "    \n",
    "    if draw_time_taken:\n",
    "        plt.plot(time_taken, label = 'time taken for each user')\n",
    "        plt.plot(np.cumsum(time_taken), label='Total time')\n",
    "        plt.legend(loc='best')\n",
    "        plt.xlabel('User')\n",
    "        plt.ylabel('Time (seconds)')\n",
    "        plt.savefig('img/u-u-cf-17k-dim-per-user.png')\n",
    "        \n",
    "    return sparse.csr_matrix((data, (rows, cols)), shape=(no_of_users, no_of_users)), time_taken "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing strted for top 200 similarities for each user...\n",
      "Computing done for 20 users [  time elapsed : 0:01:04.300235  ]\n",
      "Computing done for 40 users [  time elapsed : 0:01:57.593116  ]\n",
      "Computing done for 60 users [  time elapsed : 0:02:48.430230  ]\n",
      "Computing done for 80 users [  time elapsed : 0:03:38.995090  ]\n",
      "Computing done for 100 users [  time elapsed : 0:04:30.595433  ]\n",
      "Computing done for 120 users [  time elapsed : 0:05:25.550533  ]\n",
      "Computing done for 140 users [  time elapsed : 0:06:17.773366  ]\n",
      "Computing done for 160 users [  time elapsed : 0:07:08.075599  ]\n",
      "Computing done for 180 users [  time elapsed : 0:08:07.425267  ]\n",
      "Computing done for 200 users [  time elapsed : 0:09:08.188622  ]\n",
      "Creating Sparse matrix from the computed similarities\n",
      "Time taken for user-user cf with 17k dimensions per user : 0:09:13.713449\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "u_u_sim_sparse, _ = compute_user_similarity(train_sparse_matrix, compute_for_few=True, top = 200,\n",
    "                                                     verbose=True)\n",
    "print(\"Time taken for user-user cf with 17k dimensions per user :\",datetime.now()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Calculating user-user Similarity_Matrix (user-user collaborative filtering) is not an easy task\n",
    "- For top 200 users it took **0:09:13.713449** time, and as users count increases, complexity increases as one could find more and more similarities. \n",
    "\n",
    "* On avg per time consumed for searching similarity for one user = (9*60 + 13.71)/200 = **2.76 seconds**\n",
    "* training data have 405041 users, so approximately it would take **405041*2.76 = 1117913 seconds = 12.93 days**\n",
    "* It will take almost **13** days to just find similarities !\n",
    "\n",
    "- Hence, i would try to find user-user similarity via reduced dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Truncated SVD for reducing the dimesnion of user vector\n",
    "- SVD basically is a factorization of that matrix into three smaller matrices.\n",
    "- The SVD of mxn matrix A is given by the formula A = U Σ V^T \n",
    "- Where\n",
    "   - U is m*m matrix of orthonormal eigen vectors of AA^T\n",
    "   - V^T is n*n matrix of orthonormal eigen vectors of (A^T)A\n",
    "   - Σ is diagonal matrix with r elements, r = square root of positive eigen values of AA^T (or (A^T)A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "start = datetime.now()\n",
    "\n",
    "# All parameters are default except n_components. n_itr is for Randomized SVD solver.\n",
    "netflix_svd = TruncatedSVD(n_components=100, algorithm='randomized', random_state=42)\n",
    "print(\"Fitting started...\")\n",
    "trunc_svd = netflix_svd.fit_transform(train_sparse_matrix)\n",
    "\n",
    "# num_iterations = 10\n",
    "# for i in range(num_iterations):\n",
    "#     # Fit the TruncatedSVD model for each iteration\n",
    "#     trunc_svd = netflix_svd.fit_transform(train_sparse_matrix)\n",
    "    \n",
    "#     # Print progress update\n",
    "#     print(f\"Iteration {i+1}/{num_iterations} completed\")\n",
    "\n",
    "print(datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.23362135, 0.26270872, 0.28323418, 0.29936103, 0.31129667,\n",
       "       0.32272449, 0.33168545, 0.33816688, 0.34421001, 0.34939129,\n",
       "       0.35412811, 0.35790579, 0.36145969, 0.36481079, 0.36796535,\n",
       "       0.3709693 , 0.37381048, 0.37654066, 0.37892266, 0.38128434,\n",
       "       0.38355732, 0.38573246, 0.38787214, 0.38996681, 0.39201513,\n",
       "       0.39393495, 0.39577018, 0.39753914, 0.39924786, 0.40091947,\n",
       "       0.40251418, 0.40408101, 0.40563205, 0.40715363, 0.40864418,\n",
       "       0.41009275, 0.41151715, 0.41291575, 0.41428276, 0.4156209 ,\n",
       "       0.41692581, 0.41818944, 0.41941626, 0.4206308 , 0.42183602,\n",
       "       0.42301205, 0.42417439, 0.42530795, 0.42642577, 0.42753769,\n",
       "       0.42862981, 0.4297012 , 0.43074982, 0.43178112, 0.43281107,\n",
       "       0.43382522, 0.434825  , 0.43580279, 0.43677693, 0.43773492,\n",
       "       0.43868205, 0.43962079, 0.44054526, 0.44145286, 0.44236078,\n",
       "       0.44325366, 0.44413545, 0.4450134 , 0.44587698, 0.44672994,\n",
       "       0.44757815, 0.44841497, 0.44924205, 0.45006438, 0.45087095,\n",
       "       0.45167476, 0.45246745, 0.45326091, 0.45404195, 0.45482063,\n",
       "       0.45558687, 0.45634511, 0.45709084, 0.45783477, 0.45856918,\n",
       "       0.45929436, 0.4600164 , 0.46073887, 0.46145237, 0.46215905,\n",
       "       0.46286149, 0.46355766, 0.46424323, 0.46492155, 0.46559893,\n",
       "       0.4662713 , 0.46693426, 0.46759437, 0.46824951, 0.4688893 ])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expl_var = np.cumsum(netflix_svd.explained_variance_ratio_)\n",
    "expl_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It basically is the gain of variance explained, if we add one additional latent factor to it via np.cumsum()\n",
    "- By adding one by one latent factore to it,___gain in explained variance__ is decreasing.\n",
    "- To take it to greter than 0.60, we have to take almost 400-500+ latent factors. It's totally us-less (more compute power and memory loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, figsize=(10, 12))\n",
    "\n",
    "ax1.set_ylabel(\"Cummulative Variance Explained\")\n",
    "ax1.set_xlabel(\"Number of Latent Facors\")\n",
    "ax1.plot(expl_var)\n",
    "# annote some (latentfactors, expl_var) to make it clear\n",
    "ind = [1, 2, 4, 8, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "ax1.scatter(x = [i-1 for i in ind], y = expl_var[[i-1 for i in ind]], c='#ff3300')\n",
    "for i in ind:\n",
    "    ax1.annotate(\"({}, {})\".format(i, np.round(expl_var[i-1], 2)), xy=(i-1, expl_var[i-1]),\n",
    "                xytext = ( i+20, expl_var[i-1] - 0.01),fontweight='bold')\n",
    "\n",
    "change_in_expl_var = [expl_var[i+1] - expl_var[i] for i in range(len(expl_var)-1)]\n",
    "ax2.plot(change_in_expl_var)\n",
    "\n",
    "ax2.set_ylabel(\"Increment in Cummulative Variance with One Additional Latent Factor\", fontsize=10)\n",
    "ax2.yaxis.set_label_position(\"right\")\n",
    "ax2.set_xlabel(\"Number of Latent Factor\")\n",
    "\n",
    "plt.savefig('img/netflix_svd-expl-var.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are not getting benifitted from adding one latent factor each time. This is what is shown in the plots (specially the bottom plot, it gets almost flatten after that knee)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 0.23)\n",
      "(2, 0.26)\n",
      "(4, 0.3)\n",
      "(8, 0.34)\n",
      "(10, 0.35)\n",
      "(20, 0.38)\n",
      "(30, 0.4)\n",
      "(40, 0.42)\n",
      "(50, 0.43)\n",
      "(60, 0.44)\n",
      "(70, 0.45)\n",
      "(80, 0.45)\n",
      "(90, 0.46)\n",
      "(100, 0.47)\n"
     ]
    }
   ],
   "source": [
    "for i in ind:\n",
    "    print(\"({}, {})\".format(i, np.round(expl_var[i-1], 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:03.645870\n"
     ]
    }
   ],
   "source": [
    "# Project Original U_M matrix into into 100 Dimensional space...\n",
    "start = datetime.now()\n",
    "trunc_matrix = train_sparse_matrix.dot(netflix_svd.components_.T)\n",
    "print(datetime.now()- start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, (2649430, 100))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(trunc_matrix), trunc_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trunc_sparse_matrix.npz already exists. Loading it...\n",
      "0:00:01.445727\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isfile('trunc_sparse_matrix.npz'):\n",
    "    trunc_sparse_matrix = sparse.csr_matrix(trunc_matrix)\n",
    "    sparse.save_npz('trunc_sparse_matrix', trunc_sparse_matrix)\n",
    "else:\n",
    "    print(\"trunc_sparse_matrix.npz already exists. Loading it...\")\n",
    "    start = datetime.now()\n",
    "    trunc_sparse_matrix = sparse.load_npz('trunc_sparse_matrix.npz')\n",
    "    print(datetime.now()- start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2649430, 100)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trunc_sparse_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing strted for top 50 similarities for each user...\n",
      "Computing done for 10 users [  time elapsed : 0:00:07.767252  ]\n",
      "Computing done for 20 users [  time elapsed : 0:00:15.033117  ]\n",
      "Computing done for 30 users [  time elapsed : 0:00:22.483663  ]\n",
      "Computing done for 40 users [  time elapsed : 0:00:29.950224  ]\n",
      "Computing done for 50 users [  time elapsed : 0:00:37.384297  ]\n",
      "Creating Sparse matrix from the computed similarities\n",
      "time: 0:00:40.147739\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "trunc_u_u_sim_matrix, _ = compute_user_similarity(trunc_sparse_matrix, compute_for_few=True, top=50, verbose=True, \n",
    "                                                 verb_for_n_rows=10)\n",
    "\n",
    "print(\"time:\",datetime.now()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Time taken per user = 0:00:44.379248 / 50 = **0.88 seconds**\n",
    "-  We have total users = 405041, which means u-u similarity presize computation would take 405041*0.88 = 4.125 days\n",
    "- No doubt, svd has decreased the time of computation, but 4+ days time is also a very long time. It would take lot of memory and computation power, which is very very hard to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u_u_sim_sparse.npz already exists. Loading it...\n",
      "0:00:00.067518\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isfile('u_u_sim_sparse.npz'):\n",
    "    # Save the computed user-user similarity matrix\n",
    "    sparse.save_npz(\"u_u_sim_sparse.npz\", trunc_u_u_sim_matrix)\n",
    "else:\n",
    "    print(\"u_u_sim_sparse.npz already exists. Loading it...\")\n",
    "    start = datetime.now()\n",
    "    u_u_sim_sparse = sparse.load_npz('u_u_sim_sparse.npz')\n",
    "    print(datetime.now()- start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative/Modification to traditional SVD\n",
    "But one drawback i noticed in my above method is, it re-calculate the similarities of a user with another user in some iterations.\n",
    "To minimize/optimize it:\n",
    "- I will maintain a binary Vector for users, which tells us whether program has already computed top(say, 100) similarities for a user or not.\n",
    "-  **If not** : Compute top (say, 100) most similar users for this user, and add this to our datastructure, so that we can just access it(similar users) without recomputing it again. The way which i did above\n",
    "- But **If It is already Computed** : Just get it directly from our datastructure. In due time,i might have to recompute similarities, if it is computed a long time ago. Because user preferences changes over time. \n",
    "- So, program could maintain some kind of **Timer**, which when expires, we have to update it ( recompute it ).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Movie - Movie collaborative filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m_m_sim_saprse.npz is there already, Loading it...\n",
      "Done.\n",
      "0:00:14.750535\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "if not os.path.isfile('m_m_sim_sparse.npz'):\n",
    "    start = datetime.now()\n",
    "    m_m_sim_sparse = cosine_similarity(X=train_sparse_matrix.T, dense_output=False)\n",
    "    # store this sparse matrix in disk before using it. For future purposes.\n",
    "    sparse.save_npz(\"m_m_sim_sparse.npz\", m_m_sim_sparse)\n",
    "    print(\"Done.\")\n",
    "else:\n",
    "    print(\"m_m_sim_saprse.npz is there already, Loading it...\")\n",
    "    m_m_sim_sparse = sparse.load_npz(\"m_m_sim_sparse.npz\")\n",
    "    print(\"Done.\")\n",
    "\n",
    "# print(\"m_m_sim_sparse.npz is a \",m_m_sim_sparse.shape,\" dimensional matrix\")\n",
    "\n",
    "print(datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17771, 17771)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_m_sim_sparse.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Even though we have similarity measure of each movie, with all other movies. But generally one don't care much about least similar movies.\n",
    "- Most of the times platforms recommends only top_xx similar items (here, item = movie). It may be top 10 or 100.\n",
    "- So, its better to take only top similar movie ratings and store them in a saperate dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    1,     2,     3, ..., 17768, 17769, 17770])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_ids = np.unique(m_m_sim_sparse.nonzero()[1])\n",
    "movie_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17424"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(movie_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "m_m_sim_sparse is based on training dataset, so 0.8*17771 = 17424"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:27.543420\n",
      "Similar movies for movie id 860 are :\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 3191, 15935, 10309,  7746,  7337, 14221,  1529, 16887,  3258,\n",
       "       12041,   838,  6662,  5831,  4674,  2999,  1989,  1361,  2844,\n",
       "        7017,  3361, 10129,  8495,  3298,  1830,  4462,  2003, 11408,\n",
       "       11906,  3199,   669,  3101, 13976, 13450, 10448, 15604,   245,\n",
       "       12483,   821, 12364, 16250,  7164,  8095, 12357,  6288,  2099,\n",
       "       12389,  6757,  8523, 11951,  9168,  7280, 14369, 16391, 14263,\n",
       "        2289,  7300, 15515,  1846,  2668,  7731, 15581,  3104,  7051,\n",
       "        2513,   622,   274, 10470,  2415, 16071, 12344,  1398, 10658,\n",
       "        4758, 10093,   517, 12119,  7743, 12033, 11996, 12916,    26,\n",
       "       13043, 15405, 16706, 16351, 14674,  9806, 12271, 11870, 10323,\n",
       "        8855,  6005, 10059, 16509, 12499, 14495,  3186, 13597,  2493,\n",
       "       11588], dtype=int64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "similar_movies = dict() \n",
    "for movie in movie_ids:\n",
    "    # get the top similar movies and store them in the dictionary\n",
    "    sim_movies = m_m_sim_sparse[movie].toarray().ravel().argsort()[::-1][1:]\n",
    "    similar_movies[movie] = sim_movies[:100]\n",
    "print(datetime.now() - start)\n",
    "\n",
    "# just testing similar movies for randomly choosing movie_id\n",
    "movie=random.choice(movie_ids)\n",
    "print(f\"Similar movies for movie id {movie} are :\\n\")\n",
    "similar_movies[movie]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To verify whether these movies are actually similar? \n",
    " - #### I am using netflix's movie_titles.csv to get their names and cross check manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization took: 0.81 ms\n",
      "Type conversion took: 9.01 ms\n",
      "Parser memory cleanup took: 0.00 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shory\\AppData\\Local\\Temp\\ipykernel_15432\\3737456495.py:1: FutureWarning: The 'verbose' keyword in pd.read_csv is deprecated and will be removed in a future version.\n",
      "  movie_titles = pd.read_csv(\"Data/movie_titles.csv\", sep=',', header = None,\n"
     ]
    }
   ],
   "source": [
    "movie_titles = pd.read_csv(\"Data/movie_titles.csv\", sep=',', header = None,\n",
    "                           names=['movie_id', 'year_of_release', 'title'],\n",
    "                           usecols=[0, 1, 2], verbose=True,\n",
    "                      index_col = 'movie_id', encoding = \"ISO-8859-1\")  \n",
    "#encoding necessary as movie_titles.csv has characters outside ASCII range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year_of_release</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movie_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2003.0</td>\n",
       "      <td>Dinosaur Planet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2004.0</td>\n",
       "      <td>Isle of Man TT 2004 Review</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1997.0</td>\n",
       "      <td>Character</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>Paula Abdul's Get Up &amp; Dance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2004.0</td>\n",
       "      <td>The Rise and Fall of ECW</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          year_of_release                         title\n",
       "movie_id                                               \n",
       "1                  2003.0               Dinosaur Planet\n",
       "2                  2004.0    Isle of Man TT 2004 Review\n",
       "3                  1997.0                     Character\n",
       "4                  1994.0  Paula Abdul's Get Up & Dance\n",
       "5                  2004.0      The Rise and Fall of ECW"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_titles.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommendations similar movies for a given movie id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movie id 1061 corresponds to  Spider-Man vs. Doc Ock\n",
      "It has 744 Ratings from users.\n",
      "Movide id = 1061 have 17320 movies which are similar to this and but only top 100 most similar ones are of interest.\n"
     ]
    }
   ],
   "source": [
    "mv_id = 1061\n",
    "\n",
    "print(f\"Movie id {mv_id} corresponds to \",movie_titles.loc[mv_id].values[1])\n",
    "\n",
    "print(\"It has {} Ratings from users.\".format(train_sparse_matrix[:,mv_id].getnnz()))\n",
    "\n",
    "print(f\"Movide id = {mv_id}\" + \" have {} movies which are similar to this and but only top 100 most similar ones are of interest.\".format(m_m_sim_sparse[:,mv_id].getnnz()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12524,  2279, 13274, ...,  6725, 15104,     0], dtype=int64)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarities = m_m_sim_sparse[mv_id].toarray().ravel()\n",
    "similar_indices = similarities.argsort()[::-1][1:]\n",
    "similarities[similar_indices]\n",
    "\n",
    "# It will sort and reverse the array and ignore its similarity (i.e. 1)\n",
    "# and return its indices (movie_ids)\n",
    "\n",
    "similar_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17770"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(similar_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(similarities[similar_indices], label='All the ratings')\n",
    "plt.plot(similarities[similar_indices[:100]], label='top 100 similar movies')\n",
    "plt.title(\"Similar Movies of {}(movie_id)\".format(mv_id), fontsize=20)\n",
    "plt.xlabel(\"Movies (Not Movie_Ids)\", fontsize=15)\n",
    "plt.ylabel(\"Cosine Similarity\",fontsize=15)\n",
    "plt.legend()\n",
    "plt.savefig(\"img/similar-movies-for-movieId-1061.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year_of_release</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movie_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12524</th>\n",
       "      <td>2004.0</td>\n",
       "      <td>Spider-Man: The Return of the Green Goblin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2279</th>\n",
       "      <td>2002.0</td>\n",
       "      <td>Spider-Man: The Ultimate Villain Showdown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13274</th>\n",
       "      <td>1996.0</td>\n",
       "      <td>Daredevil vs. Spiderman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1231</th>\n",
       "      <td>1999.0</td>\n",
       "      <td>Batman Beyond: Tech Wars / Disappearing Inque</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14184</th>\n",
       "      <td>1967.0</td>\n",
       "      <td>Spider-Man: The '67 Classic Collection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2912</th>\n",
       "      <td>2003.0</td>\n",
       "      <td>Spider-Man: The New Animated Series: Season 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14017</th>\n",
       "      <td>1999.0</td>\n",
       "      <td>Batman Beyond: School Dayz / Spellbound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11088</th>\n",
       "      <td>1992.0</td>\n",
       "      <td>Adventures of Batman &amp; Robin: The Joker/Fire &amp;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4342</th>\n",
       "      <td>1992.0</td>\n",
       "      <td>Batman: The Animated Series: Out of the Shadows</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7903</th>\n",
       "      <td>1992.0</td>\n",
       "      <td>Adventures of Batman &amp; Robin: Poison Ivy/The P...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          year_of_release                                              title\n",
       "movie_id                                                                    \n",
       "12524              2004.0         Spider-Man: The Return of the Green Goblin\n",
       "2279               2002.0          Spider-Man: The Ultimate Villain Showdown\n",
       "13274              1996.0                            Daredevil vs. Spiderman\n",
       "1231               1999.0      Batman Beyond: Tech Wars / Disappearing Inque\n",
       "14184              1967.0             Spider-Man: The '67 Classic Collection\n",
       "2912               2003.0      Spider-Man: The New Animated Series: Season 1\n",
       "14017              1999.0            Batman Beyond: School Dayz / Spellbound\n",
       "11088              1992.0  Adventures of Batman & Robin: The Joker/Fire &...\n",
       "4342               1992.0    Batman: The Animated Series: Out of the Shadows\n",
       "7903               1992.0  Adventures of Batman & Robin: Poison Ivy/The P..."
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top 10 similar movies for choosed moive_id\n",
    "movie_titles.loc[similar_indices[:10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This same approach could be applied for **user-user** similarity too, then I could get top 10 etc similar users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movies already watched by user 510180: [10341, 1798, 10774, 8651, 14660, 3870, 8357, 15894, 9003, 9392, 11234, 5571, 12470, 11313, 2866, 12818, 5625, 9798, 16465, 15057, 12473, 17064, 6615, 15105, 8079, 1367, 3730, 17764, 11612, 15336, 14455, 11080, 5474, 6902, 2948, 3421, 16182, 11259, 2478, 9536, 14869, 11638, 6336, 11005, 1314, 4912, 13651, 7617, 15674, 15421, 9785, 1421, 15813, 15922, 11888, 8773, 5237, 4883, 12317, 15466, 2518, 14610, 10832, 15940, 15698, 13622, 550, 607, 6971, 6240, 9432, 1324, 432, 15875, 15455, 11490, 16668, 4870, 14624, 9189, 6574, 13705, 599, 6158, 1035, 3113, 14403, 13058, 3439, 12926, 15472, 2622, 12633, 12003, 12160, 13216, 13195, 10392, 12309, 4612, 16339, 16605, 5477, 5882, 2851, 6988, 3617, 6797, 674, 10872, 8177, 14858, 1615, 11544, 3535, 8840, 5845, 6266, 2876, 3168, 4824, 12435, 4248, 5775, 2209, 14109, 13748, 10375, 11051, 7055, 13248, 2174, 7397, 10123, 14928, 1027, 7399, 4214, 4031, 16992, 8317, 5402, 2000, 2953, 8596, 13580, 17709, 4705, 4402, 17616, 17625, 14856, 1542, 12140, 8467, 9635, 7624, 4736, 3563, 833, 12671, 4862, 13593, 2178, 46, 6867, 7364, 17756, 14667, 17638, 6166, 16075, 3798, 13511, 12172, 12305, 9307, 996, 11451, 17499, 14691, 13462, 4670, 8583, 12639, 5788, 13882, 5415, 16223, 16438, 7745, 10152, 9728, 15436, 5130, 501, 1770, 6687, 12549, 14296, 489, 5704, 11289, 15058, 17431, 13512, 1553, 8904, 2862, 8881, 8627, 7395, 17157, 615, 985, 12911, 2640, 7356, 14618, 10371, 14149, 17180, 6718, 12145, 14212, 8178, 1810, 11096, 6350, 285, 17058, 6428, 11926, 7600, 15084, 16926, 10890, 4721, 8327, 4306, 5462, 13795, 16825, 16242, 9111, 12322, 5137, 330, 12343, 4262, 17621, 15156, 12052, 16552, 4227, 15424, 8020, 15818, 9110, 11209, 1148, 9960, 7193, 2167, 14280, 4656, 16713, 8512, 12918, 6196, 4698, 12158, 15758, 2782, 14312, 11701, 8016, 269, 6702, 2122, 1200, 6994, 1955, 7828, 12837, 3936, 15627, 13829, 11198, 4123, 9051, 7019, 571, 2095, 682, 3605, 11262, 16377, 2949, 13614, 13582, 16439, 524, 5366, 9741, 12785, 9524, 14763, 15381, 17280, 4012, 6972, 14475, 4874, 14313, 4996, 6697, 9621, 4932, 4570, 9590, 14331, 199, 1595, 3047, 13082, 16499, 3058, 7533, 4460, 11315, 7585, 17295, 10889, 4998, 13728, 5223, 9242, 6552, 1931, 9310, 5617, 14827, 7563, 15124, 7523, 456, 17589, 5847, 2423, 3190, 5317, 9863, 2510, 16912, 15335, 3557, 17251, 2127, 13412, 5054, 12399, 5762, 7391, 13923, 2182, 7739, 12047, 6114, 9426, 4488, 2006, 4697, 11817, 3680, 17002, 15385, 7180, 8376, 11575, 7072, 11090, 2212, 4683, 13102, 499, 15234, 313, 5073, 14652, 759, 11446, 6220, 16272, 11161, 4352, 9509, 10094, 12347, 6754, 1145, 299, 7224, 15966, 8131, 9685, 16461, 7234, 7937, 5862, 3309, 8793, 13865, 11479, 2499, 686, 1264, 17154, 734, 17308, 12876, 16769, 14527, 3368, 6500, 14621, 708, 9972, 5191, 16879, 1650, 12881, 14712, 11265, 5614, 10241, 4577, 1884, 16495, 4216, 15132, 5245, 9591, 1078, 9290, 8606, 9080, 81, 16384, 2905, 13981, 4790, 12773, 13763, 12152, 3148, 12355, 1425, 1102, 5856, 3312, 3433, 8827, 5937, 4943, 15755, 3160, 4727, 1130, 1834, 13493, 1450, 17494, 15844, 15445, 7814, 13771, 10577, 15991, 746, 12493, 14935, 1590, 5753, 17622, 12188, 16901, 6756, 17627, 10895, 1167, 13524, 289, 353, 4660, 516, 10239, 6872, 3825, 6497, 7971, 14407, 4820, 14259, 8808, 2452, 12090, 6343, 14410, 15415, 2200, 12238, 6829, 12456, 2012, 2867, 9156, 6760, 1100, 8483, 12299, 14718, 11708, 6464, 4131, 3686, 13302, 9511, 6555, 3275, 3320, 5169, 6235, 10820, 16643, 10702, 12125, 3716, 12587, 4354, 15582, 14698, 3522, 4356, 16565, 16359, 7363, 11677, 6206, 10178, 5391, 8884, 5814, 8603, 7882, 369, 13500, 4043, 3035, 17387, 2922, 3334, 14759, 16126, 11443, 6037, 8928, 8204, 6591, 11149, 7701, 15107, 3650, 6287, 8107, 3265, 16139, 17482, 10337, 6739, 4330, 7330, 7155, 17023, 12243, 13529, 15064, 9205, 16128, 6449, 6692, 10625, 3128, 9235, 9613, 6961, 10734, 7240, 5293, 10158, 14408, 14909, 83, 8181, 12365, 11573, 4685, 11837, 6274, 9617, 6408, 5087, 10785, 16788, 14574, 8846, 7590, 12494, 6599, 11277, 1157, 7895, 2254, 15492, 12194, 14531, 275, 8644, 7509, 15689, 10730, 12084, 1080, 7513, 7763, 10027, 9136, 8387, 3756, 17671, 270, 4340, 1428, 15471, 2743, 1974, 7430, 7928, 3814, 16392, 14364, 14538, 4432, 5970, 17412, 4970, 3925, 16644, 6931, 10013, 11521, 11164, 1470, 9673, 2152, 6927, 3371, 11347, 5085, 10281, 16871, 15447, 5162, 5181, 15070, 13129, 1110, 9768, 15563, 1180, 13466, 12184, 7067, 6060, 2699, 8492, 8334, 10168, 16790, 443, 3860, 2051, 8782, 8077, 3098, 11820, 16740, 1744, 4658, 7644, 954, 15042, 2803, 12586, 16147, 6117, 1877, 6528, 7237, 14412, 6966, 10461, 10418, 9368, 6508, 5112, 15788, 11585, 14233, 4546, 30, 2533, 5304, 9218, 2016, 13809, 5360, 6860, 8413, 16325, 5308, 4472, 16711, 8447, 12205, 14240, 1495, 5836, 12870, 13391, 15010, 4951, 6667, 2586, 8691, 3249, 8584, 3491, 1392, 5368, 3216, 15345, 11089, 405, 6029, 6720, 7627, 9340, 10137, 16361, 8232, 17215, 10080, 357, 2612, 17328, 14086, 4640, 1962, 10550, 15532, 12056, 16082, 11994, 12605, 9690, 58, 658, 15530, 4545, 9592, 17305, 13103, 9350, 16977, 1255, 7607, 8854, 16150, 14047, 13128, 9098, 16580, 15662, 11307, 4628, 15116, 16954, 10446, 7786, 7716, 10042, 3684, 8954, 11032, 11040, 7755, 11531, 2112, 2342, 4049, 2988, 1220, 148, 14550, 4229, 7043, 15205, 10095, 3624, 1865, 2372, 14602, 1108, 12732, 445, 6091, 17672, 9987, 12161, 17281, 1692, 17521, 12895, 10552, 2296, 11916, 17116, 8050, 15296, 6633, 13675, 6093, 3662, 11372, 10280, 2874, 10114, 17506, 13081, 14545, 16406, 8903, 8915, 13726, 4266, 4972, 3917, 4269, 4590, 12530, 16415, 2687, 8629, 9048, 4069, 1703, 10229, 12338, 3875, 13359, 16922, 10359, 14103, 16948, 2913, 11911, 13453, 3163, 774, 17355, 5869, 15129, 4315, 12799, 12828, 6034, 14842, 11182, 13273, 3333, 12155, 1905, 361, 13217, 3742, 9481, 17725, 12371, 17036, 5760, 6084, 7379, 11283, 5515, 16765, 7683, 798, 4141, 11186, 14382, 7681, 14584, 16452, 12500, 8379, 4054, 4847, 10906, 10173, 5496, 6642, 2103, 13050, 5765, 14911, 17324]\n"
     ]
    }
   ],
   "source": [
    "def get_watched_movies(user_id, df):\n",
    "    # Filter DataFrame to get rows corresponding to the specified user\n",
    "    user_movies_df = df[df['user'] == user_id]\n",
    "    # Get the list of movies watched by the user\n",
    "    watched_movies = user_movies_df['movie'].tolist()\n",
    "    return watched_movies\n",
    "\n",
    "# Example usage:\n",
    "target_user_id = 510180  # user to which we are recommending top 10 movies\n",
    "watched_movies = get_watched_movies(target_user_id, train_df)\n",
    "print(\"Movies already watched by user {}: {}\".format(target_user_id, watched_movies))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def get_similar_users(target_user_id, train_sparse_matrix, top_n):\n",
    "    # Calculate cosine similarity between the target user and all other users\n",
    "    similarity_scores = cosine_similarity(train_sparse_matrix[target_user_id], train_sparse_matrix).ravel()\n",
    "    \n",
    "    # Sort the similarity scores in descending order and get the indices of top similar users\n",
    "    similar_users_indices = similarity_scores.argsort()[::-1][1:top_n+1]\n",
    "    \n",
    "    return similar_users_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top similar users for user 510180 : [2070820 1912012   15191  443193  829101 1791707  113369  383858 1797525\n",
      " 1413561]\n"
     ]
    }
   ],
   "source": [
    "target_user_id = 510180  # ID of the targeted user\n",
    "similar_users_indices = get_similar_users(target_user_id, train_sparse_matrix, top_n=10)\n",
    "print(\"Top similar users for user\", target_user_id, \":\", similar_users_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining both user-user and movie-movie collaborative filtering via Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_sparse_matrix(sparse_matrix, no_users, no_movies, path, verbose = True):\n",
    "    \"\"\"\n",
    "        It will get it from the ''path'' if it is present  or It will create \n",
    "        and store the sampled sparse matrix in the path specified.\n",
    "    \"\"\"\n",
    "\n",
    "    # get (row, col) and (rating) tuple from sparse_matrix...\n",
    "    row_ind, col_ind, ratings = sparse.find(sparse_matrix)\n",
    "    users = np.unique(row_ind)\n",
    "    movies = np.unique(col_ind)\n",
    "\n",
    "    print(\"Original Matrix : (users, movies) -- ({} {})\".format(len(users), len(movies)))\n",
    "    print(\"Original Matrix : Ratings -- {}\\n\".format(len(ratings)))\n",
    "\n",
    "    # It just to make sure to get same sample everytime we run this program..\n",
    "    # and pick without replacement....\n",
    "    np.random.seed(15)\n",
    "    sample_users = np.random.choice(users, no_users, replace=False)\n",
    "    sample_movies = np.random.choice(movies, no_movies, replace=False)\n",
    "    # get the boolean mask or these sampled_items in originl row/col_inds..\n",
    "    mask = np.logical_and( np.isin(row_ind, sample_users),\n",
    "                      np.isin(col_ind, sample_movies) )\n",
    "    \n",
    "    sample_sparse_matrix = sparse.csr_matrix((ratings[mask], (row_ind[mask], col_ind[mask])),\n",
    "                                             shape=(max(sample_users)+1, max(sample_movies)+1))\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Sampled Matrix : (users, movies) -- ({} {})\".format(len(sample_users), len(sample_movies)))\n",
    "        print(\"Sampled Matrix : Ratings --\", format(ratings[mask].shape[0]))\n",
    "\n",
    "    print('Saving it into pwd for furthur operations...')\n",
    "    \n",
    "    sparse.save_npz(path, sample_sparse_matrix)\n",
    "    if verbose:\n",
    "            print('Done.')\n",
    "    \n",
    "    return sample_sparse_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Sample Train data from train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is not present in pwd...\n",
      "Original Matrix : (users, movies) -- (405041 17424)\n",
      "Original Matrix : Ratings -- 80384405\n",
      "\n",
      "Sampled Matrix : (users, movies) -- (8000 800)\n",
      "Sampled Matrix : Ratings -- 78751\n",
      "Saving it into pwd for furthur operations...\n",
      "Done.\n",
      "0:01:27.232241\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "path = \"sample/small/sample_train_sparse_matrix.npz\"\n",
    "if os.path.isfile(path):\n",
    "    print(\"It is present in your pwd, getting it from disk....\")\n",
    "    # just get it from the disk instead of computing it\n",
    "    sample_train_sparse_matrix = sparse.load_npz(path)\n",
    "    print(\"DONE.\")\n",
    "else: \n",
    "    # get 8k users and 0.8k movies from available data \n",
    "    print(\"It is not present in pwd...\")\n",
    "    sample_train_sparse_matrix = get_sample_sparse_matrix(train_sparse_matrix, no_users=8000, no_movies=800,\n",
    "                                             path = path)\n",
    "\n",
    "print(datetime.now() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Sample Test Data from test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is not present in pwd...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Matrix : (users, movies) -- (349312 17757)\n",
      "Original Matrix : Ratings -- 20096102\n",
      "\n",
      "Sampled Matrix : (users, movies) -- (4000 400)\n",
      "Sampled Matrix : Ratings -- 4530\n",
      "Saving it into pwd for furthur operations...\n",
      "Done.\n",
      "0:00:17.270051\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "path = \"sample/small/sample_test_sparse_matrix.npz\"\n",
    "if os.path.isfile(path):\n",
    "    print(\"It is present in your pwd, getting it from disk....\")\n",
    "    # just get it from the disk instead of computing it\n",
    "    sample_test_sparse_matrix = sparse.load_npz(path)\n",
    "    print(\"DONE.\")\n",
    "else:\n",
    "    # get 4k users and 400 movies from available data \n",
    "    print(\"It is not present in pwd...\")\n",
    "    sample_test_sparse_matrix = get_sample_sparse_matrix(test_sparse_matrix, no_users=4000, no_movies=400,\n",
    "                                                 path = \"sample/small/sample_test_sparse_matrix.npz\")\n",
    "print(datetime.now() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding Global Average of all movie ratings, Average rating per User, and Average rating per Movie (from sampled train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_train_averages = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.6119795304186613"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the global average of ratings in our train set.\n",
    "global_average = sample_train_sparse_matrix.sum()/sample_train_sparse_matrix.count_nonzero()\n",
    "sample_train_averages['global'] = global_average\n",
    "sample_train_averages['global']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average rating of user 1179 : 3.7142857142857144\n"
     ]
    }
   ],
   "source": [
    "sample_train_averages['user'] = get_average_ratings(sample_train_sparse_matrix, of_users=True)\n",
    "print('\\nAverage rating of user 1179 :',sample_train_averages['user'][1179])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " AVerage rating of movie 6464 : 3.400396432111001\n"
     ]
    }
   ],
   "source": [
    "sample_train_averages['movie'] =  get_average_ratings(sample_train_sparse_matrix, of_users=False)\n",
    "print('\\n AVerage rating of movie 6464 :',sample_train_averages['movie'][6464])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Featurizing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of ratings in Our Sampled train matrix is : 78751\n",
      "\n",
      "No of ratings in Our Sampled test  matrix is : 4530\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('No of ratings in Our Sampled train matrix is : {}\\n'.format(sample_train_sparse_matrix.count_nonzero()))\n",
    "print('No of ratings in Our Sampled test  matrix is : {}\\n'.format(sample_test_sparse_matrix.count_nonzero()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Featurizing train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get users, movies and ratings from our samples train sparse matrix\n",
    "sample_train_users, sample_train_movies, sample_train_ratings = sparse.find(sample_train_sparse_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing 78751 tuples for the dataset..\n",
      "\n",
      "Done for 10000 rows----- 0:27:34.998557\n",
      "Done for 20000 rows----- 0:54:02.188989\n",
      "Done for 30000 rows----- 1:20:31.693370\n",
      "Done for 40000 rows----- 1:46:57.151897\n",
      "Done for 50000 rows----- 2:13:21.269826\n",
      "Done for 60000 rows----- 2:39:44.191039\n",
      "Done for 70000 rows----- 3:06:00.945686\n",
      "3:29:14.949788\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "if os.path.isfile('sample/small/reg_train.csv'):\n",
    "    print(\"File already exists you don't have to prepare again...\" )\n",
    "else:\n",
    "    print('preparing {} tuples for the dataset..\\n'.format(len(sample_train_ratings)))\n",
    "    with open('sample/small/reg_train.csv', mode='w') as reg_data_file:\n",
    "        count = 0\n",
    "        for (user, movie, rating)  in zip(sample_train_users, sample_train_movies, sample_train_ratings):\n",
    "            st = datetime.now()\n",
    "        #     print(user, movie)    \n",
    "            #--------------------- Ratings of \"movie\" by similar users of \"user\" ---------------------\n",
    "            # compute the similar Users of the \"user\"        \n",
    "            user_sim = cosine_similarity(sample_train_sparse_matrix[user], sample_train_sparse_matrix).ravel()\n",
    "            top_sim_users = user_sim.argsort()[::-1][1:] # we are ignoring 'The User' from its similar users.\n",
    "            # get the ratings of most similar users for this movie\n",
    "            top_ratings = sample_train_sparse_matrix[top_sim_users, movie].toarray().ravel()\n",
    "            # we will make it's length \"5\" by adding movie averages to .\n",
    "            top_sim_users_ratings = list(top_ratings[top_ratings != 0][:5])\n",
    "            top_sim_users_ratings.extend([sample_train_averages['movie'][movie]]*(5 - len(top_sim_users_ratings)))\n",
    "        #     print(top_sim_users_ratings, end=\" \")    \n",
    "\n",
    "\n",
    "            #--------------------- Ratings by \"user\"  to similar movies of \"movie\" ---------------------\n",
    "            # compute the similar movies of the \"movie\"        \n",
    "            movie_sim = cosine_similarity(sample_train_sparse_matrix[:,movie].T, sample_train_sparse_matrix.T).ravel()\n",
    "            top_sim_movies = movie_sim.argsort()[::-1][1:] # we are ignoring 'The User' from its similar users.\n",
    "            # get the ratings of most similar movie rated by this user..\n",
    "            top_ratings = sample_train_sparse_matrix[user, top_sim_movies].toarray().ravel()\n",
    "            # we will make it's length \"5\" by adding user averages to.\n",
    "            top_sim_movies_ratings = list(top_ratings[top_ratings != 0][:5])\n",
    "            top_sim_movies_ratings.extend([sample_train_averages['user'][user]]*(5-len(top_sim_movies_ratings))) \n",
    "        #     print(top_sim_movies_ratings, end=\" : -- \")\n",
    "\n",
    "            #-----------------prepare the row to be stores in a file-----------------#\n",
    "            row = list()\n",
    "            row.append(user)\n",
    "            row.append(movie)\n",
    "            # Now add the other features to this data...\n",
    "            row.append(sample_train_averages['global']) # first feature\n",
    "            # next 5 features are similar_users \"movie\" ratings\n",
    "            row.extend(top_sim_users_ratings)\n",
    "            # next 5 features are \"user\" ratings for similar_movies\n",
    "            row.extend(top_sim_movies_ratings)\n",
    "            # Avg_user rating\n",
    "            row.append(sample_train_averages['user'][user])\n",
    "            # Avg_movie rating\n",
    "            row.append(sample_train_averages['movie'][movie])\n",
    "\n",
    "            # finalley, The actual Rating of this user-movie pair...\n",
    "            row.append(rating)\n",
    "            count = count + 1\n",
    "\n",
    "            # add rows to the file opened..\n",
    "            reg_data_file.write(','.join(map(str, row)))\n",
    "            reg_data_file.write('\\n')        \n",
    "            if (count)%10000 == 0:\n",
    "                # print(','.join(map(str, row)))\n",
    "                print(\"Done for {} rows----- {}\".format(count, datetime.now() - start))\n",
    "\n",
    "\n",
    "print(datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>movie</th>\n",
       "      <th>GAvg</th>\n",
       "      <th>sur1</th>\n",
       "      <th>sur2</th>\n",
       "      <th>sur3</th>\n",
       "      <th>sur4</th>\n",
       "      <th>sur5</th>\n",
       "      <th>smr1</th>\n",
       "      <th>smr2</th>\n",
       "      <th>smr3</th>\n",
       "      <th>smr4</th>\n",
       "      <th>smr5</th>\n",
       "      <th>UAvg</th>\n",
       "      <th>MAvg</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>692</td>\n",
       "      <td>14621</td>\n",
       "      <td>3.61198</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.329095</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1179</td>\n",
       "      <td>2239</td>\n",
       "      <td>3.61198</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.714286</td>\n",
       "      <td>2.909091</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1179</td>\n",
       "      <td>4352</td>\n",
       "      <td>3.61198</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.714286</td>\n",
       "      <td>3.140845</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1179</td>\n",
       "      <td>6464</td>\n",
       "      <td>3.61198</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.714286</td>\n",
       "      <td>3.400396</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1179</td>\n",
       "      <td>6510</td>\n",
       "      <td>3.61198</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.714286</td>\n",
       "      <td>3.936614</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user  movie     GAvg  sur1  sur2  sur3  sur4  sur5  smr1  smr2  smr3  smr4  \\\n",
       "0   692  14621  3.61198   2.0   4.0   5.0   5.0   5.0   4.0   4.0   4.0   4.0   \n",
       "1  1179   2239  3.61198   5.0   3.0   3.0   2.0   2.0   3.0   5.0   4.0   3.0   \n",
       "2  1179   4352  3.61198   4.0   3.0   2.0   3.0   3.0   4.0   4.0   3.0   4.0   \n",
       "3  1179   6464  3.61198   3.0   5.0   4.0   2.0   4.0   4.0   4.0   2.0   5.0   \n",
       "4  1179   6510  3.61198   4.0   4.0   3.0   4.0   3.0   5.0   4.0   4.0   3.0   \n",
       "\n",
       "   smr5      UAvg      MAvg  rating  \n",
       "0   4.0  4.000000  4.329095       4  \n",
       "1   4.0  3.714286  2.909091       5  \n",
       "2   5.0  3.714286  3.140845       3  \n",
       "3   3.0  3.714286  3.400396       4  \n",
       "4   4.0  3.714286  3.936614       4  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_train = pd.read_csv('sample/small/reg_train.csv', names = ['user', 'movie', 'GAvg', 'sur1', 'sur2', 'sur3', 'sur4', 'sur5','smr1', 'smr2', 'smr3', 'smr4', 'smr5', 'UAvg', 'MAvg', 'rating'], header=None)\n",
    "reg_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- GAvg :  Avg Global Rating\n",
    "- UAvg : User's avg movie rating\n",
    "- MAvg : Movie's avg rating\n",
    "- rating : rating given by user to movie\n",
    "- sur1,sur2,sur3,sur4,sur5 : top 5 similar users who rated that movie\n",
    "- sm1r,smr2,smr3,smr4,smr5 : top 5 movies similar to that movie\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(78751, 16)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Featurising test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get users, movies and ratings from the Sampled Test \n",
    "sample_test_users, sample_test_movies, sample_test_ratings = sparse.find(sample_test_sparse_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.5304635761589402"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_average = sample_test_sparse_matrix.sum()/sample_test_sparse_matrix.count_nonzero()\n",
    "sample_train_averages['global'] = global_average\n",
    "sample_train_averages['global']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing 4530 tuples for the dataset..\n",
      "\n",
      "Done for 1000 rows----- 0:04:00.820938\n",
      "Done for 2000 rows----- 0:07:26.288614\n",
      "Done for 3000 rows----- 0:11:52.674543\n",
      "Done for 4000 rows----- 0:15:36.367723\n",
      " 0:17:06.540811\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "if os.path.isfile('sample/small/reg_test.csv'):\n",
    "    print(\"It is already created...\")\n",
    "else:\n",
    "\n",
    "    print('preparing {} tuples for the dataset..\\n'.format(len(sample_test_ratings)))\n",
    "    with open('sample/small/reg_test.csv', mode='w') as reg_data_file:\n",
    "        count = 0 \n",
    "        for (user, movie, rating)  in zip(sample_test_users, sample_test_movies, sample_test_ratings):\n",
    "            st = datetime.now()\n",
    "\n",
    "        #--------------------- Ratings of \"movie\" by similar users of \"user\" ---------------------\n",
    "            #print(user, movie)\n",
    "            try:\n",
    "                # compute the similar Users of the \"user\"        \n",
    "                user_sim = cosine_similarity(sample_train_sparse_matrix[user], sample_train_sparse_matrix).ravel()\n",
    "                top_sim_users = user_sim.argsort()[::-1][1:] # we are ignoring 'The User' from its similar users.\n",
    "                # get the ratings of most similar users for this movie\n",
    "                top_ratings = sample_train_sparse_matrix[top_sim_users, movie].toarray().ravel()\n",
    "                # we will make it's length \"5\" by adding movie averages to .\n",
    "                top_sim_users_ratings = list(top_ratings[top_ratings != 0][:5])\n",
    "                top_sim_users_ratings.extend([sample_train_averages['movie'][movie]]*(5 - len(top_sim_users_ratings)))\n",
    "                # print(top_sim_users_ratings, end=\"--\")\n",
    "\n",
    "            except (IndexError, KeyError):\n",
    "                # It is a new User or new Movie or there are no ratings for given user for top similar movies...\n",
    "                ########## Cold STart Problem ##########\n",
    "                top_sim_users_ratings.extend([sample_train_averages['global']]*(5 - len(top_sim_users_ratings)))\n",
    "                #print(top_sim_users_ratings)\n",
    "            except:\n",
    "                print(user, movie)\n",
    "                # we just want KeyErrors to be resolved. Not every Exception...\n",
    "                raise\n",
    "\n",
    "\n",
    "\n",
    "            #--------------------- Ratings by \"user\"  to similar movies of \"movie\" ---------------------\n",
    "            try:\n",
    "                # compute the similar movies of the \"movie\"        \n",
    "                movie_sim = cosine_similarity(sample_train_sparse_matrix[:,movie].T, sample_train_sparse_matrix.T).ravel()\n",
    "                top_sim_movies = movie_sim.argsort()[::-1][1:] # we are ignoring 'The User' from its similar users.\n",
    "                # get the ratings of most similar movie rated by this user..\n",
    "                top_ratings = sample_train_sparse_matrix[user, top_sim_movies].toarray().ravel()\n",
    "                # we will make it's length \"5\" by adding user averages to.\n",
    "                top_sim_movies_ratings = list(top_ratings[top_ratings != 0][:5])\n",
    "                top_sim_movies_ratings.extend([sample_train_averages['user'][user]]*(5-len(top_sim_movies_ratings))) \n",
    "                #print(top_sim_movies_ratings)\n",
    "            except (IndexError, KeyError):\n",
    "                #print(top_sim_movies_ratings, end=\" : -- \")\n",
    "                top_sim_movies_ratings.extend([sample_train_averages['global']]*(5-len(top_sim_movies_ratings)))\n",
    "                #print(top_sim_movies_ratings)\n",
    "            except :\n",
    "                raise\n",
    "\n",
    "            #-----------------prepare the row to be stores in a file-----------------#\n",
    "            row = list()\n",
    "            # add usser and movie name first\n",
    "            row.append(user)\n",
    "            row.append(movie)\n",
    "            row.append(sample_train_averages['global']) # first feature\n",
    "            #print(row)\n",
    "            # next 5 features are similar_users \"movie\" ratings\n",
    "            row.extend(top_sim_users_ratings)\n",
    "            #print(row)\n",
    "            # next 5 features are \"user\" ratings for similar_movies\n",
    "            row.extend(top_sim_movies_ratings)\n",
    "            #print(row)\n",
    "            # Avg_user rating\n",
    "            try:\n",
    "                row.append(sample_train_averages['user'][user])\n",
    "            except KeyError:\n",
    "                row.append(sample_train_averages['global'])\n",
    "            except:\n",
    "                raise\n",
    "            #print(row)\n",
    "            # Avg_movie rating\n",
    "            try:\n",
    "                row.append(sample_train_averages['movie'][movie])\n",
    "            except KeyError:\n",
    "                row.append(sample_train_averages['global'])\n",
    "            except:\n",
    "                raise\n",
    "            #print(row)\n",
    "            # finalley, The actual Rating of this user-movie pair...\n",
    "            row.append(rating)\n",
    "            #print(row)\n",
    "            count = count + 1\n",
    "\n",
    "            # add rows to the file opened..\n",
    "            reg_data_file.write(','.join(map(str, row)))\n",
    "            #print(','.join(map(str, row)))\n",
    "            reg_data_file.write('\\n')        \n",
    "            if (count)%1000 == 0:\n",
    "                #print(','.join(map(str, row)))\n",
    "                print(\"Done for {} rows----- {}\".format(count, datetime.now() - start))\n",
    "    print(\"\",datetime.now() - start)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>movie</th>\n",
       "      <th>GAvg</th>\n",
       "      <th>sur1</th>\n",
       "      <th>sur2</th>\n",
       "      <th>sur3</th>\n",
       "      <th>sur4</th>\n",
       "      <th>sur5</th>\n",
       "      <th>smr1</th>\n",
       "      <th>smr2</th>\n",
       "      <th>smr3</th>\n",
       "      <th>smr4</th>\n",
       "      <th>smr5</th>\n",
       "      <th>UAvg</th>\n",
       "      <th>MAvg</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>13072</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>126</td>\n",
       "      <td>3418</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>268</td>\n",
       "      <td>11740</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1809</td>\n",
       "      <td>705</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1809</td>\n",
       "      <td>1140</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user  movie      GAvg      sur1      sur2      sur3      sur4      sur5  \\\n",
       "0     7  13072  3.530464  3.530464  3.530464  3.530464  3.530464  3.530464   \n",
       "1   126   3418  3.530464  3.530464  3.530464  3.530464  3.530464  3.530464   \n",
       "2   268  11740  3.530464  3.530464  3.530464  3.530464  3.530464  3.530464   \n",
       "3  1809    705  3.530464  3.530464  3.530464  3.530464  3.530464  3.530464   \n",
       "4  1809   1140  3.530464  3.530464  3.530464  3.530464  3.530464  3.530464   \n",
       "\n",
       "       smr1      smr2      smr3      smr4      smr5      UAvg      MAvg  \\\n",
       "0  3.530464  3.530464  3.530464  3.530464  3.530464  3.530464  3.530464   \n",
       "1  3.530464  3.530464  3.530464  3.530464  3.530464  3.530464  3.530464   \n",
       "2  3.530464  3.530464  3.530464  3.530464  3.530464  3.530464  3.530464   \n",
       "3  3.530464  3.530464  3.530464  3.530464  3.530464  3.530464  3.530464   \n",
       "4  3.530464  3.530464  3.530464  3.530464  3.530464  3.530464  3.530464   \n",
       "\n",
       "   rating  \n",
       "0       5  \n",
       "1       5  \n",
       "2       5  \n",
       "3       4  \n",
       "4       3  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_test_df = pd.read_csv('sample/small/reg_test.csv', names = ['user', 'movie', 'GAvg', 'sur1', 'sur2', 'sur3', 'sur4', 'sur5',\n",
    "                                                          'smr1', 'smr2', 'smr3', 'smr4', 'smr5',\n",
    "                                                          'UAvg', 'MAvg', 'rating'], header=None)\n",
    "reg_test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Surprise model\n",
    "Ref : http://surprise.readthedocs.io/en/stable/getting_started.html#load-dom-dataframe-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import Reader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = Reader(rating_scale=(1,5))\n",
    "\n",
    "# create the traindata from the dataframe...\n",
    "train_data = Dataset.load_from_df(reg_train[['user', 'movie', 'rating']], reader)\n",
    "\n",
    "# build the trainset from traindata.., It is of dataset format from surprise library..\n",
    "trainset = train_data.build_full_trainset() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(7, 13072, 5),\n",
       " (126, 3418, 5),\n",
       " (268, 11740, 5),\n",
       " (1809, 705, 4),\n",
       " (1809, 1140, 3),\n",
       " (1809, 2533, 3),\n",
       " (1809, 3418, 3),\n",
       " (1809, 4011, 4),\n",
       " (1809, 6849, 5),\n",
       " (1809, 11740, 4),\n",
       " (3300, 5226, 3),\n",
       " (3300, 5601, 3),\n",
       " (3321, 348, 3),\n",
       " (3321, 750, 1),\n",
       " (3321, 1140, 2),\n",
       " (3321, 1648, 4),\n",
       " (3321, 3247, 3),\n",
       " (3321, 3913, 3),\n",
       " (3321, 4072, 2),\n",
       " (3321, 4461, 2),\n",
       " (3321, 5071, 3),\n",
       " (3321, 5521, 2),\n",
       " (3321, 5601, 3),\n",
       " (3321, 5821, 4),\n",
       " (3321, 5840, 2),\n",
       " (3321, 6131, 1),\n",
       " (3321, 6849, 2),\n",
       " (3321, 8127, 1),\n",
       " (3321, 10787, 3),\n",
       " (3321, 11149, 1),\n",
       " (3321, 11263, 2),\n",
       " (3321, 11268, 2),\n",
       " (3321, 11292, 2),\n",
       " (3321, 11350, 3),\n",
       " (3321, 11740, 3),\n",
       " (3321, 12135, 4),\n",
       " (3321, 12367, 1),\n",
       " (3321, 12846, 1),\n",
       " (3321, 13592, 3),\n",
       " (3321, 13649, 1),\n",
       " (3321, 13687, 3),\n",
       " (3321, 13770, 1),\n",
       " (3321, 13866, 1),\n",
       " (3321, 14320, 4),\n",
       " (3321, 14435, 1),\n",
       " (3321, 14766, 1),\n",
       " (3321, 14938, 3),\n",
       " (3321, 14943, 1),\n",
       " (3321, 15392, 4),\n",
       " (3321, 15564, 3),\n",
       " (3321, 15738, 1),\n",
       " (3321, 15989, 1),\n",
       " (3321, 16352, 2),\n",
       " (3321, 16858, 3),\n",
       " (3321, 16986, 2),\n",
       " (3321, 17058, 2),\n",
       " (3321, 17642, 3),\n",
       " (3959, 6274, 5),\n",
       " (3959, 16765, 4),\n",
       " (4796, 6844, 5),\n",
       " (5827, 2020, 4),\n",
       " (5827, 3784, 5),\n",
       " (5827, 5060, 5),\n",
       " (5827, 6274, 5),\n",
       " (5827, 14320, 5),\n",
       " (5851, 5053, 5),\n",
       " (8427, 6844, 4),\n",
       " (10897, 5071, 4),\n",
       " (10897, 5816, 3),\n",
       " (10897, 7970, 3),\n",
       " (10897, 9392, 4),\n",
       " (10897, 11740, 3),\n",
       " (10897, 12135, 3),\n",
       " (10897, 15392, 5),\n",
       " (11452, 3555, 2),\n",
       " (13717, 5071, 4),\n",
       " (13717, 6859, 5),\n",
       " (13717, 11149, 3),\n",
       " (14204, 658, 3),\n",
       " (14204, 705, 3),\n",
       " (14204, 4972, 2),\n",
       " (14204, 5226, 3),\n",
       " (14204, 5601, 3),\n",
       " (14204, 6274, 4),\n",
       " (14204, 8376, 3),\n",
       " (14204, 9818, 3),\n",
       " (14204, 13384, 3),\n",
       " (14204, 13787, 2),\n",
       " (14326, 13649, 3),\n",
       " (17872, 3418, 5),\n",
       " (17872, 16937, 5),\n",
       " (19117, 658, 2),\n",
       " (19117, 962, 2),\n",
       " (19117, 2851, 4),\n",
       " (19117, 4972, 3),\n",
       " (19117, 5071, 2),\n",
       " (19117, 5226, 5),\n",
       " (19117, 5231, 4),\n",
       " (19117, 5601, 3),\n",
       " (19117, 5778, 3),\n",
       " (19117, 6274, 5),\n",
       " (19117, 6844, 2),\n",
       " (19117, 6859, 4),\n",
       " (19117, 8677, 4),\n",
       " (19117, 13384, 2),\n",
       " (19117, 14233, 4),\n",
       " (19117, 15786, 5),\n",
       " (19117, 16765, 3),\n",
       " (21851, 4972, 2),\n",
       " (22338, 4972, 3),\n",
       " (23354, 2851, 5),\n",
       " (23354, 5071, 5),\n",
       " (23354, 6729, 3),\n",
       " (23354, 7864, 5),\n",
       " (23354, 9818, 5),\n",
       " (23354, 11149, 5),\n",
       " (23354, 11214, 5),\n",
       " (23354, 13787, 5),\n",
       " (23354, 16767, 5),\n",
       " (23404, 5226, 4),\n",
       " (28572, 111, 1),\n",
       " (28572, 5071, 4),\n",
       " (29719, 658, 3),\n",
       " (29719, 6274, 5),\n",
       " (29719, 6844, 2),\n",
       " (29719, 6866, 4),\n",
       " (29719, 9175, 5),\n",
       " (29719, 16420, 5),\n",
       " (29719, 16765, 4),\n",
       " (31829, 11068, 5),\n",
       " (36123, 6859, 4),\n",
       " (36123, 8651, 1),\n",
       " (37533, 16765, 3),\n",
       " (38043, 3418, 5),\n",
       " (38043, 6859, 4),\n",
       " (38043, 11149, 5),\n",
       " (38264, 6844, 1),\n",
       " (39329, 5071, 3),\n",
       " (41914, 4972, 4),\n",
       " (41914, 6274, 4),\n",
       " (41914, 13074, 3),\n",
       " (42598, 658, 4),\n",
       " (42598, 9818, 4),\n",
       " (42791, 8127, 3),\n",
       " (44434, 2851, 3),\n",
       " (44434, 5071, 3),\n",
       " (44734, 3441, 2),\n",
       " (46103, 5226, 2),\n",
       " (46544, 10372, 5),\n",
       " (46679, 3128, 3),\n",
       " (46679, 5071, 3),\n",
       " (46679, 6274, 4),\n",
       " (46679, 8376, 4),\n",
       " (46679, 8651, 3),\n",
       " (46679, 9818, 4),\n",
       " (46679, 12446, 4),\n",
       " (46679, 13074, 4),\n",
       " (46679, 14137, 2),\n",
       " (55736, 11149, 2),\n",
       " (57778, 6512, 4),\n",
       " (60856, 5071, 3),\n",
       " (62625, 3441, 3),\n",
       " (63623, 5071, 4),\n",
       " (64311, 5071, 5),\n",
       " (64311, 6859, 5),\n",
       " (66062, 4972, 5),\n",
       " (66062, 11149, 5),\n",
       " (66062, 13787, 3),\n",
       " (67516, 11149, 4),\n",
       " (67517, 5778, 4),\n",
       " (67517, 7864, 4),\n",
       " (67517, 9818, 5),\n",
       " (67517, 15861, 5),\n",
       " (67517, 16765, 5),\n",
       " (67517, 16767, 3),\n",
       " (68985, 1163, 3),\n",
       " (68985, 3128, 3),\n",
       " (68985, 5778, 3),\n",
       " (68985, 6844, 3),\n",
       " (68985, 9818, 2),\n",
       " (68985, 16765, 3),\n",
       " (69863, 5071, 5),\n",
       " (69863, 6274, 4),\n",
       " (72087, 10734, 3),\n",
       " (72087, 11149, 2),\n",
       " (78601, 6274, 4),\n",
       " (81040, 3418, 3),\n",
       " (81040, 6512, 5),\n",
       " (81040, 11149, 3),\n",
       " (82371, 11149, 3),\n",
       " (85701, 6274, 4),\n",
       " (89748, 15010, 3),\n",
       " (90724, 4972, 3),\n",
       " (90724, 6844, 4),\n",
       " (90724, 16765, 1),\n",
       " (93473, 5601, 4),\n",
       " (93473, 6844, 3),\n",
       " (93473, 13384, 3),\n",
       " (96386, 5226, 5),\n",
       " (96666, 6274, 4),\n",
       " (98586, 251, 4),\n",
       " (98586, 4046, 5),\n",
       " (98586, 9622, 4),\n",
       " (98586, 13649, 4),\n",
       " (98586, 15010, 4),\n",
       " (98601, 7970, 4),\n",
       " (99208, 16767, 5),\n",
       " (99920, 6844, 3),\n",
       " (100035, 5226, 4),\n",
       " (100035, 6859, 4),\n",
       " (100035, 7060, 3),\n",
       " (100035, 11149, 3),\n",
       " (100035, 11292, 3),\n",
       " (101069, 4801, 3),\n",
       " (104604, 4801, 2),\n",
       " (104821, 348, 3),\n",
       " (104821, 962, 3),\n",
       " (104821, 5071, 3),\n",
       " (104821, 6844, 1),\n",
       " (104821, 14320, 5),\n",
       " (104821, 15392, 4),\n",
       " (104917, 6844, 4),\n",
       " (105089, 5226, 4),\n",
       " (105260, 9818, 4),\n",
       " (107276, 3526, 5),\n",
       " (109377, 5226, 5),\n",
       " (111942, 5226, 5),\n",
       " (113068, 3441, 3),\n",
       " (113068, 5071, 4),\n",
       " (117108, 6844, 4),\n",
       " (117108, 11149, 3),\n",
       " (117617, 111, 4),\n",
       " (117617, 705, 4),\n",
       " (117617, 6844, 4),\n",
       " (117617, 6859, 5),\n",
       " (117617, 8376, 4),\n",
       " (117617, 11149, 4),\n",
       " (117617, 12090, 4),\n",
       " (117617, 16767, 4),\n",
       " (121182, 4046, 4),\n",
       " (121182, 10296, 4),\n",
       " (122948, 5601, 4),\n",
       " (124603, 3418, 5),\n",
       " (124603, 5226, 2),\n",
       " (125307, 5071, 5),\n",
       " (125307, 8651, 5),\n",
       " (125307, 14137, 4),\n",
       " (127427, 4972, 4),\n",
       " (127427, 6844, 5),\n",
       " (127427, 11149, 4),\n",
       " (127746, 4865, 5),\n",
       " (127746, 16562, 3),\n",
       " (127920, 13787, 4),\n",
       " (129839, 3441, 3),\n",
       " (129839, 6859, 2),\n",
       " (129839, 9546, 2),\n",
       " (132170, 4865, 4),\n",
       " (132170, 5226, 4),\n",
       " (132170, 9818, 5),\n",
       " (132170, 11214, 4),\n",
       " (132170, 14320, 3),\n",
       " (132734, 4972, 5),\n",
       " (132734, 12090, 4),\n",
       " (134712, 4522, 1),\n",
       " (134712, 5071, 5),\n",
       " (134712, 6274, 4),\n",
       " (134712, 6844, 4),\n",
       " (134712, 16765, 4),\n",
       " (136608, 6844, 1),\n",
       " (136608, 11149, 1),\n",
       " (136617, 3418, 3),\n",
       " (136617, 3441, 3),\n",
       " (136617, 3526, 3),\n",
       " (136617, 5778, 4),\n",
       " (136617, 6844, 1),\n",
       " (136617, 8651, 2),\n",
       " (136617, 11149, 2),\n",
       " (136617, 12135, 4),\n",
       " (136617, 13787, 4),\n",
       " (136617, 15392, 3),\n",
       " (136617, 16765, 3),\n",
       " (137362, 12367, 3),\n",
       " (137690, 3441, 1),\n",
       " (139858, 11149, 3),\n",
       " (143821, 6844, 1),\n",
       " (143944, 962, 4),\n",
       " (143944, 3128, 4),\n",
       " (143944, 4522, 5),\n",
       " (143944, 6274, 5),\n",
       " (143944, 8651, 5),\n",
       " (143944, 9175, 5),\n",
       " (143944, 13074, 5),\n",
       " (143944, 16765, 3),\n",
       " (144086, 3418, 2),\n",
       " (144086, 6844, 4),\n",
       " (144265, 348, 4),\n",
       " (144265, 3441, 2),\n",
       " (144265, 5778, 3),\n",
       " (144265, 16094, 2),\n",
       " (145476, 4801, 1),\n",
       " (146307, 5071, 3),\n",
       " (146986, 6844, 3),\n",
       " (146986, 11149, 3),\n",
       " (147386, 666, 3),\n",
       " (147386, 2521, 4),\n",
       " (147386, 3976, 3),\n",
       " (147386, 4522, 4),\n",
       " (147386, 5071, 3),\n",
       " (147386, 6550, 3),\n",
       " (147386, 7344, 4),\n",
       " (147386, 7516, 4),\n",
       " (147386, 7970, 4),\n",
       " (147386, 9383, 4),\n",
       " (147386, 11292, 4),\n",
       " (147386, 13577, 3),\n",
       " (147386, 13757, 4),\n",
       " (147386, 14857, 3),\n",
       " (147386, 15126, 3),\n",
       " (148350, 7864, 4),\n",
       " (151228, 943, 4),\n",
       " (151732, 5071, 4),\n",
       " (153322, 5601, 3),\n",
       " (153322, 6274, 5),\n",
       " (153322, 6844, 4),\n",
       " (153322, 16765, 3),\n",
       " (160407, 3441, 3),\n",
       " (160407, 5071, 3),\n",
       " (163166, 4972, 5),\n",
       " (165446, 11149, 2),\n",
       " (167347, 4801, 1),\n",
       " (167347, 6844, 4),\n",
       " (167347, 8651, 2),\n",
       " (167347, 12090, 2),\n",
       " (167347, 13074, 4),\n",
       " (167347, 16765, 4),\n",
       " (168971, 5071, 5),\n",
       " (169148, 6859, 5),\n",
       " (169463, 6274, 4),\n",
       " (171840, 5601, 4),\n",
       " (171840, 9818, 5),\n",
       " (173128, 5071, 4),\n",
       " (176300, 6098, 3),\n",
       " (176300, 7970, 5),\n",
       " (176300, 11740, 4),\n",
       " (176300, 13384, 4),\n",
       " (176300, 16986, 3),\n",
       " (176543, 5226, 4),\n",
       " (178028, 3418, 5),\n",
       " (178028, 3441, 4),\n",
       " (178028, 6859, 4),\n",
       " (178028, 11149, 5),\n",
       " (180515, 6844, 4),\n",
       " (180515, 11350, 2),\n",
       " (182118, 11149, 5),\n",
       " (183339, 3441, 5),\n",
       " (183339, 5071, 5),\n",
       " (185473, 5071, 3),\n",
       " (185473, 5226, 5),\n",
       " (185473, 8376, 3),\n",
       " (185473, 8677, 3),\n",
       " (185473, 9392, 3),\n",
       " (185473, 12090, 2),\n",
       " (185473, 13384, 5),\n",
       " (185873, 1033, 4),\n",
       " (185873, 6274, 4),\n",
       " (186209, 705, 1),\n",
       " (186209, 6844, 3),\n",
       " (186209, 9818, 3),\n",
       " (186209, 16765, 3),\n",
       " (191777, 14233, 4),\n",
       " (191777, 16562, 3),\n",
       " (192375, 6686, 5),\n",
       " (197435, 4972, 4),\n",
       " (197435, 6859, 4),\n",
       " (197958, 13384, 3),\n",
       " (199323, 1140, 4),\n",
       " (199323, 3418, 3),\n",
       " (199323, 4005, 4),\n",
       " (199323, 12090, 3),\n",
       " (202909, 6859, 3),\n",
       " (202909, 15010, 3),\n",
       " (206222, 2111, 4),\n",
       " (206222, 2851, 4),\n",
       " (206222, 6948, 3),\n",
       " (206222, 7739, 4),\n",
       " (206222, 11751, 3),\n",
       " (206222, 13384, 3),\n",
       " (206222, 13787, 3),\n",
       " (206222, 14137, 3),\n",
       " (206222, 16937, 3),\n",
       " (207196, 12367, 5),\n",
       " (207285, 4972, 5),\n",
       " (207285, 6844, 2),\n",
       " (207798, 4972, 4),\n",
       " (207798, 5226, 4),\n",
       " (207798, 6844, 4),\n",
       " (207798, 9818, 5),\n",
       " (207798, 16765, 5),\n",
       " (207858, 11149, 2),\n",
       " (209180, 6859, 3),\n",
       " (210286, 5071, 3),\n",
       " (210743, 4522, 4),\n",
       " (210743, 6274, 4),\n",
       " (211906, 4972, 2),\n",
       " (211906, 5226, 5),\n",
       " (211976, 15786, 4),\n",
       " (212329, 658, 3),\n",
       " (212329, 962, 3),\n",
       " (212329, 2778, 4),\n",
       " (212329, 5226, 3),\n",
       " (212329, 13074, 2),\n",
       " (212329, 14320, 3),\n",
       " (212329, 15861, 4),\n",
       " (212329, 16767, 4),\n",
       " (213844, 658, 3),\n",
       " (213844, 6844, 3),\n",
       " (213844, 12090, 3),\n",
       " (214950, 4972, 4),\n",
       " (214950, 6844, 4),\n",
       " (215084, 11149, 1),\n",
       " (217706, 5060, 4),\n",
       " (217982, 7197, 3),\n",
       " (218648, 16755, 4),\n",
       " (220002, 16765, 4),\n",
       " (220487, 11751, 4),\n",
       " (220589, 6844, 2),\n",
       " (220589, 11149, 3),\n",
       " (222946, 11149, 3),\n",
       " (223169, 3128, 4),\n",
       " (223169, 6274, 5),\n",
       " (223169, 6844, 3),\n",
       " (223169, 7739, 2),\n",
       " (223169, 9818, 4),\n",
       " (223169, 13787, 4),\n",
       " (223464, 5778, 3),\n",
       " (224109, 8127, 2),\n",
       " (224109, 11538, 1),\n",
       " (224109, 11970, 1),\n",
       " (224109, 13870, 3),\n",
       " (226137, 943, 5),\n",
       " (226137, 3441, 3),\n",
       " (230122, 6859, 4),\n",
       " (231100, 6859, 4),\n",
       " (232629, 6274, 4),\n",
       " (235258, 5071, 3),\n",
       " (237427, 5071, 4),\n",
       " (238767, 16767, 5),\n",
       " (240399, 658, 4),\n",
       " (240399, 3128, 3),\n",
       " (240399, 3441, 3),\n",
       " (240399, 5071, 4),\n",
       " (240399, 6859, 3),\n",
       " (240399, 7739, 2),\n",
       " (243727, 2533, 3),\n",
       " (243727, 4865, 4),\n",
       " (243727, 5226, 5),\n",
       " (243727, 5778, 2),\n",
       " (243727, 9818, 4),\n",
       " (243727, 10734, 4),\n",
       " (243727, 13384, 4),\n",
       " (243727, 16538, 3),\n",
       " (243800, 11149, 5),\n",
       " (243800, 12090, 1),\n",
       " (245944, 14320, 4),\n",
       " (251595, 5816, 5),\n",
       " (251595, 15392, 5),\n",
       " (251595, 16538, 5),\n",
       " (254895, 13787, 4),\n",
       " (254895, 16765, 4),\n",
       " (255120, 14320, 5),\n",
       " (255570, 3418, 3),\n",
       " (255570, 4972, 5),\n",
       " (255570, 6844, 4),\n",
       " (255570, 11149, 3),\n",
       " (255570, 12865, 4),\n",
       " (255570, 13074, 5),\n",
       " (256601, 11149, 4),\n",
       " (256620, 6844, 1),\n",
       " (257294, 6948, 3),\n",
       " (257294, 8174, 4),\n",
       " (260855, 8677, 4),\n",
       " (260855, 11149, 1),\n",
       " (260855, 12090, 2),\n",
       " (262854, 5231, 5),\n",
       " (262926, 3441, 4),\n",
       " (262926, 16765, 3),\n",
       " (264772, 1163, 4),\n",
       " (264772, 11350, 5),\n",
       " (267529, 16986, 3),\n",
       " (273564, 5071, 3),\n",
       " (273564, 13074, 3),\n",
       " (274212, 11149, 4),\n",
       " (276405, 4972, 1),\n",
       " (276405, 6844, 1),\n",
       " (276827, 6844, 3),\n",
       " (276827, 6859, 5),\n",
       " (281338, 12367, 2),\n",
       " (281815, 1163, 3),\n",
       " (281815, 5778, 5),\n",
       " (281815, 13074, 4),\n",
       " (282486, 4865, 4),\n",
       " (285218, 4972, 3),\n",
       " (285218, 6859, 3),\n",
       " (286773, 3418, 5),\n",
       " (286773, 6844, 5),\n",
       " (286773, 11149, 3),\n",
       " (289045, 4972, 1),\n",
       " (289045, 5226, 4),\n",
       " (289045, 6844, 2),\n",
       " (289045, 7516, 5),\n",
       " (289045, 8376, 4),\n",
       " (291773, 6098, 5),\n",
       " (291773, 6844, 5),\n",
       " (297546, 6844, 3),\n",
       " (299851, 3128, 3),\n",
       " (300282, 678, 5),\n",
       " (303901, 111, 3),\n",
       " (303901, 705, 3),\n",
       " (303901, 3418, 3),\n",
       " (303901, 5071, 4),\n",
       " (303901, 9392, 3),\n",
       " (303901, 11149, 3),\n",
       " (307248, 3526, 4),\n",
       " (307248, 5071, 3),\n",
       " (307248, 13074, 4),\n",
       " (307767, 5071, 5),\n",
       " (307811, 6844, 3),\n",
       " (307811, 11149, 5),\n",
       " (309524, 6844, 3),\n",
       " (309524, 11149, 4),\n",
       " (309524, 16765, 3),\n",
       " (310887, 111, 4),\n",
       " (310887, 658, 5),\n",
       " (310887, 681, 3),\n",
       " (310887, 705, 3),\n",
       " (310887, 823, 4),\n",
       " (310887, 962, 4),\n",
       " (310887, 1140, 4),\n",
       " (310887, 2521, 3),\n",
       " (310887, 2851, 3),\n",
       " (310887, 3128, 3),\n",
       " (310887, 3418, 3),\n",
       " (310887, 3526, 5),\n",
       " (310887, 4441, 4),\n",
       " (310887, 4801, 3),\n",
       " (310887, 4972, 4),\n",
       " (310887, 5071, 5),\n",
       " (310887, 5226, 2),\n",
       " (310887, 5521, 3),\n",
       " (310887, 5778, 4),\n",
       " (310887, 6274, 5),\n",
       " (310887, 6729, 3),\n",
       " (310887, 6844, 4),\n",
       " (310887, 6859, 4),\n",
       " (310887, 6948, 4),\n",
       " (310887, 7739, 4),\n",
       " (310887, 7864, 3),\n",
       " (310887, 8437, 4),\n",
       " (310887, 8518, 2),\n",
       " (310887, 8651, 3),\n",
       " (310887, 9392, 4),\n",
       " (310887, 9546, 3),\n",
       " (310887, 11034, 3),\n",
       " (310887, 11149, 2),\n",
       " (310887, 11158, 3),\n",
       " (310887, 11214, 2),\n",
       " (310887, 11350, 4),\n",
       " (310887, 11740, 3),\n",
       " (310887, 12090, 4),\n",
       " (310887, 12303, 4),\n",
       " (310887, 12446, 3),\n",
       " (310887, 13072, 3),\n",
       " (310887, 13074, 5),\n",
       " (310887, 13787, 3),\n",
       " (310887, 14137, 5),\n",
       " (310887, 16094, 3),\n",
       " (310887, 16352, 3),\n",
       " (310887, 16765, 5),\n",
       " (310887, 16767, 5),\n",
       " (310887, 17251, 5),\n",
       " (310887, 17274, 3),\n",
       " (312549, 705, 4),\n",
       " (312549, 6844, 2),\n",
       " (312549, 6859, 3),\n",
       " (312549, 9818, 3),\n",
       " (312549, 10372, 3),\n",
       " (312549, 13074, 1),\n",
       " (313962, 111, 1),\n",
       " (313962, 5071, 1),\n",
       " (313962, 7197, 2),\n",
       " (313962, 8174, 4),\n",
       " (313962, 13384, 3),\n",
       " (316162, 5778, 4),\n",
       " (316240, 4972, 4),\n",
       " (318001, 13074, 3),\n",
       " (321623, 6844, 2),\n",
       " (321623, 11149, 4),\n",
       " (322070, 5071, 5),\n",
       " (322909, 3418, 5),\n",
       " (322909, 5601, 4),\n",
       " (322909, 11149, 3),\n",
       " (322909, 12090, 3),\n",
       " (322909, 13072, 4),\n",
       " (322909, 13384, 5),\n",
       " (323697, 8376, 4),\n",
       " (324800, 6274, 5),\n",
       " (325327, 111, 5),\n",
       " (325327, 705, 2),\n",
       " (325327, 3418, 3),\n",
       " (325327, 4522, 5),\n",
       " (325327, 4801, 5),\n",
       " (325327, 4972, 5),\n",
       " (325327, 6844, 5),\n",
       " (325327, 6859, 4),\n",
       " (325327, 9392, 4),\n",
       " (325327, 11149, 5),\n",
       " (325327, 11350, 4),\n",
       " (325327, 16765, 5),\n",
       " (325332, 2111, 4),\n",
       " (325332, 4801, 3),\n",
       " (325581, 658, 4),\n",
       " (325581, 6274, 3),\n",
       " (325581, 16765, 5),\n",
       " (326328, 7516, 4),\n",
       " (330079, 5071, 1),\n",
       " (330430, 750, 3),\n",
       " (330430, 4972, 5),\n",
       " (330430, 6274, 5),\n",
       " (330897, 11149, 4),\n",
       " (333392, 11149, 4),\n",
       " (334443, 6274, 2),\n",
       " (334443, 16767, 5),\n",
       " (334587, 6859, 5),\n",
       " (334957, 943, 3),\n",
       " (337357, 11149, 5),\n",
       " (338633, 658, 4),\n",
       " (338633, 705, 5),\n",
       " (338633, 3418, 4),\n",
       " (338633, 3526, 3),\n",
       " (338633, 4972, 3),\n",
       " (338633, 6844, 4),\n",
       " (338633, 11149, 4),\n",
       " (338633, 16765, 5),\n",
       " (340736, 3418, 3),\n",
       " (342454, 13074, 4),\n",
       " (344394, 4972, 2),\n",
       " (344394, 6274, 4),\n",
       " (344394, 6844, 2),\n",
       " (344394, 13787, 3),\n",
       " (344394, 14137, 3),\n",
       " (344394, 16765, 4),\n",
       " (345502, 5601, 2),\n",
       " (347834, 5778, 4),\n",
       " (349185, 13074, 3),\n",
       " (350055, 5071, 4),\n",
       " (350875, 4972, 2),\n",
       " (356035, 111, 3),\n",
       " (356035, 4972, 2),\n",
       " (356035, 5071, 3),\n",
       " (356035, 6836, 5),\n",
       " (356035, 7739, 2),\n",
       " (356035, 13074, 2),\n",
       " (357632, 6844, 3),\n",
       " (363047, 7970, 3),\n",
       " (364143, 750, 3),\n",
       " (364143, 5071, 3),\n",
       " (364740, 9818, 5),\n",
       " (364740, 10372, 5),\n",
       " (364740, 15786, 3),\n",
       " (365855, 6274, 3),\n",
       " (365855, 6844, 4),\n",
       " (365855, 6859, 5),\n",
       " (365855, 16765, 3),\n",
       " (366815, 5071, 4),\n",
       " (366815, 8376, 4),\n",
       " (368978, 6844, 2),\n",
       " (368978, 16562, 5),\n",
       " (369164, 348, 4),\n",
       " (369164, 8677, 5),\n",
       " (369164, 11214, 3),\n",
       " (369706, 5071, 1),\n",
       " (369706, 14233, 2),\n",
       " (372233, 16858, 2),\n",
       " (376234, 6844, 5),\n",
       " (376234, 6859, 4),\n",
       " (376234, 11149, 3),\n",
       " (377504, 13074, 3),\n",
       " (377504, 16986, 4),\n",
       " (379215, 6274, 4),\n",
       " (379215, 16767, 5),\n",
       " (380044, 4972, 4),\n",
       " (380044, 6859, 4),\n",
       " (389668, 5226, 5),\n",
       " (389668, 10372, 5),\n",
       " (391370, 11149, 3),\n",
       " (391666, 1691, 5),\n",
       " (391666, 2895, 5),\n",
       " (391666, 4972, 3),\n",
       " (391666, 6859, 3),\n",
       " (391666, 9587, 5),\n",
       " (391666, 11563, 5),\n",
       " (391666, 15861, 3),\n",
       " (391666, 16765, 3),\n",
       " (397212, 943, 5),\n",
       " (397212, 3389, 5),\n",
       " (397212, 3950, 1),\n",
       " (397212, 8415, 4),\n",
       " (397212, 12385, 5),\n",
       " (397212, 14435, 4),\n",
       " (399409, 11149, 3),\n",
       " (399567, 11149, 2),\n",
       " (399713, 658, 5),\n",
       " (399713, 3418, 3),\n",
       " (399713, 4972, 3),\n",
       " (399713, 5601, 4),\n",
       " (399713, 16767, 5),\n",
       " (399761, 11149, 1),\n",
       " (399927, 3418, 2),\n",
       " (400368, 4972, 2),\n",
       " (400831, 6844, 3),\n",
       " (401081, 6274, 4),\n",
       " (401081, 6859, 4),\n",
       " (401081, 9818, 3),\n",
       " (406789, 3128, 4),\n",
       " (406789, 4972, 4),\n",
       " (406789, 7739, 4),\n",
       " (406789, 13787, 4),\n",
       " (406789, 14137, 4),\n",
       " (407414, 4522, 4),\n",
       " (407414, 6274, 5),\n",
       " (407414, 14137, 5),\n",
       " (407414, 16858, 5),\n",
       " (407528, 420, 4),\n",
       " (407528, 658, 4),\n",
       " (407528, 705, 4),\n",
       " (407528, 5071, 3),\n",
       " (407528, 5226, 4),\n",
       " (407528, 6844, 1),\n",
       " (407528, 9818, 5),\n",
       " (407528, 13074, 2),\n",
       " (407528, 13384, 4),\n",
       " (407528, 16765, 2),\n",
       " (407738, 6844, 4),\n",
       " (407738, 16765, 5),\n",
       " (410206, 658, 3),\n",
       " (410206, 3309, 3),\n",
       " (410206, 7739, 1),\n",
       " (410206, 8437, 2),\n",
       " (410206, 8651, 3),\n",
       " (410206, 9175, 3),\n",
       " (410206, 9546, 2),\n",
       " (410206, 9818, 5),\n",
       " (410206, 10296, 4),\n",
       " (410206, 12311, 3),\n",
       " (410206, 12429, 3),\n",
       " (410206, 12800, 4),\n",
       " (410206, 13384, 3),\n",
       " (410206, 14832, 3),\n",
       " (410206, 15010, 3),\n",
       " (410206, 15738, 3),\n",
       " (410206, 15805, 3),\n",
       " (410206, 16094, 2),\n",
       " (410206, 16443, 3),\n",
       " (411471, 5071, 2),\n",
       " (411684, 14233, 4),\n",
       " (411684, 15786, 3),\n",
       " (412935, 16538, 3),\n",
       " (414769, 2851, 3),\n",
       " (414769, 17251, 3),\n",
       " (416156, 14137, 1),\n",
       " (416698, 5821, 4),\n",
       " (416817, 4972, 1),\n",
       " (416817, 6274, 4),\n",
       " (419250, 6844, 4),\n",
       " (419250, 11149, 5),\n",
       " (420020, 6844, 3),\n",
       " (420020, 11149, 3),\n",
       " (420026, 15520, 4),\n",
       " (421926, 658, 4),\n",
       " (421926, 7739, 3),\n",
       " (421926, 14233, 4),\n",
       " (422144, 6844, 3),\n",
       " (422144, 11149, 1),\n",
       " (422234, 111, 2),\n",
       " (422234, 5071, 2),\n",
       " (422243, 3418, 5),\n",
       " (422243, 4972, 4),\n",
       " (422243, 6844, 5),\n",
       " (422243, 11149, 4),\n",
       " (422243, 16765, 5),\n",
       " (424964, 5778, 3),\n",
       " (428331, 11149, 2),\n",
       " (428867, 3441, 3),\n",
       " (429025, 705, 3),\n",
       " (429072, 3441, 3),\n",
       " (431513, 705, 4),\n",
       " (431513, 4972, 4),\n",
       " (431513, 6274, 4),\n",
       " (431513, 6844, 3),\n",
       " (431513, 16765, 4),\n",
       " (433268, 2895, 4),\n",
       " (436841, 4972, 3),\n",
       " (436841, 6859, 1),\n",
       " (441277, 11149, 3),\n",
       " (441277, 12367, 4),\n",
       " (442034, 11149, 4),\n",
       " (442262, 11149, 2),\n",
       " (442262, 12090, 3),\n",
       " (443183, 5071, 5),\n",
       " (443353, 2851, 3),\n",
       " (443353, 6274, 3),\n",
       " (443353, 12446, 4),\n",
       " (443863, 7144, 2),\n",
       " (443863, 8572, 4),\n",
       " (443863, 12385, 2),\n",
       " (443863, 15264, 3),\n",
       " (443951, 11149, 3),\n",
       " (443951, 16767, 1),\n",
       " (449231, 8376, 4),\n",
       " (450217, 6274, 5),\n",
       " (450217, 9818, 4),\n",
       " (451776, 943, 4),\n",
       " (452724, 5226, 5),\n",
       " (453137, 5226, 5),\n",
       " (453137, 9818, 4),\n",
       " (457292, 5226, 5),\n",
       " (457618, 6859, 3),\n",
       " (457618, 11149, 4),\n",
       " (457685, 5071, 4),\n",
       " (460982, 12090, 2),\n",
       " (463920, 6844, 4),\n",
       " (464539, 9818, 5),\n",
       " (464539, 16767, 4),\n",
       " (467292, 6844, 3),\n",
       " (467292, 11149, 4),\n",
       " (467380, 13074, 4),\n",
       " (470500, 2533, 4),\n",
       " (471086, 15010, 3),\n",
       " (471173, 3441, 3),\n",
       " (471266, 705, 3),\n",
       " (471266, 3526, 5),\n",
       " (471266, 5840, 5),\n",
       " (471266, 13384, 4),\n",
       " (471414, 5778, 3),\n",
       " (471414, 6098, 4),\n",
       " (471414, 11350, 3),\n",
       " (471621, 2533, 3),\n",
       " (471621, 3441, 3),\n",
       " (471621, 5071, 5),\n",
       " (473955, 6274, 4),\n",
       " (473955, 13074, 3),\n",
       " (473955, 16765, 3),\n",
       " (475784, 13876, 2),\n",
       " (476043, 6844, 3),\n",
       " (476043, 16765, 4),\n",
       " (476043, 17372, 3),\n",
       " (482332, 6274, 4),\n",
       " (488271, 6859, 4),\n",
       " (489013, 6844, 4),\n",
       " (489927, 6844, 2),\n",
       " (491297, 5071, 5),\n",
       " (492280, 9818, 4),\n",
       " (492280, 10372, 5),\n",
       " (492280, 13384, 5),\n",
       " (492706, 6844, 3),\n",
       " (492706, 11149, 4),\n",
       " (494040, 11149, 1),\n",
       " (494494, 3418, 5),\n",
       " (495549, 6859, 3),\n",
       " (496561, 11292, 5),\n",
       " (496576, 5071, 3),\n",
       " (499538, 5071, 2),\n",
       " (500932, 4972, 3),\n",
       " (500932, 11149, 3),\n",
       " (500932, 16765, 4),\n",
       " (501612, 4972, 3),\n",
       " (501612, 6859, 3),\n",
       " (501656, 5071, 4),\n",
       " (506377, 5226, 4),\n",
       " (506377, 9818, 3),\n",
       " (508355, 6844, 5),\n",
       " (510355, 6844, 1),\n",
       " (510355, 8174, 3),\n",
       " (511141, 11149, 3),\n",
       " (514728, 6859, 3),\n",
       " (515303, 11068, 5),\n",
       " (516060, 16765, 3),\n",
       " (518137, 111, 3),\n",
       " (518137, 251, 2),\n",
       " (518137, 420, 3),\n",
       " (518137, 658, 4),\n",
       " (518137, 705, 4),\n",
       " (518137, 957, 4),\n",
       " (518137, 962, 3),\n",
       " (518137, 1163, 3),\n",
       " (518137, 1648, 5),\n",
       " (518137, 1828, 3),\n",
       " (518137, 1830, 4),\n",
       " (518137, 2111, 5),\n",
       " (518137, 2851, 4),\n",
       " (518137, 2859, 4),\n",
       " (518137, 2895, 3),\n",
       " (518137, 3247, 4),\n",
       " (518137, 3381, 1),\n",
       " (518137, 3389, 4),\n",
       " (518137, 3418, 3),\n",
       " (518137, 3554, 5),\n",
       " (518137, 3784, 2),\n",
       " (518137, 3913, 2),\n",
       " (518137, 4011, 4),\n",
       " (518137, 4105, 3),\n",
       " (518137, 4461, 4),\n",
       " (518137, 4972, 3),\n",
       " (518137, 5071, 3),\n",
       " (518137, 5226, 4),\n",
       " (518137, 5231, 5),\n",
       " (518137, 5601, 3),\n",
       " (518137, 5816, 4),\n",
       " (518137, 5840, 4),\n",
       " (518137, 6274, 4),\n",
       " (518137, 6514, 3),\n",
       " (518137, 6686, 4),\n",
       " (518137, 6799, 3),\n",
       " (518137, 6849, 4),\n",
       " (518137, 7060, 3),\n",
       " (518137, 7516, 4),\n",
       " (518137, 7970, 3),\n",
       " (518137, 8376, 3),\n",
       " (518137, 8572, 4),\n",
       " (518137, 8677, 4),\n",
       " (518137, 9622, 2),\n",
       " (518137, 10372, 3),\n",
       " (518137, 12135, 4),\n",
       " (518137, 12446, 4),\n",
       " (518137, 12925, 3),\n",
       " (518137, 13787, 4),\n",
       " (518137, 13866, 3),\n",
       " (518137, 14320, 5),\n",
       " (518137, 15264, 4),\n",
       " (518137, 15392, 5),\n",
       " (518137, 16264, 5),\n",
       " (518137, 16937, 1),\n",
       " (518439, 3128, 4),\n",
       " (518439, 8677, 4),\n",
       " (518439, 9818, 5),\n",
       " (518439, 10372, 5),\n",
       " (518439, 13384, 5),\n",
       " (518439, 14233, 5),\n",
       " (518439, 14437, 4),\n",
       " (518439, 15805, 5),\n",
       " (520645, 14387, 4),\n",
       " (520909, 10372, 2),\n",
       " (521364, 5071, 5),\n",
       " (521443, 5601, 5),\n",
       " (521569, 750, 2),\n",
       " (521569, 5071, 3),\n",
       " (522578, 6844, 3),\n",
       " (522578, 6859, 4),\n",
       " (532679, 6859, 3),\n",
       " (535419, 658, 3),\n",
       " (535419, 3441, 2),\n",
       " (535419, 5071, 3),\n",
       " (535419, 10372, 4),\n",
       " (535419, 11214, 4),\n",
       " (536429, 6274, 4),\n",
       " (536797, 6859, 5),\n",
       " (537462, 8376, 4),\n",
       " (538160, 6844, 3),\n",
       " (538406, 5226, 4),\n",
       " (539743, 3554, 4),\n",
       " (539743, 6512, 4),\n",
       " (539743, 6612, 4),\n",
       " (539743, 10734, 4),\n",
       " (539743, 11292, 3),\n",
       " (539743, 12446, 3),\n",
       " (539743, 12800, 4),\n",
       " (539743, 14685, 3),\n",
       " (542405, 4972, 4),\n",
       " (542405, 8651, 4),\n",
       " (542405, 12090, 4),\n",
       " (542405, 13787, 4),\n",
       " (542405, 16767, 5),\n",
       " (543019, 5778, 4),\n",
       " (543019, 14137, 4),\n",
       " (545915, 309, 4),\n",
       " (545915, 8677, 4),\n",
       " (546439, 6859, 3),\n",
       " (550655, 4972, 3),\n",
       " (550655, 17274, 2),\n",
       " (550802, 6948, 5),\n",
       " (551154, 11149, 2),\n",
       " (554811, 4972, 4),\n",
       " (554811, 6844, 5),\n",
       " (556633, 12385, 4),\n",
       " (556633, 14233, 3),\n",
       " (558543, 5071, 2),\n",
       " (558543, 16765, 3),\n",
       " (558658, 4972, 3),\n",
       " (558658, 13074, 4),\n",
       " (560116, 11149, 2),\n",
       " ...]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset = list(zip(reg_test_df.user.values, reg_test_df.movie.values, reg_test_df.rating.values))\n",
    "testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4530"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(testset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying ML models \n",
    "I would be using ML models and storing their MAPE and RMSE in following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({}, {})"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_evaluation_train = dict()\n",
    "models_evaluation_test = dict()\n",
    "\n",
    "models_evaluation_train, models_evaluation_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XgBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get rmse and mape given actual and predicted ratings..\n",
    "def get_error_metrics(y_true, y_pred):\n",
    "    rmse = np.sqrt(np.mean([ (y_true[i] - y_pred[i])**2 for i in range(len(y_pred)) ]))\n",
    "    mape = np.mean(np.abs( (y_true - y_pred)/y_true )) * 100\n",
    "    return rmse, mape\n",
    "\n",
    "def run_xgboost(algo,  x_train, y_train, x_test, y_test, verbose=True):\n",
    "    \"\"\"\n",
    "    It will return train_results and test_results\n",
    "    \"\"\"\n",
    "    \n",
    "    # dictionaries for storing train and test results\n",
    "    train_results = dict()\n",
    "    test_results = dict()\n",
    "    \n",
    "    \n",
    "    # fit the model\n",
    "    print('Training the model..')\n",
    "    start =datetime.now()\n",
    "    algo.fit(x_train, y_train, eval_metric = 'rmse')\n",
    "    print('Done. Time taken : {}\\n'.format(datetime.now()-start))\n",
    "    print('Done \\n')\n",
    "\n",
    "    # from the trained model, get the predictions....\n",
    "    print('Evaluating the model with TRAIN data...')\n",
    "    start =datetime.now()\n",
    "    y_train_pred = algo.predict(x_train)\n",
    "    # get the rmse and mape of train data...\n",
    "    rmse_train, mape_train = get_error_metrics(y_train.values, y_train_pred)\n",
    "    \n",
    "    # store the results in train_results dictionary..\n",
    "    train_results = {'rmse': rmse_train,\n",
    "                    'mape' : mape_train,\n",
    "                    'predictions' : y_train_pred}\n",
    "    \n",
    "  \n",
    "    # get the test data predictions and compute rmse and mape\n",
    "    print('Evaluating Test data')\n",
    "    y_test_pred = algo.predict(x_test) \n",
    "    rmse_test, mape_test = get_error_metrics(y_true=y_test.values, y_pred=y_test_pred)\n",
    "    # store them in our test results dictionary.\n",
    "    test_results = {'rmse': rmse_test,\n",
    "                    'mape' : mape_test,\n",
    "                    'predictions':y_test_pred}\n",
    "    if verbose:\n",
    "        print('-'*15)\n",
    "        print('\\nTEST DATA')\n",
    "        print('-'*15)\n",
    "        print('RMSE : ', rmse_test)\n",
    "        print('MAPE : ', mape_test)\n",
    "        \n",
    "    # return these train and test results...\n",
    "    return train_results, test_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Surprise Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it is just to makesure that all of our algorithms should produce same results\n",
    "# everytime it run\n",
    "\n",
    "my_seed = 15\n",
    "random.seed(my_seed)\n",
    "np.random.seed(my_seed)\n",
    "\n",
    "\n",
    "def get_ratings(predictions):\n",
    "    actual = np.array([pred.r_ui for pred in predictions])\n",
    "    pred = np.array([pred.est for pred in predictions])\n",
    "    \n",
    "    return actual, pred\n",
    "\n",
    "\n",
    "def get_errors(predictions, print_them=False):\n",
    "\n",
    "    actual, pred = get_ratings(predictions)\n",
    "    rmse = np.sqrt(np.mean((pred - actual)**2))\n",
    "    mape = np.mean(np.abs(pred - actual)/actual)\n",
    "\n",
    "    return rmse, mape*100\n",
    "\n",
    "\n",
    "def run_surprise(algo, trainset, testset, verbose=True): \n",
    "    '''\n",
    "        return train_dict, test_dict\n",
    "    \n",
    "        It returns two dictionaries, one for train and the other is for test\n",
    "        Each of them have 3 key-value pairs, which specify ''rmse'', ''mape'', and ''predicted ratings''.\n",
    "    '''\n",
    "    start = datetime.now()\n",
    "    # dictionaries that stores metrics for train and test..\n",
    "    train = dict()\n",
    "    test = dict()\n",
    "    \n",
    "    # train the algorithm with the trainset\n",
    "    st = datetime.now()\n",
    "    print('Training the model...')\n",
    "    algo.fit(trainset)\n",
    "    print('Done. time taken : {} \\n'.format(datetime.now()-st))\n",
    "    \n",
    "    # ---------------- Evaluating train data--------------------#\n",
    "    st = datetime.now()\n",
    "    print('Evaluating the model with train data..')\n",
    "    # get the train predictions (list of prediction class inside Surprise)\n",
    "    train_preds = algo.test(trainset.build_testset())\n",
    "    # get predicted ratings from the train predictions..\n",
    "    train_actual_ratings, train_pred_ratings = get_ratings(train_preds)\n",
    "    # get ''rmse'' and ''mape'' from the train predictions.\n",
    "    train_rmse, train_mape = get_errors(train_preds)\n",
    "    print('time taken : {}'.format(datetime.now()-st))\n",
    "    \n",
    "    if verbose:\n",
    "        print('-'*15)\n",
    "        print('Train Data')\n",
    "        print('-'*15)\n",
    "        print(\"RMSE : {}\\n\\nMAPE : {}\\n\".format(train_rmse, train_mape))\n",
    "    \n",
    "    #store them in the train dictionary\n",
    "    if verbose:\n",
    "        print('adding train results in the dictionary..')\n",
    "    train['rmse'] = train_rmse\n",
    "    train['mape'] = train_mape\n",
    "    train['predictions'] = train_pred_ratings\n",
    "    \n",
    "    #------------ Evaluating Test data---------------#\n",
    "    st = datetime.now()\n",
    "    print('\\nEvaluating for test data...')\n",
    "    # get the predictions( list of prediction classes) of test data\n",
    "    test_preds = algo.test(testset)\n",
    "    # get the predicted ratings from the list of predictions\n",
    "    test_actual_ratings, test_pred_ratings = get_ratings(test_preds)\n",
    "    # get error metrics from the predicted and actual ratings\n",
    "    test_rmse, test_mape = get_errors(test_preds)\n",
    "    print('time taken : {}'.format(datetime.now()-st))\n",
    "    \n",
    "    if verbose:\n",
    "        print('-'*15)\n",
    "        print('Test Data')\n",
    "        print('-'*15)\n",
    "        print(\"RMSE : {}\\n\\nMAPE : {}\\n\".format(test_rmse, test_mape))\n",
    "    # store them in test dictionary\n",
    "    if verbose:\n",
    "        print('storing the test results in test dictionary...')\n",
    "    test['rmse'] = test_rmse\n",
    "    test['mape'] = test_mape\n",
    "    test['predictions'] = test_pred_ratings\n",
    "    \n",
    "    print('\\n'+'-'*45)\n",
    "    print('Total time taken to run this algorithm :', datetime.now() - start)\n",
    "    \n",
    "    # return two dictionaries train and test\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost with 13 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare Train data\n",
    "x_train = reg_train.drop(['user','movie','rating'], axis=1)\n",
    "y_train = reg_train['rating']\n",
    "\n",
    "# Prepare Test data\n",
    "x_test = reg_test_df.drop(['user','movie','rating'], axis=1)\n",
    "y_test = reg_test_df['rating']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running XGBRegressor, we will tune hyperparameter using gridsearch cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'max_depth':[1,2,3],\n",
    "              'learning_rate':[0.001,0.01,0.1],\n",
    "              'n_estimators':[100,300,500,700,900,1100,1300]} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 63 candidates, totalling 315 fits\n",
      "Best: -0.017268 using {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 1300}\n",
      "-1.012954 (0.030909) with: {'learning_rate': 0.001, 'max_depth': 1, 'n_estimators': 100}\n",
      "-0.794077 (0.026827) with: {'learning_rate': 0.001, 'max_depth': 1, 'n_estimators': 300}\n",
      "-0.639678 (0.022093) with: {'learning_rate': 0.001, 'max_depth': 1, 'n_estimators': 500}\n",
      "-0.519820 (0.018441) with: {'learning_rate': 0.001, 'max_depth': 1, 'n_estimators': 700}\n",
      "-0.424710 (0.015506) with: {'learning_rate': 0.001, 'max_depth': 1, 'n_estimators': 900}\n",
      "-0.349214 (0.013238) with: {'learning_rate': 0.001, 'max_depth': 1, 'n_estimators': 1100}\n",
      "-0.289241 (0.011460) with: {'learning_rate': 0.001, 'max_depth': 1, 'n_estimators': 1300}\n",
      "-0.961326 (0.028048) with: {'learning_rate': 0.001, 'max_depth': 2, 'n_estimators': 100}\n",
      "-0.665545 (0.020358) with: {'learning_rate': 0.001, 'max_depth': 2, 'n_estimators': 300}\n",
      "-0.466127 (0.015410) with: {'learning_rate': 0.001, 'max_depth': 2, 'n_estimators': 500}\n",
      "-0.329994 (0.012028) with: {'learning_rate': 0.001, 'max_depth': 2, 'n_estimators': 700}\n",
      "-0.236688 (0.009644) with: {'learning_rate': 0.001, 'max_depth': 2, 'n_estimators': 900}\n",
      "-0.172644 (0.007924) with: {'learning_rate': 0.001, 'max_depth': 2, 'n_estimators': 1100}\n",
      "-0.128615 (0.006687) with: {'learning_rate': 0.001, 'max_depth': 2, 'n_estimators': 1300}\n",
      "-0.955324 (0.027834) with: {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 100}\n",
      "-0.651150 (0.019687) with: {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 300}\n",
      "-0.446866 (0.014269) with: {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 500}\n",
      "-0.309668 (0.010705) with: {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 700}\n",
      "-0.217385 (0.008314) with: {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 900}\n",
      "-0.155317 (0.006726) with: {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 1100}\n",
      "-0.113551 (0.005679) with: {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 1300}\n",
      "-0.383049 (0.014172) with: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 100}\n",
      "-0.078265 (0.004879) with: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 300}\n",
      "-0.036266 (0.003678) with: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 500}\n",
      "-0.030067 (0.003597) with: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 700}\n",
      "-0.029157 (0.003606) with: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 900}\n",
      "-0.028822 (0.003582) with: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 1100}\n",
      "-0.028557 (0.003561) with: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 1300}\n",
      "-0.200257 (0.008675) with: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 100}\n",
      "-0.033397 (0.003748) with: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 300}\n",
      "-0.028035 (0.003553) with: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 500}\n",
      "-0.026466 (0.003500) with: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 700}\n",
      "-0.025311 (0.003390) with: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 900}\n",
      "-0.024574 (0.003327) with: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 1100}\n",
      "-0.024132 (0.003330) with: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 1300}\n",
      "-0.181929 (0.007447) with: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 100}\n",
      "-0.029295 (0.003601) with: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 300}\n",
      "-0.024162 (0.003509) with: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 500}\n",
      "-0.023011 (0.003378) with: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 700}\n",
      "-0.021625 (0.003229) with: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 900}\n",
      "-0.020589 (0.003212) with: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 1100}\n",
      "-0.019970 (0.003193) with: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 1300}\n",
      "-0.029087 (0.003629) with: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 100}\n",
      "-0.027403 (0.003462) with: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 300}\n",
      "-0.026849 (0.003372) with: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 500}\n",
      "-0.026635 (0.003329) with: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 700}\n",
      "-0.026542 (0.003304) with: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 900}\n",
      "-0.026489 (0.003291) with: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 1100}\n",
      "-0.026443 (0.003290) with: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 1300}\n",
      "-0.024835 (0.003267) with: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 100}\n",
      "-0.022397 (0.003219) with: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 300}\n",
      "-0.021293 (0.003147) with: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 500}\n",
      "-0.020567 (0.003125) with: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 700}\n",
      "-0.020071 (0.003085) with: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 900}\n",
      "-0.019749 (0.003052) with: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 1100}\n",
      "-0.019485 (0.003063) with: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 1300}\n",
      "-0.021021 (0.003127) with: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100}\n",
      "-0.018344 (0.003173) with: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 300}\n",
      "-0.017683 (0.003198) with: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 500}\n",
      "-0.017446 (0.003265) with: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 700}\n",
      "-0.017354 (0.003362) with: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 900}\n",
      "-0.017282 (0.003436) with: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 1100}\n",
      "-0.017268 (0.003507) with: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 1300}\n",
      "\n",
      "Time Taken:  -1 day, 23:59:06.694851\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "# Initialize Our first XGBoost model\n",
    "first_xgb = xgb.XGBRegressor(nthread=-1)\n",
    "\n",
    "# Perform cross validation \n",
    "gscv = GridSearchCV(first_xgb,\n",
    "                    param_grid = parameters,\n",
    "                    scoring=\"neg_mean_squared_error\",\n",
    "                    cv = TimeSeriesSplit(n_splits=5),\n",
    "                    n_jobs = -1,\n",
    "                    verbose = 1)\n",
    "gscv_result = gscv.fit(x_train, y_train)\n",
    "\n",
    "# Summarize results\n",
    "print(\"Best: %f using %s\" % (gscv_result.best_score_, gscv_result.best_params_))\n",
    "means = gscv_result.cv_results_['mean_test_score']\n",
    "stds = gscv_result.cv_results_['std_test_score']\n",
    "params = gscv_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))   \n",
    "    \n",
    "print(\"\\nTime Taken: \",start - datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=3, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             multi_strategy=None, n_estimators=500, n_jobs=None, nthread=-1,\n",
       "             num_parallel_tree=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBRegressor</label><div class=\"sk-toggleable__content\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=3, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             multi_strategy=None, n_estimators=500, n_jobs=None, nthread=-1,\n",
       "             num_parallel_tree=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=3, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             multi_strategy=None, n_estimators=500, n_jobs=None, nthread=-1,\n",
       "             num_parallel_tree=None, ...)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create new instance of XGBRegressor with tuned hyperparameters\n",
    "first_xgb = xgb.XGBRegressor(max_depth=3,learning_rate = 0.1,n_estimators=500,nthread=-1)\n",
    "first_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shory\\anaconda3\\envs\\myenv\\lib\\site-packages\\xgboost\\sklearn.py:889: UserWarning: `eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Time taken : 0:00:00.507970\n",
      "\n",
      "Done \n",
      "\n",
      "Evaluating the model with TRAIN data...\n",
      "Evaluating Test data\n",
      "---------------\n",
      "\n",
      "TEST DATA\n",
      "---------------\n",
      "RMSE :  1.1075854286551927\n",
      "MAPE :  38.06150792818722\n"
     ]
    }
   ],
   "source": [
    "train_results, test_results = run_xgboost(first_xgb, x_train, y_train, x_test, y_test)\n",
    "\n",
    "# store the results in models_evaluations dictionaries\n",
    "models_evaluation_train['first_algo'] = train_results\n",
    "models_evaluation_test['first_algo'] = test_results\n",
    "\n",
    "xgb.plot_importance(first_xgb)\n",
    "plt.savefig(\"sample/small/img/feature-importance-xgboost.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Suprise BaselineModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import BaselineOnly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<surprise.prediction_algorithms.baseline_only.BaselineOnly at 0x2b3d507c310>"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate BaselineOnly\n",
    "bsl_options = {'method': 'sgd',\n",
    "               'reg':0.01,\n",
    "               'learning_rate': 0.001,\n",
    "               'n_epochs':120\n",
    "               }\n",
    "bsl_algo = BaselineOnly(bsl_options=bsl_options)\n",
    "bsl_algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n",
      "Estimating biases using sgd...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. time taken : 0:00:00.626486 \n",
      "\n",
      "Evaluating the model with train data..\n",
      "time taken : 0:00:00.441202\n",
      "---------------\n",
      "Train Data\n",
      "---------------\n",
      "RMSE : 0.8828766422875534\n",
      "\n",
      "MAPE : 26.93089077683406\n",
      "\n",
      "adding train results in the dictionary..\n",
      "\n",
      "Evaluating for test data...\n",
      "time taken : 0:00:00.021342\n",
      "---------------\n",
      "Test Data\n",
      "---------------\n",
      "RMSE : 1.0822513101982842\n",
      "\n",
      "MAPE : 36.634142696886315\n",
      "\n",
      "storing the test results in test dictionary...\n",
      "\n",
      "---------------------------------------------\n",
      "Total time taken to run this algorithm : 0:00:01.090014\n",
      "CPU times: total: 203 ms\n",
      "Wall time: 1.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# run this algorithm.., It will return the train and test results..\n",
    "bsl_train_results, bsl_test_results = run_surprise(bsl_algo, trainset, testset, verbose=True)\n",
    "\n",
    "\n",
    "# Just store these error metrics in our models_evaluation datastructure\n",
    "models_evaluation_train['bsl_algo'] = bsl_train_results \n",
    "models_evaluation_test['bsl_algo'] = bsl_test_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost with initial 13 features + Surprise Baseline predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "updating train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>movie</th>\n",
       "      <th>GAvg</th>\n",
       "      <th>sur1</th>\n",
       "      <th>sur2</th>\n",
       "      <th>sur3</th>\n",
       "      <th>sur4</th>\n",
       "      <th>sur5</th>\n",
       "      <th>smr1</th>\n",
       "      <th>smr2</th>\n",
       "      <th>...</th>\n",
       "      <th>smr4</th>\n",
       "      <th>smr5</th>\n",
       "      <th>UAvg</th>\n",
       "      <th>MAvg</th>\n",
       "      <th>rating</th>\n",
       "      <th>bslpr</th>\n",
       "      <th>knn_bsl_u</th>\n",
       "      <th>knn_bsl_m</th>\n",
       "      <th>svd</th>\n",
       "      <th>svdpp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>692</td>\n",
       "      <td>14621</td>\n",
       "      <td>3.61198</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.329095</td>\n",
       "      <td>4</td>\n",
       "      <td>4.299739</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.183244</td>\n",
       "      <td>4.010225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1179</td>\n",
       "      <td>2239</td>\n",
       "      <td>3.61198</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.714286</td>\n",
       "      <td>2.909091</td>\n",
       "      <td>5</td>\n",
       "      <td>3.294208</td>\n",
       "      <td>4.992258</td>\n",
       "      <td>4.889138</td>\n",
       "      <td>3.617130</td>\n",
       "      <td>3.475458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1179</td>\n",
       "      <td>4352</td>\n",
       "      <td>3.61198</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.714286</td>\n",
       "      <td>3.140845</td>\n",
       "      <td>3</td>\n",
       "      <td>3.273046</td>\n",
       "      <td>3.031813</td>\n",
       "      <td>2.952089</td>\n",
       "      <td>3.308964</td>\n",
       "      <td>3.084976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1179</td>\n",
       "      <td>6464</td>\n",
       "      <td>3.61198</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.714286</td>\n",
       "      <td>3.400396</td>\n",
       "      <td>4</td>\n",
       "      <td>3.537082</td>\n",
       "      <td>3.986084</td>\n",
       "      <td>3.716389</td>\n",
       "      <td>3.488253</td>\n",
       "      <td>3.685102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1179</td>\n",
       "      <td>6510</td>\n",
       "      <td>3.61198</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.714286</td>\n",
       "      <td>3.936614</td>\n",
       "      <td>4</td>\n",
       "      <td>4.021263</td>\n",
       "      <td>4.080001</td>\n",
       "      <td>4.014558</td>\n",
       "      <td>3.853108</td>\n",
       "      <td>4.039471</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   user  movie     GAvg  sur1  sur2  sur3  sur4  sur5  smr1  smr2  ...  smr4  \\\n",
       "0   692  14621  3.61198   2.0   4.0   5.0   5.0   5.0   4.0   4.0  ...   4.0   \n",
       "1  1179   2239  3.61198   5.0   3.0   3.0   2.0   2.0   3.0   5.0  ...   3.0   \n",
       "2  1179   4352  3.61198   4.0   3.0   2.0   3.0   3.0   4.0   4.0  ...   4.0   \n",
       "3  1179   6464  3.61198   3.0   5.0   4.0   2.0   4.0   4.0   4.0  ...   5.0   \n",
       "4  1179   6510  3.61198   4.0   4.0   3.0   4.0   3.0   5.0   4.0  ...   3.0   \n",
       "\n",
       "   smr5      UAvg      MAvg  rating     bslpr  knn_bsl_u  knn_bsl_m       svd  \\\n",
       "0   4.0  4.000000  4.329095       4  4.299739   4.000000   4.000000  4.183244   \n",
       "1   4.0  3.714286  2.909091       5  3.294208   4.992258   4.889138  3.617130   \n",
       "2   5.0  3.714286  3.140845       3  3.273046   3.031813   2.952089  3.308964   \n",
       "3   3.0  3.714286  3.400396       4  3.537082   3.986084   3.716389  3.488253   \n",
       "4   4.0  3.714286  3.936614       4  4.021263   4.080001   4.014558  3.853108   \n",
       "\n",
       "      svdpp  \n",
       "0  4.010225  \n",
       "1  3.475458  \n",
       "2  3.084976  \n",
       "3  3.685102  \n",
       "4  4.039471  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add our baseline_predicted value as our feature..\n",
    "reg_train['bslpr'] = models_evaluation_train['bsl_algo']['predictions']\n",
    "reg_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "updating test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>movie</th>\n",
       "      <th>GAvg</th>\n",
       "      <th>sur1</th>\n",
       "      <th>sur2</th>\n",
       "      <th>sur3</th>\n",
       "      <th>sur4</th>\n",
       "      <th>sur5</th>\n",
       "      <th>smr1</th>\n",
       "      <th>smr2</th>\n",
       "      <th>...</th>\n",
       "      <th>smr4</th>\n",
       "      <th>smr5</th>\n",
       "      <th>UAvg</th>\n",
       "      <th>MAvg</th>\n",
       "      <th>rating</th>\n",
       "      <th>bslpr</th>\n",
       "      <th>knn_bsl_u</th>\n",
       "      <th>knn_bsl_m</th>\n",
       "      <th>svd</th>\n",
       "      <th>svdpp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>13072</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>...</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>5</td>\n",
       "      <td>3.61198</td>\n",
       "      <td>3.61198</td>\n",
       "      <td>3.61198</td>\n",
       "      <td>3.61198</td>\n",
       "      <td>3.61198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>126</td>\n",
       "      <td>3418</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>...</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>5</td>\n",
       "      <td>3.61198</td>\n",
       "      <td>3.61198</td>\n",
       "      <td>3.61198</td>\n",
       "      <td>3.61198</td>\n",
       "      <td>3.61198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>268</td>\n",
       "      <td>11740</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>...</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>5</td>\n",
       "      <td>3.61198</td>\n",
       "      <td>3.61198</td>\n",
       "      <td>3.61198</td>\n",
       "      <td>3.61198</td>\n",
       "      <td>3.61198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1809</td>\n",
       "      <td>705</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>...</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>4</td>\n",
       "      <td>3.61198</td>\n",
       "      <td>3.61198</td>\n",
       "      <td>3.61198</td>\n",
       "      <td>3.61198</td>\n",
       "      <td>3.61198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1809</td>\n",
       "      <td>1140</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>...</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3</td>\n",
       "      <td>3.61198</td>\n",
       "      <td>3.61198</td>\n",
       "      <td>3.61198</td>\n",
       "      <td>3.61198</td>\n",
       "      <td>3.61198</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   user  movie      GAvg      sur1      sur2      sur3      sur4      sur5  \\\n",
       "0     7  13072  3.530464  3.530464  3.530464  3.530464  3.530464  3.530464   \n",
       "1   126   3418  3.530464  3.530464  3.530464  3.530464  3.530464  3.530464   \n",
       "2   268  11740  3.530464  3.530464  3.530464  3.530464  3.530464  3.530464   \n",
       "3  1809    705  3.530464  3.530464  3.530464  3.530464  3.530464  3.530464   \n",
       "4  1809   1140  3.530464  3.530464  3.530464  3.530464  3.530464  3.530464   \n",
       "\n",
       "       smr1      smr2  ...      smr4      smr5      UAvg      MAvg  rating  \\\n",
       "0  3.530464  3.530464  ...  3.530464  3.530464  3.530464  3.530464       5   \n",
       "1  3.530464  3.530464  ...  3.530464  3.530464  3.530464  3.530464       5   \n",
       "2  3.530464  3.530464  ...  3.530464  3.530464  3.530464  3.530464       5   \n",
       "3  3.530464  3.530464  ...  3.530464  3.530464  3.530464  3.530464       4   \n",
       "4  3.530464  3.530464  ...  3.530464  3.530464  3.530464  3.530464       3   \n",
       "\n",
       "     bslpr  knn_bsl_u  knn_bsl_m      svd    svdpp  \n",
       "0  3.61198    3.61198    3.61198  3.61198  3.61198  \n",
       "1  3.61198    3.61198    3.61198  3.61198  3.61198  \n",
       "2  3.61198    3.61198    3.61198  3.61198  3.61198  \n",
       "3  3.61198    3.61198    3.61198  3.61198  3.61198  \n",
       "4  3.61198    3.61198    3.61198  3.61198  3.61198  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add that baseline predicted ratings with Surprise to the test data as well\n",
    "reg_test_df['bslpr']  = models_evaluation_test['bsl_algo']['predictions']\n",
    "reg_test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare train data\n",
    "x_train = reg_train.drop(['user', 'movie','rating'], axis=1)\n",
    "y_train = reg_train['rating']\n",
    "\n",
    "# Prepare Test data\n",
    "x_test = reg_test_df.drop(['user','movie','rating'], axis=1)\n",
    "y_test = reg_test_df['rating']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running XGBRegressor, we will tune hyperparameter using gridsearch cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 63 candidates, totalling 315 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: -0.011012 using {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 1300}\n",
      "\n",
      "-1.012954 (0.030909) with: {'learning_rate': 0.001, 'max_depth': 1, 'n_estimators': 100}\n",
      "-0.794077 (0.026827) with: {'learning_rate': 0.001, 'max_depth': 1, 'n_estimators': 300}\n",
      "-0.639678 (0.022093) with: {'learning_rate': 0.001, 'max_depth': 1, 'n_estimators': 500}\n",
      "-0.519820 (0.018441) with: {'learning_rate': 0.001, 'max_depth': 1, 'n_estimators': 700}\n",
      "-0.424710 (0.015506) with: {'learning_rate': 0.001, 'max_depth': 1, 'n_estimators': 900}\n",
      "-0.349214 (0.013238) with: {'learning_rate': 0.001, 'max_depth': 1, 'n_estimators': 1100}\n",
      "-0.289241 (0.011460) with: {'learning_rate': 0.001, 'max_depth': 1, 'n_estimators': 1300}\n",
      "-0.961326 (0.028048) with: {'learning_rate': 0.001, 'max_depth': 2, 'n_estimators': 100}\n",
      "-0.665545 (0.020358) with: {'learning_rate': 0.001, 'max_depth': 2, 'n_estimators': 300}\n",
      "-0.466127 (0.015410) with: {'learning_rate': 0.001, 'max_depth': 2, 'n_estimators': 500}\n",
      "-0.329994 (0.012028) with: {'learning_rate': 0.001, 'max_depth': 2, 'n_estimators': 700}\n",
      "-0.236688 (0.009644) with: {'learning_rate': 0.001, 'max_depth': 2, 'n_estimators': 900}\n",
      "-0.172644 (0.007924) with: {'learning_rate': 0.001, 'max_depth': 2, 'n_estimators': 1100}\n",
      "-0.128615 (0.006687) with: {'learning_rate': 0.001, 'max_depth': 2, 'n_estimators': 1300}\n",
      "-0.955324 (0.027834) with: {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 100}\n",
      "-0.651150 (0.019687) with: {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 300}\n",
      "-0.446866 (0.014269) with: {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 500}\n",
      "-0.309668 (0.010705) with: {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 700}\n",
      "-0.217382 (0.008315) with: {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 900}\n",
      "-0.155278 (0.006725) with: {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 1100}\n",
      "-0.113440 (0.005671) with: {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 1300}\n",
      "-0.383049 (0.014172) with: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 100}\n",
      "-0.078265 (0.004879) with: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 300}\n",
      "-0.036266 (0.003678) with: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 500}\n",
      "-0.030063 (0.003595) with: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 700}\n",
      "-0.028340 (0.003494) with: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 900}\n",
      "-0.027188 (0.003385) with: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 1100}\n",
      "-0.026376 (0.003292) with: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 1300}\n",
      "-0.200257 (0.008675) with: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 100}\n",
      "-0.033275 (0.003739) with: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 300}\n",
      "-0.025851 (0.003412) with: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 500}\n",
      "-0.023413 (0.003174) with: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 700}\n",
      "-0.022317 (0.003059) with: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 900}\n",
      "-0.021800 (0.003008) with: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 1100}\n",
      "-0.021519 (0.002987) with: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 1300}\n",
      "-0.181916 (0.007441) with: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 100}\n",
      "-0.027456 (0.003570) with: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 300}\n",
      "-0.021237 (0.003207) with: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 500}\n",
      "-0.019729 (0.002942) with: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 700}\n",
      "-0.018710 (0.002900) with: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 900}\n",
      "-0.018111 (0.002887) with: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 1100}\n",
      "-0.017661 (0.002835) with: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 1300}\n",
      "-0.027671 (0.003443) with: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 100}\n",
      "-0.024279 (0.002998) with: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 300}\n",
      "-0.023845 (0.002899) with: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 500}\n",
      "-0.023638 (0.002864) with: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 700}\n",
      "-0.023478 (0.002841) with: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 900}\n",
      "-0.023356 (0.002823) with: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 1100}\n",
      "-0.023254 (0.002809) with: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 1300}\n",
      "-0.022064 (0.002970) with: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 100}\n",
      "-0.019838 (0.002916) with: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 300}\n",
      "-0.018608 (0.002863) with: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 500}\n",
      "-0.017675 (0.002860) with: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 700}\n",
      "-0.016954 (0.002821) with: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 900}\n",
      "-0.016406 (0.002756) with: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 1100}\n",
      "-0.015916 (0.002709) with: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 1300}\n",
      "-0.018455 (0.002904) with: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100}\n",
      "-0.013926 (0.002567) with: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 300}\n",
      "-0.012567 (0.002427) with: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 500}\n",
      "-0.011849 (0.002417) with: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 700}\n",
      "-0.011488 (0.002393) with: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 900}\n",
      "-0.011189 (0.002407) with: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 1100}\n",
      "-0.011012 (0.002439) with: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 1300}\n",
      "\n",
      "Time Taken:  0:01:49.100366\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "# Initialize Our first XGBoost model\n",
    "xgb = xgb.XGBRegressor(nthread=-1)\n",
    "\n",
    "# Perform cross validation \n",
    "gscv = GridSearchCV(xgb,\n",
    "                    param_grid = parameters,\n",
    "                    scoring=\"neg_mean_squared_error\",\n",
    "                    cv = TimeSeriesSplit(n_splits=5),\n",
    "                    n_jobs = -1,\n",
    "                    verbose = 1)\n",
    "gscv_result = gscv.fit(x_train, y_train)\n",
    "\n",
    "# Summarize results\n",
    "print(\"Best: %f using %s\" % (gscv_result.best_score_, gscv_result.best_params_))\n",
    "print()\n",
    "means = gscv_result.cv_results_['mean_test_score']\n",
    "stds = gscv_result.cv_results_['std_test_score']\n",
    "params = gscv_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))  \n",
    "\n",
    "print(\"\\nTime Taken: \",datetime.now() -start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=3, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             multi_strategy=None, n_estimators=1300, n_jobs=None, nthread=-1,\n",
       "             num_parallel_tree=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBRegressor</label><div class=\"sk-toggleable__content\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=3, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             multi_strategy=None, n_estimators=1300, n_jobs=None, nthread=-1,\n",
       "             num_parallel_tree=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=3, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             multi_strategy=None, n_estimators=1300, n_jobs=None, nthread=-1,\n",
       "             num_parallel_tree=None, ...)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create new instance of XGBRegressor with tuned hyperparameters\n",
    "xgb_bsl = xgb.XGBRegressor(max_depth=3,learning_rate = 0.01,n_estimators=1300,nthread=-1)\n",
    "xgb_bsl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shory\\anaconda3\\envs\\myenv\\lib\\site-packages\\xgboost\\sklearn.py:889: UserWarning: `eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Time taken : 0:00:01.690594\n",
      "\n",
      "Done \n",
      "\n",
      "Evaluating the model with TRAIN data...\n",
      "Evaluating Test data\n",
      "---------------\n",
      "\n",
      "TEST DATA\n",
      "---------------\n",
      "RMSE :  1.1234619836629212\n",
      "MAPE :  38.588619195862314\n"
     ]
    }
   ],
   "source": [
    "# Run XGBRegressor\n",
    "train_results, test_results = run_xgboost(xgb_bsl, x_train, y_train, x_test, y_test)\n",
    "\n",
    "# store the results in models_evaluations dictionaries\n",
    "models_evaluation_train['xgb_bsl'] = train_results\n",
    "models_evaluation_test['xgb_bsl'] = test_results\n",
    "\n",
    "xgb.plot_importance(xgb_bsl)\n",
    "plt.savefig(\"sample/small/img/feature-importance-xgboost+surprise.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Surprise KNNBaseline predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import KNNBaseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprise KNNBaseline with user user similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n",
      "Estimating biases using sgd...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Done. time taken : 0:00:05.905061 \n",
      "\n",
      "Evaluating the model with train data..\n",
      "time taken : 0:00:43.490439\n",
      "---------------\n",
      "Train Data\n",
      "---------------\n",
      "RMSE : 0.2941885927227935\n",
      "\n",
      "MAPE : 7.815052348749038\n",
      "\n",
      "adding train results in the dictionary..\n",
      "\n",
      "Evaluating for test data...\n",
      "time taken : 0:00:00.014664\n",
      "---------------\n",
      "Test Data\n",
      "---------------\n",
      "RMSE : 1.0822531134111517\n",
      "\n",
      "MAPE : 36.63794599842355\n",
      "\n",
      "storing the test results in test dictionary...\n",
      "\n",
      "---------------------------------------------\n",
      "Total time taken to run this algorithm : 0:00:49.410164\n"
     ]
    }
   ],
   "source": [
    "# we specify , how to compute similarities and what to consider with sim_options to our algorithm\n",
    "sim_options = {'user_based' : True,\n",
    "               'name': 'pearson_baseline',\n",
    "               'shrinkage': 100,\n",
    "               'min_support': 2\n",
    "              } \n",
    "# we keep other parameters like regularization parameter and learning_rate as default values.\n",
    "bsl_options = {'method': 'sgd'} \n",
    "\n",
    "knn_bsl_u = KNNBaseline(k=40, sim_options = sim_options, bsl_options = bsl_options)\n",
    "knn_bsl_u_train_results, knn_bsl_u_test_results = run_surprise(knn_bsl_u, trainset, testset, verbose=True)\n",
    "\n",
    "# Just store these error metrics in our models_evaluation datastructure\n",
    "models_evaluation_train['knn_bsl_u'] = knn_bsl_u_train_results \n",
    "models_evaluation_test['knn_bsl_u'] = knn_bsl_u_test_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprise KNNBaseline with movie movie similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n",
      "Estimating biases using sgd...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Done. time taken : 0:00:00.217665 \n",
      "\n",
      "Evaluating the model with train data..\n",
      "time taken : 0:00:01.896365\n",
      "---------------\n",
      "Train Data\n",
      "---------------\n",
      "RMSE : 0.26140322547389855\n",
      "\n",
      "MAPE : 6.516388525578446\n",
      "\n",
      "adding train results in the dictionary..\n",
      "\n",
      "Evaluating for test data...\n",
      "time taken : 0:00:00.034854\n",
      "---------------\n",
      "Test Data\n",
      "---------------\n",
      "RMSE : 1.0821949464872673\n",
      "\n",
      "MAPE : 36.63194849597806\n",
      "\n",
      "storing the test results in test dictionary...\n",
      "\n",
      "---------------------------------------------\n",
      "Total time taken to run this algorithm : 0:00:02.148884\n"
     ]
    }
   ],
   "source": [
    "# I specified , how to compute similarities and what to consider with sim_options to our algorithm\n",
    "\n",
    "# 'user_based' : Fals => this considers the similarities of movies instead of users\n",
    "\n",
    "sim_options = {'user_based' : False,\n",
    "               'name': 'pearson_baseline',\n",
    "               'shrinkage': 100,\n",
    "               'min_support': 2\n",
    "              } \n",
    "# we keep other parameters like regularization parameter and learning_rate as default values.\n",
    "bsl_options = {'method': 'sgd'}\n",
    "\n",
    "\n",
    "knn_bsl_m = KNNBaseline(k=40, sim_options = sim_options, bsl_options = bsl_options)\n",
    "\n",
    "knn_bsl_m_train_results, knn_bsl_m_test_results = run_surprise(knn_bsl_m, trainset, testset, verbose=True)\n",
    "\n",
    "# Just store these error metrics in our models_evaluation datastructure\n",
    "models_evaluation_train['knn_bsl_m'] = knn_bsl_m_train_results \n",
    "models_evaluation_test['knn_bsl_m'] = knn_bsl_m_test_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost with initial 13 features + Surprise Baseline predictor + KNNBaseline predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First I will run XGBoost with predictions from both KNN's ( that uses User_User and Item_Item similarities along with our previous features)\n",
    "- Then I will run XGBoost with just predictions form both knn models and preditions from our baseline model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preparing train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>movie</th>\n",
       "      <th>GAvg</th>\n",
       "      <th>sur1</th>\n",
       "      <th>sur2</th>\n",
       "      <th>sur3</th>\n",
       "      <th>sur4</th>\n",
       "      <th>sur5</th>\n",
       "      <th>smr1</th>\n",
       "      <th>smr2</th>\n",
       "      <th>smr3</th>\n",
       "      <th>smr4</th>\n",
       "      <th>smr5</th>\n",
       "      <th>UAvg</th>\n",
       "      <th>MAvg</th>\n",
       "      <th>rating</th>\n",
       "      <th>bslpr</th>\n",
       "      <th>knn_bsl_u</th>\n",
       "      <th>knn_bsl_m</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>692</td>\n",
       "      <td>14621</td>\n",
       "      <td>3.61198</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.329095</td>\n",
       "      <td>4</td>\n",
       "      <td>4.299739</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1179</td>\n",
       "      <td>2239</td>\n",
       "      <td>3.61198</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.714286</td>\n",
       "      <td>2.909091</td>\n",
       "      <td>5</td>\n",
       "      <td>3.294208</td>\n",
       "      <td>4.992258</td>\n",
       "      <td>4.889138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1179</td>\n",
       "      <td>4352</td>\n",
       "      <td>3.61198</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.714286</td>\n",
       "      <td>3.140845</td>\n",
       "      <td>3</td>\n",
       "      <td>3.273046</td>\n",
       "      <td>3.031813</td>\n",
       "      <td>2.952089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1179</td>\n",
       "      <td>6464</td>\n",
       "      <td>3.61198</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.714286</td>\n",
       "      <td>3.400396</td>\n",
       "      <td>4</td>\n",
       "      <td>3.537082</td>\n",
       "      <td>3.986084</td>\n",
       "      <td>3.716389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1179</td>\n",
       "      <td>6510</td>\n",
       "      <td>3.61198</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.714286</td>\n",
       "      <td>3.936614</td>\n",
       "      <td>4</td>\n",
       "      <td>4.021263</td>\n",
       "      <td>4.080001</td>\n",
       "      <td>4.014558</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user  movie     GAvg  sur1  sur2  sur3  sur4  sur5  smr1  smr2  smr3  smr4  \\\n",
       "0   692  14621  3.61198   2.0   4.0   5.0   5.0   5.0   4.0   4.0   4.0   4.0   \n",
       "1  1179   2239  3.61198   5.0   3.0   3.0   2.0   2.0   3.0   5.0   4.0   3.0   \n",
       "2  1179   4352  3.61198   4.0   3.0   2.0   3.0   3.0   4.0   4.0   3.0   4.0   \n",
       "3  1179   6464  3.61198   3.0   5.0   4.0   2.0   4.0   4.0   4.0   2.0   5.0   \n",
       "4  1179   6510  3.61198   4.0   4.0   3.0   4.0   3.0   5.0   4.0   4.0   3.0   \n",
       "\n",
       "   smr5      UAvg      MAvg  rating     bslpr  knn_bsl_u  knn_bsl_m  \n",
       "0   4.0  4.000000  4.329095       4  4.299739   4.000000   4.000000  \n",
       "1   4.0  3.714286  2.909091       5  3.294208   4.992258   4.889138  \n",
       "2   5.0  3.714286  3.140845       3  3.273046   3.031813   2.952089  \n",
       "3   3.0  3.714286  3.400396       4  3.537082   3.986084   3.716389  \n",
       "4   4.0  3.714286  3.936614       4  4.021263   4.080001   4.014558  "
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add the predicted values from both knns to this dataframe\n",
    "reg_train['knn_bsl_u'] = models_evaluation_train['knn_bsl_u']['predictions']\n",
    "reg_train['knn_bsl_m'] = models_evaluation_train['knn_bsl_m']['predictions']\n",
    "\n",
    "reg_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preparing test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>movie</th>\n",
       "      <th>GAvg</th>\n",
       "      <th>sur1</th>\n",
       "      <th>sur2</th>\n",
       "      <th>sur3</th>\n",
       "      <th>sur4</th>\n",
       "      <th>sur5</th>\n",
       "      <th>smr1</th>\n",
       "      <th>smr2</th>\n",
       "      <th>smr3</th>\n",
       "      <th>smr4</th>\n",
       "      <th>smr5</th>\n",
       "      <th>UAvg</th>\n",
       "      <th>MAvg</th>\n",
       "      <th>rating</th>\n",
       "      <th>bslpr</th>\n",
       "      <th>knn_bsl_u</th>\n",
       "      <th>knn_bsl_m</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>13072</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>5</td>\n",
       "      <td>3.61198</td>\n",
       "      <td>3.61198</td>\n",
       "      <td>3.61198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>126</td>\n",
       "      <td>3418</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>5</td>\n",
       "      <td>3.61198</td>\n",
       "      <td>3.61198</td>\n",
       "      <td>3.61198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>268</td>\n",
       "      <td>11740</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>5</td>\n",
       "      <td>3.61198</td>\n",
       "      <td>3.61198</td>\n",
       "      <td>3.61198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1809</td>\n",
       "      <td>705</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>4</td>\n",
       "      <td>3.61198</td>\n",
       "      <td>3.61198</td>\n",
       "      <td>3.61198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1809</td>\n",
       "      <td>1140</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3</td>\n",
       "      <td>3.61198</td>\n",
       "      <td>3.61198</td>\n",
       "      <td>3.61198</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user  movie      GAvg      sur1      sur2      sur3      sur4      sur5  \\\n",
       "0     7  13072  3.530464  3.530464  3.530464  3.530464  3.530464  3.530464   \n",
       "1   126   3418  3.530464  3.530464  3.530464  3.530464  3.530464  3.530464   \n",
       "2   268  11740  3.530464  3.530464  3.530464  3.530464  3.530464  3.530464   \n",
       "3  1809    705  3.530464  3.530464  3.530464  3.530464  3.530464  3.530464   \n",
       "4  1809   1140  3.530464  3.530464  3.530464  3.530464  3.530464  3.530464   \n",
       "\n",
       "       smr1      smr2      smr3      smr4      smr5      UAvg      MAvg  \\\n",
       "0  3.530464  3.530464  3.530464  3.530464  3.530464  3.530464  3.530464   \n",
       "1  3.530464  3.530464  3.530464  3.530464  3.530464  3.530464  3.530464   \n",
       "2  3.530464  3.530464  3.530464  3.530464  3.530464  3.530464  3.530464   \n",
       "3  3.530464  3.530464  3.530464  3.530464  3.530464  3.530464  3.530464   \n",
       "4  3.530464  3.530464  3.530464  3.530464  3.530464  3.530464  3.530464   \n",
       "\n",
       "   rating    bslpr  knn_bsl_u  knn_bsl_m  \n",
       "0       5  3.61198    3.61198    3.61198  \n",
       "1       5  3.61198    3.61198    3.61198  \n",
       "2       5  3.61198    3.61198    3.61198  \n",
       "3       4  3.61198    3.61198    3.61198  \n",
       "4       3  3.61198    3.61198    3.61198  "
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_test_df['knn_bsl_u'] = models_evaluation_test['knn_bsl_u']['predictions']\n",
    "reg_test_df['knn_bsl_m'] = models_evaluation_test['knn_bsl_m']['predictions']\n",
    "\n",
    "reg_test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the train data....\n",
    "x_train = reg_train.drop(['user', 'movie', 'rating'], axis=1)\n",
    "y_train = reg_train['rating']\n",
    "\n",
    "# prepare the train data....\n",
    "x_test = reg_test_df.drop(['user','movie','rating'], axis=1)\n",
    "y_test = reg_test_df['rating']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running XGBRegressor, we will tune hyperparameter using gridsearch cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 63 candidates, totalling 315 fits\n",
      "Best: -0.011088 using {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 1300}\n",
      "\n",
      "-1.012954 (0.030909) with: {'learning_rate': 0.001, 'max_depth': 1, 'n_estimators': 100}\n",
      "-0.794077 (0.026827) with: {'learning_rate': 0.001, 'max_depth': 1, 'n_estimators': 300}\n",
      "-0.639678 (0.022093) with: {'learning_rate': 0.001, 'max_depth': 1, 'n_estimators': 500}\n",
      "-0.519820 (0.018441) with: {'learning_rate': 0.001, 'max_depth': 1, 'n_estimators': 700}\n",
      "-0.424710 (0.015506) with: {'learning_rate': 0.001, 'max_depth': 1, 'n_estimators': 900}\n",
      "-0.349214 (0.013238) with: {'learning_rate': 0.001, 'max_depth': 1, 'n_estimators': 1100}\n",
      "-0.289241 (0.011460) with: {'learning_rate': 0.001, 'max_depth': 1, 'n_estimators': 1300}\n",
      "-0.961326 (0.028048) with: {'learning_rate': 0.001, 'max_depth': 2, 'n_estimators': 100}\n",
      "-0.665545 (0.020358) with: {'learning_rate': 0.001, 'max_depth': 2, 'n_estimators': 300}\n",
      "-0.466127 (0.015410) with: {'learning_rate': 0.001, 'max_depth': 2, 'n_estimators': 500}\n",
      "-0.329994 (0.012028) with: {'learning_rate': 0.001, 'max_depth': 2, 'n_estimators': 700}\n",
      "-0.236688 (0.009644) with: {'learning_rate': 0.001, 'max_depth': 2, 'n_estimators': 900}\n",
      "-0.172644 (0.007924) with: {'learning_rate': 0.001, 'max_depth': 2, 'n_estimators': 1100}\n",
      "-0.128615 (0.006687) with: {'learning_rate': 0.001, 'max_depth': 2, 'n_estimators': 1300}\n",
      "-0.955324 (0.027834) with: {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 100}\n",
      "-0.651150 (0.019687) with: {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 300}\n",
      "-0.446866 (0.014269) with: {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 500}\n",
      "-0.309668 (0.010705) with: {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 700}\n",
      "-0.217382 (0.008315) with: {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 900}\n",
      "-0.155278 (0.006725) with: {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 1100}\n",
      "-0.113440 (0.005671) with: {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 1300}\n",
      "-0.383049 (0.014172) with: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 100}\n",
      "-0.078265 (0.004879) with: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 300}\n",
      "-0.036266 (0.003678) with: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 500}\n",
      "-0.030063 (0.003595) with: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 700}\n",
      "-0.028340 (0.003494) with: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 900}\n",
      "-0.027188 (0.003385) with: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 1100}\n",
      "-0.026376 (0.003292) with: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 1300}\n",
      "-0.200257 (0.008675) with: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 100}\n",
      "-0.033275 (0.003739) with: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 300}\n",
      "-0.025851 (0.003412) with: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 500}\n",
      "-0.023413 (0.003174) with: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 700}\n",
      "-0.022305 (0.003037) with: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 900}\n",
      "-0.021788 (0.002979) with: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 1100}\n",
      "-0.021509 (0.002967) with: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 1300}\n",
      "-0.181916 (0.007441) with: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 100}\n",
      "-0.027458 (0.003571) with: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 300}\n",
      "-0.021267 (0.003177) with: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 500}\n",
      "-0.019771 (0.002885) with: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 700}\n",
      "-0.018805 (0.002830) with: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 900}\n",
      "-0.018190 (0.002840) with: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 1100}\n",
      "-0.017739 (0.002808) with: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 1300}\n",
      "-0.027671 (0.003443) with: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 100}\n",
      "-0.024273 (0.002991) with: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 300}\n",
      "-0.023863 (0.002882) with: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 500}\n",
      "-0.023672 (0.002855) with: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 700}\n",
      "-0.023533 (0.002835) with: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 900}\n",
      "-0.023427 (0.002819) with: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 1100}\n",
      "-0.023343 (0.002809) with: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 1300}\n",
      "-0.022053 (0.002950) with: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 100}\n",
      "-0.019852 (0.002942) with: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 300}\n",
      "-0.018699 (0.002970) with: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 500}\n",
      "-0.017767 (0.002888) with: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 700}\n",
      "-0.017088 (0.002835) with: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 900}\n",
      "-0.016538 (0.002781) with: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 1100}\n",
      "-0.016065 (0.002689) with: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 1300}\n",
      "-0.018455 (0.002812) with: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100}\n",
      "-0.013872 (0.002728) with: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 300}\n",
      "-0.012500 (0.002381) with: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 500}\n",
      "-0.011819 (0.002280) with: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 700}\n",
      "-0.011461 (0.002307) with: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 900}\n",
      "-0.011257 (0.002299) with: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 1100}\n",
      "-0.011088 (0.002337) with: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 1300}\n",
      "\n",
      "Time Taken:  0:01:45.914724\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "# Initialize Our first XGBoost model\n",
    "model = xgb.XGBRegressor(nthread=-1)\n",
    "\n",
    "# Perform cross validation \n",
    "gscv = GridSearchCV(model,\n",
    "                    param_grid = parameters,\n",
    "                    scoring=\"neg_mean_squared_error\",\n",
    "                    cv = TimeSeriesSplit(n_splits=5),\n",
    "                    n_jobs = -1,\n",
    "                    verbose = 1)\n",
    "gscv_result = gscv.fit(x_train, y_train)\n",
    "\n",
    "# Summarize results\n",
    "print(\"Best: %f using %s\" % (gscv_result.best_score_, gscv_result.best_params_))\n",
    "print()\n",
    "means = gscv_result.cv_results_['mean_test_score']\n",
    "stds = gscv_result.cv_results_['std_test_score']\n",
    "params = gscv_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))  \n",
    "\n",
    "print(\"\\nTime Taken: \",datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=3, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             multi_strategy=None, n_estimators=300, n_jobs=None, nthread=-1,\n",
       "             num_parallel_tree=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBRegressor</label><div class=\"sk-toggleable__content\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=3, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             multi_strategy=None, n_estimators=300, n_jobs=None, nthread=-1,\n",
       "             num_parallel_tree=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=3, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             multi_strategy=None, n_estimators=300, n_jobs=None, nthread=-1,\n",
       "             num_parallel_tree=None, ...)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create new instance of XGBRegressor with tuned hyperparameters\n",
    "xgb_knn_bsl = xgb.XGBRegressor(max_depth=3,learning_rate = 0.1,n_estimators=300,nthread=-1)\n",
    "xgb_knn_bsl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shory\\anaconda3\\envs\\myenv\\lib\\site-packages\\xgboost\\sklearn.py:889: UserWarning: `eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Time taken : 0:00:00.468740\n",
      "\n",
      "Done \n",
      "\n",
      "Evaluating the model with TRAIN data...\n",
      "Evaluating Test data\n",
      "---------------\n",
      "\n",
      "TEST DATA\n",
      "---------------\n",
      "RMSE :  1.11901091975606\n",
      "MAPE :  38.45632696116646\n"
     ]
    }
   ],
   "source": [
    "train_results, test_results = run_xgboost(xgb_knn_bsl, x_train, y_train, x_test, y_test)\n",
    "\n",
    "# store the results in models_evaluations dictionaries\n",
    "models_evaluation_train['xgb_knn_bsl'] = train_results\n",
    "models_evaluation_test['xgb_knn_bsl'] = test_results\n",
    "\n",
    "\n",
    "xgb.plot_importance(xgb_knn_bsl)\n",
    "plt.savefig(\"sample/small/img/feature-importance-xgb+bsl+knn.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Factorization Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n",
      "Processing epoch 0\n",
      "Processing epoch 1\n",
      "Processing epoch 2\n",
      "Processing epoch 3\n",
      "Processing epoch 4\n",
      "Processing epoch 5\n",
      "Processing epoch 6\n",
      "Processing epoch 7\n",
      "Processing epoch 8\n",
      "Processing epoch 9\n",
      "Processing epoch 10\n",
      "Processing epoch 11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing epoch 12\n",
      "Processing epoch 13\n",
      "Processing epoch 14\n",
      "Processing epoch 15\n",
      "Processing epoch 16\n",
      "Processing epoch 17\n",
      "Processing epoch 18\n",
      "Processing epoch 19\n",
      "Done. time taken : 0:00:00.382676 \n",
      "\n",
      "Evaluating the model with train data..\n",
      "time taken : 0:00:00.450654\n",
      "---------------\n",
      "Train Data\n",
      "---------------\n",
      "RMSE : 0.652056585294612\n",
      "\n",
      "MAPE : 19.474744533179717\n",
      "\n",
      "adding train results in the dictionary..\n",
      "\n",
      "Evaluating for test data...\n",
      "time taken : 0:00:00.018005\n",
      "---------------\n",
      "Test Data\n",
      "---------------\n",
      "RMSE : 1.0821912114098393\n",
      "\n",
      "MAPE : 36.63640232612494\n",
      "\n",
      "storing the test results in test dictionary...\n",
      "\n",
      "---------------------------------------------\n",
      "Total time taken to run this algorithm : 0:00:00.864688\n"
     ]
    }
   ],
   "source": [
    "# initiallize the model\n",
    "svd = SVD(n_factors=100, biased=True, random_state=30, verbose=True)\n",
    "svd_train_results, svd_test_results = run_surprise(svd, trainset, testset, verbose=True)\n",
    "\n",
    "# Just store these error metrics in our models_evaluation datastructure\n",
    "models_evaluation_train['svd'] = svd_train_results \n",
    "models_evaluation_test['svd'] = svd_test_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD Matrix Factorization with implicit feedback from user ( user rated movies )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import SVDpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n",
      " processing epoch 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing epoch 1\n",
      " processing epoch 2\n",
      " processing epoch 3\n",
      " processing epoch 4\n",
      " processing epoch 5\n",
      " processing epoch 6\n",
      " processing epoch 7\n",
      " processing epoch 8\n",
      " processing epoch 9\n",
      " processing epoch 10\n",
      " processing epoch 11\n",
      " processing epoch 12\n",
      " processing epoch 13\n",
      " processing epoch 14\n",
      " processing epoch 15\n",
      " processing epoch 16\n",
      " processing epoch 17\n",
      " processing epoch 18\n",
      " processing epoch 19\n",
      "Done. time taken : 0:00:04.654456 \n",
      "\n",
      "Evaluating the model with train data..\n",
      "time taken : 0:00:01.861405\n",
      "---------------\n",
      "Train Data\n",
      "---------------\n",
      "RMSE : 0.5906323996698382\n",
      "\n",
      "MAPE : 17.159395676027554\n",
      "\n",
      "adding train results in the dictionary..\n",
      "\n",
      "Evaluating for test data...\n",
      "time taken : 0:00:00.016660\n",
      "---------------\n",
      "Test Data\n",
      "---------------\n",
      "RMSE : 1.0822543401393163\n",
      "\n",
      "MAPE : 36.641062221542555\n",
      "\n",
      "storing the test results in test dictionary...\n",
      "\n",
      "---------------------------------------------\n",
      "Total time taken to run this algorithm : 0:00:06.532521\n"
     ]
    }
   ],
   "source": [
    "# initiallize the model\n",
    "svdpp = SVDpp(n_factors=50, random_state=30, verbose=True)\n",
    "svdpp_train_results, svdpp_test_results = run_surprise(svdpp, trainset, testset, verbose=True)\n",
    "\n",
    "# Just store these error metrics in our models_evaluation datastructure\n",
    "models_evaluation_train['svdpp'] = svdpp_train_results \n",
    "models_evaluation_test['svdpp'] = svdpp_test_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XgBoost with 13 features + Surprise Baseline + Surprise KNNbaseline + MF Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>movie</th>\n",
       "      <th>GAvg</th>\n",
       "      <th>sur1</th>\n",
       "      <th>sur2</th>\n",
       "      <th>sur3</th>\n",
       "      <th>sur4</th>\n",
       "      <th>sur5</th>\n",
       "      <th>smr1</th>\n",
       "      <th>smr2</th>\n",
       "      <th>...</th>\n",
       "      <th>smr4</th>\n",
       "      <th>smr5</th>\n",
       "      <th>UAvg</th>\n",
       "      <th>MAvg</th>\n",
       "      <th>rating</th>\n",
       "      <th>bslpr</th>\n",
       "      <th>knn_bsl_u</th>\n",
       "      <th>knn_bsl_m</th>\n",
       "      <th>svd</th>\n",
       "      <th>svdpp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>692</td>\n",
       "      <td>14621</td>\n",
       "      <td>3.61198</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.329095</td>\n",
       "      <td>4</td>\n",
       "      <td>4.299739</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.183244</td>\n",
       "      <td>4.010225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1179</td>\n",
       "      <td>2239</td>\n",
       "      <td>3.61198</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.714286</td>\n",
       "      <td>2.909091</td>\n",
       "      <td>5</td>\n",
       "      <td>3.294208</td>\n",
       "      <td>4.992258</td>\n",
       "      <td>4.889138</td>\n",
       "      <td>3.617130</td>\n",
       "      <td>3.475458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1179</td>\n",
       "      <td>4352</td>\n",
       "      <td>3.61198</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.714286</td>\n",
       "      <td>3.140845</td>\n",
       "      <td>3</td>\n",
       "      <td>3.273046</td>\n",
       "      <td>3.031813</td>\n",
       "      <td>2.952089</td>\n",
       "      <td>3.308964</td>\n",
       "      <td>3.084976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1179</td>\n",
       "      <td>6464</td>\n",
       "      <td>3.61198</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.714286</td>\n",
       "      <td>3.400396</td>\n",
       "      <td>4</td>\n",
       "      <td>3.537082</td>\n",
       "      <td>3.986084</td>\n",
       "      <td>3.716389</td>\n",
       "      <td>3.488253</td>\n",
       "      <td>3.685102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1179</td>\n",
       "      <td>6510</td>\n",
       "      <td>3.61198</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.714286</td>\n",
       "      <td>3.936614</td>\n",
       "      <td>4</td>\n",
       "      <td>4.021263</td>\n",
       "      <td>4.080001</td>\n",
       "      <td>4.014558</td>\n",
       "      <td>3.853108</td>\n",
       "      <td>4.039471</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   user  movie     GAvg  sur1  sur2  sur3  sur4  sur5  smr1  smr2  ...  smr4  \\\n",
       "0   692  14621  3.61198   2.0   4.0   5.0   5.0   5.0   4.0   4.0  ...   4.0   \n",
       "1  1179   2239  3.61198   5.0   3.0   3.0   2.0   2.0   3.0   5.0  ...   3.0   \n",
       "2  1179   4352  3.61198   4.0   3.0   2.0   3.0   3.0   4.0   4.0  ...   4.0   \n",
       "3  1179   6464  3.61198   3.0   5.0   4.0   2.0   4.0   4.0   4.0  ...   5.0   \n",
       "4  1179   6510  3.61198   4.0   4.0   3.0   4.0   3.0   5.0   4.0  ...   3.0   \n",
       "\n",
       "   smr5      UAvg      MAvg  rating     bslpr  knn_bsl_u  knn_bsl_m       svd  \\\n",
       "0   4.0  4.000000  4.329095       4  4.299739   4.000000   4.000000  4.183244   \n",
       "1   4.0  3.714286  2.909091       5  3.294208   4.992258   4.889138  3.617130   \n",
       "2   5.0  3.714286  3.140845       3  3.273046   3.031813   2.952089  3.308964   \n",
       "3   3.0  3.714286  3.400396       4  3.537082   3.986084   3.716389  3.488253   \n",
       "4   4.0  3.714286  3.936614       4  4.021263   4.080001   4.014558  3.853108   \n",
       "\n",
       "      svdpp  \n",
       "0  4.010225  \n",
       "1  3.475458  \n",
       "2  3.084976  \n",
       "3  3.685102  \n",
       "4  4.039471  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add the predicted values from both knns to this dataframe\n",
    "reg_train['svd'] = models_evaluation_train['svd']['predictions']\n",
    "reg_train['svdpp'] = models_evaluation_train['svdpp']['predictions']\n",
    "\n",
    "reg_train.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>movie</th>\n",
       "      <th>GAvg</th>\n",
       "      <th>sur1</th>\n",
       "      <th>sur2</th>\n",
       "      <th>sur3</th>\n",
       "      <th>sur4</th>\n",
       "      <th>sur5</th>\n",
       "      <th>smr1</th>\n",
       "      <th>smr2</th>\n",
       "      <th>...</th>\n",
       "      <th>smr4</th>\n",
       "      <th>smr5</th>\n",
       "      <th>UAvg</th>\n",
       "      <th>MAvg</th>\n",
       "      <th>rating</th>\n",
       "      <th>bslpr</th>\n",
       "      <th>knn_bsl_u</th>\n",
       "      <th>knn_bsl_m</th>\n",
       "      <th>svd</th>\n",
       "      <th>svdpp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>13072</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>...</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>5</td>\n",
       "      <td>3.61198</td>\n",
       "      <td>3.61198</td>\n",
       "      <td>3.61198</td>\n",
       "      <td>3.61198</td>\n",
       "      <td>3.61198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>126</td>\n",
       "      <td>3418</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>...</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>5</td>\n",
       "      <td>3.61198</td>\n",
       "      <td>3.61198</td>\n",
       "      <td>3.61198</td>\n",
       "      <td>3.61198</td>\n",
       "      <td>3.61198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>268</td>\n",
       "      <td>11740</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>...</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>5</td>\n",
       "      <td>3.61198</td>\n",
       "      <td>3.61198</td>\n",
       "      <td>3.61198</td>\n",
       "      <td>3.61198</td>\n",
       "      <td>3.61198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1809</td>\n",
       "      <td>705</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>...</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>4</td>\n",
       "      <td>3.61198</td>\n",
       "      <td>3.61198</td>\n",
       "      <td>3.61198</td>\n",
       "      <td>3.61198</td>\n",
       "      <td>3.61198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1809</td>\n",
       "      <td>1140</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>...</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3.530464</td>\n",
       "      <td>3</td>\n",
       "      <td>3.61198</td>\n",
       "      <td>3.61198</td>\n",
       "      <td>3.61198</td>\n",
       "      <td>3.61198</td>\n",
       "      <td>3.61198</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   user  movie      GAvg      sur1      sur2      sur3      sur4      sur5  \\\n",
       "0     7  13072  3.530464  3.530464  3.530464  3.530464  3.530464  3.530464   \n",
       "1   126   3418  3.530464  3.530464  3.530464  3.530464  3.530464  3.530464   \n",
       "2   268  11740  3.530464  3.530464  3.530464  3.530464  3.530464  3.530464   \n",
       "3  1809    705  3.530464  3.530464  3.530464  3.530464  3.530464  3.530464   \n",
       "4  1809   1140  3.530464  3.530464  3.530464  3.530464  3.530464  3.530464   \n",
       "\n",
       "       smr1      smr2  ...      smr4      smr5      UAvg      MAvg  rating  \\\n",
       "0  3.530464  3.530464  ...  3.530464  3.530464  3.530464  3.530464       5   \n",
       "1  3.530464  3.530464  ...  3.530464  3.530464  3.530464  3.530464       5   \n",
       "2  3.530464  3.530464  ...  3.530464  3.530464  3.530464  3.530464       5   \n",
       "3  3.530464  3.530464  ...  3.530464  3.530464  3.530464  3.530464       4   \n",
       "4  3.530464  3.530464  ...  3.530464  3.530464  3.530464  3.530464       3   \n",
       "\n",
       "     bslpr  knn_bsl_u  knn_bsl_m      svd    svdpp  \n",
       "0  3.61198    3.61198    3.61198  3.61198  3.61198  \n",
       "1  3.61198    3.61198    3.61198  3.61198  3.61198  \n",
       "2  3.61198    3.61198    3.61198  3.61198  3.61198  \n",
       "3  3.61198    3.61198    3.61198  3.61198  3.61198  \n",
       "4  3.61198    3.61198    3.61198  3.61198  3.61198  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_test_df['svd'] = models_evaluation_test['svd']['predictions']\n",
    "reg_test_df['svdpp'] = models_evaluation_test['svdpp']['predictions']\n",
    "\n",
    "reg_test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare x_train and y_train\n",
    "x_train = reg_train.drop(['user', 'movie', 'rating',], axis=1)\n",
    "y_train = reg_train['rating']\n",
    "\n",
    "# prepare test data\n",
    "x_test = reg_test_df.drop(['user', 'movie', 'rating'], axis=1)\n",
    "y_test = reg_test_df['rating']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running XGBRegressor, we will tune hyperparameter using gridsearch cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 63 candidates, totalling 315 fits\n",
      "Best: -0.011012 using {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 1300}\n",
      "\n",
      "-1.012954 (0.030909) with: {'learning_rate': 0.001, 'max_depth': 1, 'n_estimators': 100}\n",
      "-0.794077 (0.026827) with: {'learning_rate': 0.001, 'max_depth': 1, 'n_estimators': 300}\n",
      "-0.639678 (0.022093) with: {'learning_rate': 0.001, 'max_depth': 1, 'n_estimators': 500}\n",
      "-0.519820 (0.018441) with: {'learning_rate': 0.001, 'max_depth': 1, 'n_estimators': 700}\n",
      "-0.424710 (0.015506) with: {'learning_rate': 0.001, 'max_depth': 1, 'n_estimators': 900}\n",
      "-0.349214 (0.013238) with: {'learning_rate': 0.001, 'max_depth': 1, 'n_estimators': 1100}\n",
      "-0.289241 (0.011460) with: {'learning_rate': 0.001, 'max_depth': 1, 'n_estimators': 1300}\n",
      "-0.961326 (0.028048) with: {'learning_rate': 0.001, 'max_depth': 2, 'n_estimators': 100}\n",
      "-0.665545 (0.020358) with: {'learning_rate': 0.001, 'max_depth': 2, 'n_estimators': 300}\n",
      "-0.466127 (0.015410) with: {'learning_rate': 0.001, 'max_depth': 2, 'n_estimators': 500}\n",
      "-0.329994 (0.012028) with: {'learning_rate': 0.001, 'max_depth': 2, 'n_estimators': 700}\n",
      "-0.236688 (0.009644) with: {'learning_rate': 0.001, 'max_depth': 2, 'n_estimators': 900}\n",
      "-0.172644 (0.007924) with: {'learning_rate': 0.001, 'max_depth': 2, 'n_estimators': 1100}\n",
      "-0.128615 (0.006687) with: {'learning_rate': 0.001, 'max_depth': 2, 'n_estimators': 1300}\n",
      "-0.955324 (0.027834) with: {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 100}\n",
      "-0.651150 (0.019687) with: {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 300}\n",
      "-0.446866 (0.014269) with: {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 500}\n",
      "-0.309668 (0.010705) with: {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 700}\n",
      "-0.217382 (0.008315) with: {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 900}\n",
      "-0.155278 (0.006725) with: {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 1100}\n",
      "-0.113440 (0.005671) with: {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 1300}\n",
      "-0.383049 (0.014172) with: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 100}\n",
      "-0.078265 (0.004879) with: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 300}\n",
      "-0.036266 (0.003678) with: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 500}\n",
      "-0.030063 (0.003595) with: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 700}\n",
      "-0.028340 (0.003494) with: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 900}\n",
      "-0.027188 (0.003385) with: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 1100}\n",
      "-0.026376 (0.003292) with: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 1300}\n",
      "-0.200257 (0.008675) with: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 100}\n",
      "-0.033275 (0.003739) with: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 300}\n",
      "-0.025851 (0.003412) with: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 500}\n",
      "-0.023413 (0.003174) with: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 700}\n",
      "-0.022317 (0.003059) with: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 900}\n",
      "-0.021800 (0.003008) with: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 1100}\n",
      "-0.021519 (0.002987) with: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 1300}\n",
      "-0.181916 (0.007441) with: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 100}\n",
      "-0.027456 (0.003570) with: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 300}\n",
      "-0.021237 (0.003207) with: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 500}\n",
      "-0.019729 (0.002942) with: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 700}\n",
      "-0.018710 (0.002900) with: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 900}\n",
      "-0.018111 (0.002887) with: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 1100}\n",
      "-0.017661 (0.002835) with: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 1300}\n",
      "-0.027671 (0.003443) with: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 100}\n",
      "-0.024279 (0.002998) with: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 300}\n",
      "-0.023845 (0.002899) with: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 500}\n",
      "-0.023638 (0.002864) with: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 700}\n",
      "-0.023478 (0.002841) with: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 900}\n",
      "-0.023356 (0.002823) with: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 1100}\n",
      "-0.023254 (0.002809) with: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 1300}\n",
      "-0.022064 (0.002970) with: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 100}\n",
      "-0.019838 (0.002916) with: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 300}\n",
      "-0.018608 (0.002863) with: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 500}\n",
      "-0.017675 (0.002860) with: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 700}\n",
      "-0.016954 (0.002821) with: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 900}\n",
      "-0.016406 (0.002756) with: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 1100}\n",
      "-0.015916 (0.002709) with: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 1300}\n",
      "-0.018455 (0.002904) with: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100}\n",
      "-0.013926 (0.002567) with: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 300}\n",
      "-0.012567 (0.002427) with: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 500}\n",
      "-0.011849 (0.002417) with: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 700}\n",
      "-0.011488 (0.002393) with: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 900}\n",
      "-0.011189 (0.002407) with: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 1100}\n",
      "-0.011012 (0.002439) with: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 1300}\n",
      "\n",
      "Time Taken:  0:01:55.579627\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "# Initialize Our first XGBoost model\n",
    "model = xgb.XGBRegressor(nthread=-1)\n",
    "\n",
    "# Perform cross validation \n",
    "gscv = GridSearchCV(model,\n",
    "                    param_grid = parameters,\n",
    "                    scoring=\"neg_mean_squared_error\",\n",
    "                    cv = TimeSeriesSplit(n_splits=5),\n",
    "                    n_jobs = -1,\n",
    "                    verbose = 1)\n",
    "gscv_result = gscv.fit(x_train, y_train)\n",
    "\n",
    "# Summarize results\n",
    "print(\"Best: %f using %s\" % (gscv_result.best_score_, gscv_result.best_params_))\n",
    "print()\n",
    "means = gscv_result.cv_results_['mean_test_score']\n",
    "stds = gscv_result.cv_results_['std_test_score']\n",
    "params = gscv_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))  \n",
    "\n",
    "print(\"\\nTime Taken: \",datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=3, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             multi_strategy=None, n_estimators=1300, n_jobs=None, nthread=-1,\n",
       "             num_parallel_tree=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBRegressor</label><div class=\"sk-toggleable__content\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=3, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             multi_strategy=None, n_estimators=1300, n_jobs=None, nthread=-1,\n",
       "             num_parallel_tree=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=3, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             multi_strategy=None, n_estimators=1300, n_jobs=None, nthread=-1,\n",
       "             num_parallel_tree=None, ...)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create new instance of XGBRegressor with tuned hyperparameters\n",
    "xgb_final = xgb.XGBRegressor(max_depth=3,learning_rate = 0.01,n_estimators=1300,nthread=-1)\n",
    "xgb_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shory\\anaconda3\\envs\\myenv\\lib\\site-packages\\xgboost\\sklearn.py:889: UserWarning: `eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Time taken : 0:00:01.771107\n",
      "\n",
      "Done \n",
      "\n",
      "Evaluating the model with TRAIN data...\n",
      "Evaluating Test data\n",
      "---------------\n",
      "\n",
      "TEST DATA\n",
      "---------------\n",
      "RMSE :  1.1234619836629212\n",
      "MAPE :  38.588619195862314\n"
     ]
    }
   ],
   "source": [
    "train_results, test_results = run_xgboost(xgb_final, x_train, y_train, x_test, y_test)\n",
    "\n",
    "# store the results in models_evaluations dictionaries\n",
    "models_evaluation_train['xgb_final'] = train_results\n",
    "models_evaluation_test['xgb_final'] = test_results\n",
    "\n",
    "\n",
    "xgb.plot_importance(xgb_final)\n",
    "plt.savefig(\"sample/small/img/feature-importance-xgb+knn+bsl+svd.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XgBoost with Surprise Baseline + Surprise KNNbaseline + MF Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare train data\n",
    "x_train = reg_train[['knn_bsl_u', 'knn_bsl_m', 'svd', 'svdpp']]\n",
    "y_train = reg_train['rating']\n",
    "\n",
    "# test data\n",
    "x_test = reg_test_df[['knn_bsl_u', 'knn_bsl_m', 'svd', 'svdpp']]\n",
    "y_test = reg_test_df['rating']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running XGBRegressor, we will tune hyperparameter using gridsearch cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 63 candidates, totalling 315 fits\n",
      "Best: -0.017268 using {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 1300}\n",
      "\n",
      "-1.012954 (0.030909) with: {'learning_rate': 0.001, 'max_depth': 1, 'n_estimators': 100}\n",
      "-0.794077 (0.026827) with: {'learning_rate': 0.001, 'max_depth': 1, 'n_estimators': 300}\n",
      "-0.639678 (0.022093) with: {'learning_rate': 0.001, 'max_depth': 1, 'n_estimators': 500}\n",
      "-0.519820 (0.018441) with: {'learning_rate': 0.001, 'max_depth': 1, 'n_estimators': 700}\n",
      "-0.424710 (0.015506) with: {'learning_rate': 0.001, 'max_depth': 1, 'n_estimators': 900}\n",
      "-0.349214 (0.013238) with: {'learning_rate': 0.001, 'max_depth': 1, 'n_estimators': 1100}\n",
      "-0.289241 (0.011460) with: {'learning_rate': 0.001, 'max_depth': 1, 'n_estimators': 1300}\n",
      "-0.961326 (0.028048) with: {'learning_rate': 0.001, 'max_depth': 2, 'n_estimators': 100}\n",
      "-0.665545 (0.020358) with: {'learning_rate': 0.001, 'max_depth': 2, 'n_estimators': 300}\n",
      "-0.466127 (0.015410) with: {'learning_rate': 0.001, 'max_depth': 2, 'n_estimators': 500}\n",
      "-0.329994 (0.012028) with: {'learning_rate': 0.001, 'max_depth': 2, 'n_estimators': 700}\n",
      "-0.236688 (0.009644) with: {'learning_rate': 0.001, 'max_depth': 2, 'n_estimators': 900}\n",
      "-0.172644 (0.007924) with: {'learning_rate': 0.001, 'max_depth': 2, 'n_estimators': 1100}\n",
      "-0.128615 (0.006687) with: {'learning_rate': 0.001, 'max_depth': 2, 'n_estimators': 1300}\n",
      "-0.955324 (0.027834) with: {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 100}\n",
      "-0.651150 (0.019687) with: {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 300}\n",
      "-0.446866 (0.014269) with: {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 500}\n",
      "-0.309668 (0.010705) with: {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 700}\n",
      "-0.217385 (0.008314) with: {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 900}\n",
      "-0.155317 (0.006726) with: {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 1100}\n",
      "-0.113551 (0.005679) with: {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 1300}\n",
      "-0.383049 (0.014172) with: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 100}\n",
      "-0.078265 (0.004879) with: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 300}\n",
      "-0.036266 (0.003678) with: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 500}\n",
      "-0.030067 (0.003597) with: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 700}\n",
      "-0.029157 (0.003606) with: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 900}\n",
      "-0.028822 (0.003582) with: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 1100}\n",
      "-0.028557 (0.003561) with: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 1300}\n",
      "-0.200257 (0.008675) with: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 100}\n",
      "-0.033397 (0.003748) with: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 300}\n",
      "-0.028035 (0.003553) with: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 500}\n",
      "-0.026466 (0.003500) with: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 700}\n",
      "-0.025311 (0.003390) with: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 900}\n",
      "-0.024574 (0.003327) with: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 1100}\n",
      "-0.024132 (0.003330) with: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 1300}\n",
      "-0.181929 (0.007447) with: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 100}\n",
      "-0.029295 (0.003601) with: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 300}\n",
      "-0.024162 (0.003509) with: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 500}\n",
      "-0.023011 (0.003378) with: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 700}\n",
      "-0.021625 (0.003229) with: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 900}\n",
      "-0.020589 (0.003212) with: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 1100}\n",
      "-0.019970 (0.003193) with: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 1300}\n",
      "-0.029087 (0.003629) with: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 100}\n",
      "-0.027403 (0.003462) with: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 300}\n",
      "-0.026849 (0.003372) with: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 500}\n",
      "-0.026635 (0.003329) with: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 700}\n",
      "-0.026542 (0.003304) with: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 900}\n",
      "-0.026489 (0.003291) with: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 1100}\n",
      "-0.026443 (0.003290) with: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 1300}\n",
      "-0.024835 (0.003267) with: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 100}\n",
      "-0.022397 (0.003219) with: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 300}\n",
      "-0.021293 (0.003147) with: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 500}\n",
      "-0.020567 (0.003125) with: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 700}\n",
      "-0.020071 (0.003085) with: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 900}\n",
      "-0.019749 (0.003052) with: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 1100}\n",
      "-0.019485 (0.003063) with: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 1300}\n",
      "-0.021021 (0.003127) with: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100}\n",
      "-0.018344 (0.003173) with: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 300}\n",
      "-0.017683 (0.003198) with: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 500}\n",
      "-0.017446 (0.003265) with: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 700}\n",
      "-0.017354 (0.003362) with: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 900}\n",
      "-0.017282 (0.003436) with: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 1100}\n",
      "-0.017268 (0.003507) with: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 1300}\n",
      "\n",
      "Time Taken:  0:01:09.879470\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "# Initialize Our first XGBoost model\n",
    "model = xgb.XGBRegressor(nthread=-1)\n",
    "\n",
    "# Perform cross validation \n",
    "gscv = GridSearchCV(model,\n",
    "                    param_grid = parameters,\n",
    "                    scoring=\"neg_mean_squared_error\",\n",
    "                    cv = TimeSeriesSplit(n_splits=5),\n",
    "                    n_jobs = -1,\n",
    "                    verbose = 1)\n",
    "gscv_result = gscv.fit(x_train, y_train)\n",
    "\n",
    "# Summarize results\n",
    "print(\"Best: %f using %s\" % (gscv_result.best_score_, gscv_result.best_params_))\n",
    "print()\n",
    "means = gscv_result.cv_results_['mean_test_score']\n",
    "stds = gscv_result.cv_results_['std_test_score']\n",
    "params = gscv_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))  \n",
    "\n",
    "print(\"\\nTime Taken: \",datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=1, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             multi_strategy=None, n_estimators=700, n_jobs=None, nthread=-1,\n",
       "             num_parallel_tree=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBRegressor</label><div class=\"sk-toggleable__content\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=1, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             multi_strategy=None, n_estimators=700, n_jobs=None, nthread=-1,\n",
       "             num_parallel_tree=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=1, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             multi_strategy=None, n_estimators=700, n_jobs=None, nthread=-1,\n",
       "             num_parallel_tree=None, ...)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create new instance of XGBRegressor with tuned hyperparameters\n",
    "xgb_all_models = xgb.XGBRegressor(max_depth=1,learning_rate = 0.01,n_estimators=700,nthread=-1)\n",
    "xgb_all_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shory\\anaconda3\\envs\\myenv\\lib\\site-packages\\xgboost\\sklearn.py:889: UserWarning: `eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Time taken : 0:00:00.460385\n",
      "\n",
      "Done \n",
      "\n",
      "Evaluating the model with TRAIN data...\n",
      "Evaluating Test data\n",
      "---------------\n",
      "\n",
      "TEST DATA\n",
      "---------------\n",
      "RMSE :  1.1367862682953647\n",
      "MAPE :  38.96381974913251\n"
     ]
    }
   ],
   "source": [
    "train_results, test_results = run_xgboost(xgb_all_models, x_train, y_train, x_test, y_test)\n",
    "\n",
    "# store the results in models_evaluations dictionaries\n",
    "models_evaluation_train['xgb_all_models'] = train_results\n",
    "models_evaluation_test['xgb_all_models'] = test_results\n",
    "\n",
    "xgb.plot_importance(xgb_all_models)\n",
    "plt.savefig(\"sample/small/img/feature-importance-xgb_with_bsl+knn+svd.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparision between all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "svd               1.0821912114098393\n",
       "knn_bsl_m         1.0821949464872673\n",
       "bsl_algo          1.0822513101982842\n",
       "knn_bsl_u         1.0822531134111517\n",
       "svdpp             1.0822543401393163\n",
       "first_algo        1.1075854286551927\n",
       "xgb_knn_bsl         1.11901091975606\n",
       "xgb_bsl           1.1234619836629212\n",
       "xgb_final         1.1234619836629212\n",
       "xgb_all_models    1.1367862682953647\n",
       "Name: rmse, dtype: object"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Saving our TEST_RESULTS into a dataframe to avoid running it again\n",
    "pd.DataFrame(models_evaluation_test).to_csv('sample/small/models-rmse-comparison.csv')\n",
    "models = pd.read_csv('sample/small/models-rmse-comparison.csv', index_col=0)\n",
    "models.loc['rmse'].sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Which is best method/model and Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### XGBoost with Surprise Baseline Predictor model (xgb_bsl) showed good result among all the models I tried.\n",
    "- ### Due to high computational power and time, I have completed this case study on (8000,800) training dataset and (4000,400) testing dataset.\n",
    "- ### Approach followed is as mentioned in [this](https://datajobs.com/data-science-repo/Recommender-Systems-%5BNetflix%5D.pdf) research paper\n",
    "- ### Small decrease in 'RMSE' score is observed, but this can be drastically improved by using the whole dataset for modeling.(Not feasible at the moment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SOTA Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As per [this](https://datajobs.com/data-science-repo/Recommender-Systems-%5BNetflix%5D.pdf) research paper, the Netflix system achieves **RMSE = 0.9514** on the same dataset\n",
    "### While the grand prize’s required accuracy is RMSE = 0.8563, and it was won by BigChaos solution with **RMSE=0.8567**\n",
    "\n",
    "### I acheived minimum **RMSE = 1.0821912114098393** "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
